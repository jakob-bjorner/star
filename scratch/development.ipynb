{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "os.chdir(\"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-29/00-52-56\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "['/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_0/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_229/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_465/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_702/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_940/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_1176/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_1411/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_1642/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_1869/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/train_2093/generated.csv']\n",
      "['/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/val_0/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/val_0/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/val_940/generated.csv',\n",
      " '/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0/val_2313/generated.csv']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing index 0 and index 1\n",
      "len(indices_where_a_not_eq_b)=2463\n",
      "len(indices_where_both_wrong)=2579 len(indices_where_a_is_right_b_is_wrong)=1122 len(indices_where_b_is_right_a_is_wrong)=1341 len(indices_where_a_not_eq_b)=2463\n",
      "len(set(indices_where_both_wrong+indices_where_a_is_right_b_is_wrong+indices_where_b_is_right_a_is_wrong+indices_where_a_not_eq_b))=5042\n",
      "number 0 of 2463\n",
      "SUBJECT: us_foreign_policy\n",
      "CORRECT ANSWER: C\n",
      "MODEL ANSWERS: ex0: A, ex1: C\n",
      "QUESTION: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\n",
      "\n",
      "In general, how do conservatives and liberals differ when it comes to defense spending?\n",
      "\n",
      "A) Conservatives are skeptical of increases in defense spending; liberals advocate increases in defense spending.\n",
      "B) Conservatives favor social spending over defense spending; liberals favor defense spending over social spending.\n",
      "C) Conservatives advocate increases in defense spending; liberals are generally skeptical of increases in defense spending.\n",
      "D) Conservatives emphasize strong economies; liberals emphasize strong military readiness.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "example 0 ****************************************************************************************************\n",
      "(\"To answer this question, let's break down the general differences in \"\n",
      " 'ideology between conservatives and liberals in the context of defense '\n",
      " 'spending.\\n'\n",
      " '\\n'\n",
      " '1. **Understanding Conservatism and Liberalism:**\\n'\n",
      " '   - **Conservatism** often emphasizes limited government intervention in '\n",
      " 'economic matters, individual freedoms, and a more isolationist approach to '\n",
      " 'international relations, at least when it comes to military interventions. '\n",
      " 'In terms of defense spending, conservatives might prioritize fiscal '\n",
      " 'responsibility and often have concerns about the financial implications of '\n",
      " 'high defense budgets, especially if they feel they could be allocated '\n",
      " 'differently (e.g., to cut taxes or address domestic issues).\\n'\n",
      " '   - **Liberalism**, on the other hand, typically prioritizes social welfare '\n",
      " 'programs, healthcare, education, and, in many cases, a more interventionist '\n",
      " 'foreign policy. When it comes to defense, liberals might be less concerned '\n",
      " 'with fiscal prudence regarding defense spending and more concerned with the '\n",
      " 'need for a strong military to protect national interests and intervene in '\n",
      " 'global conflicts.\\n'\n",
      " '\\n'\n",
      " '2. **Analyzing the Options:**\\n'\n",
      " '   - **Option A** suggests conservatives are skeptical of increases in '\n",
      " 'defense spending and liberals advocate for them. This aligns with the '\n",
      " 'general conservative emphasis on fiscal responsibility and the potential '\n",
      " 'prioritization of social spending over defense by liberals.\\n'\n",
      " '   - **Option B** suggests conservatives favor social spending over defense '\n",
      " 'and liberals favor defense over social spending, which flips the '\n",
      " 'conservative-liberal dichotomy.\\n'\n",
      " '   - **Option C** reverses the skepticism regarding defense spending, '\n",
      " 'stating that conservatives advocate for increases and liberals are '\n",
      " 'skeptical. This contradicts the general conservative fiscal responsibility '\n",
      " 'stance.\\n'\n",
      " '   - **Option D** suggests a focus on economies and military readiness as '\n",
      " \"opposing priorities, which doesn't accurately capture the traditional \"\n",
      " 'conservative-liberal split on defense spending.\\n'\n",
      " '\\n'\n",
      " '3. **Conclusion:**\\n'\n",
      " '   Based on the general ideological positions of conservatives and liberals '\n",
      " 'on defense spending, the correct answer is that conservatives tend to be '\n",
      " 'skeptical of increases in defense spending, whereas liberals are more '\n",
      " 'supportive of increases in defense spending as part of a broader approach to '\n",
      " 'national security and global engagement.\\n'\n",
      " '\\n'\n",
      " 'Answer: A<|eot_id|>')\n",
      "example 1 ****************************************************************************************************\n",
      "(\"To answer this question, let's break down the general tendencies of \"\n",
      " 'conservatives and liberals regarding defense spending.\\n'\n",
      " '\\n'\n",
      " 'Conservatives often prioritize national security and the military, and as '\n",
      " 'such, they tend to support increases in defense spending to ensure the '\n",
      " \"country's military readiness and power. They might argue that a strong \"\n",
      " 'military is essential for national defense and international influence.\\n'\n",
      " '\\n'\n",
      " 'Liberals, on the other hand, might be more concerned with social welfare and '\n",
      " 'domestic issues, which can lead them to be skeptical of significant '\n",
      " 'increases in defense spending, especially if it means diverting funds from '\n",
      " 'domestic programs or increasing the national debt.\\n'\n",
      " '\\n'\n",
      " \"Given this, let's review the options:\\n\"\n",
      " '\\n'\n",
      " 'A) Conservatives are skeptical of increases in defense spending; liberals '\n",
      " 'advocate increases in defense spending. This statement is the opposite of '\n",
      " 'what is generally observed, so it is incorrect.\\n'\n",
      " '\\n'\n",
      " 'B) Conservatives favor social spending over defense spending; liberals favor '\n",
      " 'defense spending over social spending. This statement does not accurately '\n",
      " 'capture the general attitudes of either group.\\n'\n",
      " '\\n'\n",
      " 'C) Conservatives advocate increases in defense spending; liberals are '\n",
      " 'generally skeptical of increases in defense spending. This statement aligns '\n",
      " 'with the general tendencies of conservatives and liberals regarding defense '\n",
      " 'spending.\\n'\n",
      " '\\n'\n",
      " 'D) Conservatives emphasize strong economies; liberals emphasize strong '\n",
      " 'military readiness. This option does not capture the primary difference in '\n",
      " 'their views on defense spending.\\n'\n",
      " '\\n'\n",
      " 'Answer: C<|eot_id|>')\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# exploring the datasets collected with ipywidgets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive, Layout\n",
    "from IPython.display import display, HTML, display_pretty\n",
    "import glob\n",
    "from pprint import pprint\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from star.agent.agent import GeneratedSamples\n",
    "\n",
    "def interactive_function(output_dir: str, index_a, index_b):\n",
    "    get_iteration = lambda name : int(name.split(\"_\")[-1].split(\"/\")[0])\n",
    "    train_files = sorted(glob.glob(os.path.join(output_dir, \"train_*/generated.csv\")), key=get_iteration)\n",
    "    val_files = sorted(glob.glob(os.path.join(output_dir, \"val_*/generated.csv\")), key=get_iteration)\n",
    "    pprint(train_files)\n",
    "    filtered_iterations = set(get_iteration(file) for file in train_files)\n",
    "    val_files = [val_files[0]] + [val_file for val_file in val_files if (get_iteration(val_file) + 0) in filtered_iterations] + [val_files[-1]]\n",
    "    pprint(val_files)\n",
    "\n",
    "    def get_dfs(files):\n",
    "        df_generated_samples_per_file = {\n",
    "            file: pd.read_csv(file, index_col=0)\n",
    "                for file in files\n",
    "        }\n",
    "        return df_generated_samples_per_file\n",
    "    def get_gen_samples(files):\n",
    "        gen_samples_per_file = dict()\n",
    "        df_generated_samples_per_file = get_dfs(files)\n",
    "        for file, df in df_generated_samples_per_file.items():\n",
    "            gen_samples_per_file[file] = GeneratedSamples(**df.to_dict(\"list\"))\n",
    "        return gen_samples_per_file\n",
    "    def get_step_accs(generated_samples_per_file):\n",
    "        acc = lambda it: sum(it) / len(it)\n",
    "        steps = []\n",
    "        accs = []\n",
    "        for file, gen_samples in generated_samples_per_file.items():\n",
    "            step_i = int(file.split('_')[-1].split(\"/\")[0])\n",
    "            acc_i = acc(gen_samples.scores)\n",
    "\n",
    "            steps.append(step_i)\n",
    "            accs.append(acc_i)\n",
    "        steps, accs = list(zip(*sorted(list(zip(steps, accs)))))\n",
    "        steps = list(range(len(steps))) # relabeling from gradient steps to star iterations\n",
    "        return steps, accs\n",
    "    train_gen_samples_per_file = get_gen_samples(train_files)\n",
    "    val_gen_samples_per_file = get_gen_samples(val_files)\n",
    "    plt.plot(*get_step_accs(train_gen_samples_per_file), linestyle='--', marker='o', label=\"train\")\n",
    "    plt.plot(*get_step_accs(val_gen_samples_per_file), linestyle='-', marker='x', label=\"val\")\n",
    "    plt.title(\"Accuracy vs iterations of SFT\")\n",
    "    plt.xlabel(\"iterations of SFT\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if len(train_files) <= index_a or len(train_files) <= index_b:\n",
    "        print(f\"invalid indices given for len {len(train_files)}\")\n",
    "        return\n",
    "    print(f\"comparing index {index_a} and index {index_b}\")\n",
    "    gen_samples_a: GeneratedSamples = list(train_gen_samples_per_file.values())[index_a]\n",
    "    gen_samples_b: GeneratedSamples = list(train_gen_samples_per_file.values())[index_b]\n",
    "    \n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # find the interesting points\n",
    "    # print(f\"{len(gen_samples_a.correct_answers)=}  {len(gen_samples_b.correct_answers)=}\")\n",
    "    # assert len(gen_samples_a.correct_answers) == len(gen_samples_b.correct_answers)\n",
    "    # I can simply sort the \n",
    "    indices_where_a_not_eq_b = [i for i in range(len(gen_samples_a.correct_answers)) if (gen_samples_a.scores[i] != gen_samples_b.scores[i])]\n",
    "    indices_where_both_wrong = [i for i in range(len(gen_samples_a.correct_answers)) if (gen_samples_a.scores[i] != 1.0 and gen_samples_b.scores[i] != 1.0)]\n",
    "    indices_where_a_is_right_b_is_wrong = [i for i in range(len(gen_samples_a.correct_answers)) if (gen_samples_a.scores[i] == 1.0 and gen_samples_b.scores[i] != 1.0)]\n",
    "    indices_where_b_is_right_a_is_wrong = [i for i in range(len(gen_samples_a.correct_answers)) if (gen_samples_a.scores[i] != 1.0 and gen_samples_b.scores[i] == 1.0)]\n",
    "    print(f\"{len(indices_where_a_not_eq_b)=}\")\n",
    "    train_dfs_per_file = list(get_dfs(train_files).values())\n",
    "    train_df_a = train_dfs_per_file[index_a]\n",
    "    train_df_b = train_dfs_per_file[index_b]\n",
    "    print(f\"{len(indices_where_both_wrong)=} {len(indices_where_a_is_right_b_is_wrong)=} {len(indices_where_b_is_right_a_is_wrong)=} {len(indices_where_a_not_eq_b)=}\")\n",
    "    print(f\"{len(set(indices_where_both_wrong+indices_where_a_is_right_b_is_wrong+indices_where_b_is_right_a_is_wrong+indices_where_a_not_eq_b))=}\")\n",
    "    for iteration, index in enumerate(indices_where_a_not_eq_b):\n",
    "        print(f\"number {iteration} of {len(indices_where_a_not_eq_b)}\")\n",
    "        example_a = train_df_a.iloc[index].to_dict()\n",
    "        example_b = train_df_b.iloc[index].to_dict()\n",
    "        assert example_a['prompted_questions'] == example_b['prompted_questions'] and example_a['correct_answers'] == example_b['correct_answers']\n",
    "        print(f\"SUBJECT: {example_a['subjects']}\")\n",
    "        print(f\"CORRECT ANSWER: {example_a['correct_answers']}\")\n",
    "        print(f\"MODEL ANSWERS: ex{index_a}: {example_a['model_answers']}, ex{index_b}: {example_b['model_answers']}\")\n",
    "        print(f\"QUESTION: {example_a['prompted_questions']}\")\n",
    "        print(f\"example {index_a} {'*'*100}\")\n",
    "        pprint(example_a[\"raw_responses\"])\n",
    "        print(f\"example {index_b} {'*'*100}\")\n",
    "        pprint(example_b[\"raw_responses\"])\n",
    "        break\n",
    "\n",
    "\n",
    "    \n",
    "# output_dirs=sorted(glob.glob(\"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/*/*\"))[:-12:-1]\n",
    "# # pprint(output_dirs)\n",
    "# output_dir = \"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-30/00-12-18_0\"\n",
    "# # output_dir = \"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/train_2024-10-01/03-06-34_0\"\n",
    "# # output_dir = \"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/train_translate_2024-10-02/04-20-40_0\"\n",
    "# interactive_function(output_dir, 0, 1)\n",
    "# # get_val_and_test_plot(\"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/None_2024-09-29/02-34-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_gen_us_foreign_policy_acc': 0.275,\n",
       " 'train_gen_professional_law_acc': 0.2148626817447496,\n",
       " 'train_gen_college_chemistry_acc': 0.20987654320987653,\n",
       " 'train_gen_prehistory_acc': 0.199203187250996,\n",
       " 'train_gen_elementary_mathematics_acc': 0.2870967741935484,\n",
       " 'train_gen_security_studies_acc': 0.23,\n",
       " 'train_gen_philosophy_acc': 0.20851063829787234,\n",
       " 'train_gen_virology_acc': 0.2706766917293233,\n",
       " 'train_gen_moral_scenarios_acc': 0.21112696148359486,\n",
       " 'train_gen_high_school_geography_acc': 0.31446540880503143,\n",
       " 'train_gen_clinical_knowledge_acc': 0.19069767441860466,\n",
       " 'train_gen_high_school_computer_science_acc': 0.2564102564102564,\n",
       " 'train_gen_high_school_us_history_acc': 0.2893081761006289,\n",
       " 'train_gen_college_medicine_acc': 0.2571428571428571,\n",
       " 'train_gen_nutrition_acc': 0.1825726141078838,\n",
       " 'train_gen_econometrics_acc': 0.25,\n",
       " 'train_gen_moral_disputes_acc': 0.2518518518518518,\n",
       " 'train_gen_professional_accounting_acc': 0.18723404255319148,\n",
       " 'train_gen_miscellaneous_acc': 0.2679738562091503,\n",
       " 'train_gen_high_school_chemistry_acc': 0.24161073825503357,\n",
       " 'train_gen_high_school_biology_acc': 0.26422764227642276,\n",
       " 'train_gen_electrical_engineering_acc': 0.25,\n",
       " 'train_gen_college_computer_science_acc': 0.12658227848101267,\n",
       " 'train_gen_abstract_algebra_acc': 0.17647058823529413,\n",
       " 'train_gen_formal_logic_acc': 0.24468085106382978,\n",
       " 'train_gen_professional_medicine_acc': 0.2523364485981308,\n",
       " 'train_gen_medical_genetics_acc': 0.30864197530864196,\n",
       " 'train_gen_professional_psychology_acc': 0.2603305785123967,\n",
       " 'train_gen_high_school_microeconomics_acc': 0.30158730158730157,\n",
       " 'train_gen_conceptual_physics_acc': 0.1783783783783784,\n",
       " 'train_gen_college_biology_acc': 0.2672413793103448,\n",
       " 'train_gen_high_school_psychology_acc': 0.22297297297297297,\n",
       " 'train_gen_high_school_world_history_acc': 0.3034825870646766,\n",
       " 'train_gen_management_acc': 0.1794871794871795,\n",
       " 'train_gen_anatomy_acc': 0.22123893805309736,\n",
       " 'train_gen_machine_learning_acc': 0.2,\n",
       " 'train_gen_human_aging_acc': 0.2784090909090909,\n",
       " 'train_gen_public_relations_acc': 0.21505376344086022,\n",
       " 'train_gen_high_school_mathematics_acc': 0.11822660098522167,\n",
       " 'train_gen_astronomy_acc': 0.24,\n",
       " 'train_gen_high_school_physics_acc': 0.14754098360655737,\n",
       " 'train_gen_marketing_acc': 0.22994652406417113,\n",
       " 'train_gen_human_sexuality_acc': 0.22727272727272727,\n",
       " 'train_gen_jurisprudence_acc': 0.24050632911392406,\n",
       " 'train_gen_business_ethics_acc': 0.28378378378378377,\n",
       " 'train_gen_high_school_statistics_acc': 0.21893491124260356,\n",
       " 'train_gen_logical_fallacies_acc': 0.33587786259541985,\n",
       " 'train_gen_high_school_macroeconomics_acc': 0.19444444444444445,\n",
       " 'train_gen_sociology_acc': 0.281437125748503,\n",
       " 'train_gen_college_physics_acc': 0.2345679012345679,\n",
       " 'train_gen_high_school_government_and_politics_acc': 0.26174496644295303,\n",
       " 'train_gen_computer_security_acc': 0.22784810126582278,\n",
       " 'train_gen_international_law_acc': 0.4020618556701031,\n",
       " 'train_gen_global_facts_acc': 0.2235294117647059,\n",
       " 'train_gen_high_school_european_history_acc': 0.16153846153846155,\n",
       " 'train_gen_college_mathematics_test.csv_YO-NG.csv_acc': 0.13924050632911392,\n",
       " 'train_gen_world_religions_acc': 0.265625}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "samples = GeneratedSamples(**pd.read_csv(\"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/train_translate_2024-10-02/04-20-40_0/train_301/generated.csv\", index_col=0).to_dict('list'))\n",
    "acc = lambda x: sum(x) / len(x)\n",
    "{f\"train_gen_{subject}_acc\": acc(samples.get_subset_by_subject(subject).scores) for subject in samples.subjects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.385 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from pprint import pprint\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.385 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FastLanguageModel.for_inference(model2)\n",
    "torch.set_grad_enabled(False)\n",
    "with torch.no_grad(): # looking for lowest perplexity decoding. do something on your own in terms of replicating beam search or replicating MCTS, don't want to have all beams degenerate, we should enforce exploration...\n",
    "    # sample for first 10 tokens, and then perform greedy rollout from there? (or even beam search from there?)\n",
    "    output = model2.generate(**tokenizer([\"print forever \"] * 3, return_tensors='pt'), max_new_tokens=20, use_cache=True, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>print forever 2023\\n'\n",
      " '\\n'\n",
      " '**Forever in 2023: A Year of Endless Possibilities**\\n'\n",
      " '\\n'\n",
      " 'As we',\n",
      " '<|begin_of_text|>print forever 2023\\n'\n",
      " 'Forever is a popular annual print magazine that has been published since '\n",
      " '2001. It',\n",
      " '<|begin_of_text|>print forever 2019\\n'\n",
      " 'Print Forever is a 2019 American science fiction comedy film directed by '\n",
      " 'David Robert Mitchell']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(output.tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model2.generate(input_ids=output, max_new_tokens=20, use_cache=True, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>print forever 2022\\n'\n",
      " 'Print Forever is a 2022 documentary film directed by Michael McFarland. The '\n",
      " 'film explores the concept of time and the human experience through the lens '\n",
      " 'of a group of people who are',\n",
      " '<|begin_of_text|>print forever 1\\n'\n",
      " '================\\n'\n",
      " '\\n'\n",
      " 'A simple Python program that prints \"Forever 1\" indefinitely.\\n'\n",
      " '\\n'\n",
      " '**Code**\\n'\n",
      " '```python\\n'\n",
      " 'while True:\\n'\n",
      " '    print(\"Forever 1\")\\n'\n",
      " '```\\n'\n",
      " '**Explanation**\\n'\n",
      " '\\n'\n",
      " 'This program',\n",
      " '<|begin_of_text|>print forever 2022\\n'\n",
      " 'Forever 2022 is a digital art exhibition that celebrates the intersection of '\n",
      " 'technology and art. The exhibition features a wide range of digital art '\n",
      " 'pieces, including paintings, sculptures, and installations,']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(output.tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-524., -756., -604.], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model(output)\n",
    "# o.logits.shape, output.shape #[output]\n",
    "# o.logits[output[...,None]]# .shape\n",
    "# output.max()\n",
    "# p.logits.shape # torch.arange(output.shape[0])[:,None,None], torch.arange(output.shape[1])[None, :, None]\n",
    "token_scores = o.logits.log_softmax(dim=-1)\n",
    "torch.gather(token_scores, dim=2, index=output[...,None]).flatten(1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-548., -732., -596.], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = FastLanguageModel.for_inference(model2)\n",
    "o = model2(output)\n",
    "# o.logits.shape, output.shape #[output]\n",
    "# o.logits[output[...,None]]# .shape\n",
    "# output.max()\n",
    "# p.logits.shape # torch.arange(output.shape[0])[:,None,None], torch.arange(output.shape[1])[None, :, None]\n",
    "token_scores = o.logits.log_softmax(dim=-1)\n",
    "torch.gather(token_scores, dim=2, index=output[...,None]).flatten(1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-12.9375, -10.4375,  -5.5938,  ..., -14.5000, -14.5000, -14.5000],\n",
       "         [-13.6875,  -8.2500, -12.6250,  ..., -19.0000, -19.0000, -19.0000],\n",
       "         [ -5.0312,  -7.6875, -10.8750,  ..., -20.8750, -20.8750, -20.8750],\n",
       "         ...,\n",
       "         [-14.1250, -16.1250, -19.1250,  ..., -21.7500, -21.7500, -21.7500],\n",
       "         [-28.1250, -30.0000, -26.0000,  ..., -27.0000, -27.0000, -27.0000],\n",
       "         [-21.0000, -20.5000, -22.6250,  ..., -28.5000, -28.5000, -28.5000]],\n",
       "\n",
       "        [[-12.9375, -10.4375,  -5.5938,  ..., -14.5000, -14.5000, -14.5000],\n",
       "         [-13.6875,  -8.2500, -12.6250,  ..., -19.0000, -19.0000, -19.0000],\n",
       "         [ -5.0312,  -7.6875, -10.8750,  ..., -20.8750, -20.8750, -20.8750],\n",
       "         ...,\n",
       "         [-14.4375, -14.6875, -15.9375,  ..., -21.2500, -21.2500, -21.2500],\n",
       "         [-14.7500, -16.2500, -15.8125,  ..., -20.7500, -20.7500, -20.7500],\n",
       "         [-10.5000, -14.9375, -14.5625,  ..., -21.2500, -21.2500, -21.2500]],\n",
       "\n",
       "        [[-12.9375, -10.4375,  -5.5938,  ..., -14.5000, -14.5000, -14.5000],\n",
       "         [-13.6875,  -8.2500, -12.6250,  ..., -19.0000, -19.0000, -19.0000],\n",
       "         [ -5.0312,  -7.6875, -10.8750,  ..., -20.8750, -20.8750, -20.8750],\n",
       "         ...,\n",
       "         [-18.0000, -19.5000, -20.3750,  ..., -21.6250, -21.6250, -21.6250],\n",
       "         [-14.8750, -14.6250, -15.6875,  ..., -21.2500, -21.2500, -21.2500],\n",
       "         [-13.8125, -16.7500, -17.2500,  ..., -22.1250, -22.1250, -22.1250]]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.logits.log_softmax(dim=-1)# [0,0].exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>print forever 2020\\n'\n",
      " 'Print Forever is a limited edition publication of 20 prints by 20 artists, '\n",
      " 'each',\n",
      " '<|begin_of_text|>print forever 2024\\n'\n",
      " 'The 2024 print forever stamp features a colorful image of a monarch '\n",
      " 'butterfly. The',\n",
      " '<|begin_of_text|>print forever 3d printing\\n'\n",
      " 'The Benefits of 3D Printing\\n'\n",
      " '3D printing, also known as additive']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(output.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (75) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     output_3 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprint forever \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beam_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/unsloth/models/llama.py:1401\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m \n\u001b[1;32m   1399\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1401\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m \n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/transformers/generation/utils.py:2094\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2087\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2088\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2089\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2090\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2091\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2092\u001b[0m     )\n\u001b[1;32m   2093\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 2094\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_group_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONSTRAINED_BEAM_SEARCH:\n\u001b[1;32m   2105\u001b[0m     final_constraints \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/transformers/generation/utils.py:3493\u001b[0m, in \u001b[0;36mGenerationMixin._group_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3490\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3491\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m-> 3493\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3496\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/unsloth/models/llama.py:937\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    950\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/unsloth/models/llama.py:801\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 801\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/unsloth/models/llama.py:486\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    485\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 486\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/unsloth/models/llama.py:376\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m rotary_emb(V, seq_len \u001b[38;5;241m=\u001b[39m kv_seq_len)\n\u001b[0;32m--> 376\u001b[0m     Q, K \u001b[38;5;241m=\u001b[39m \u001b[43minplace_rope_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/unsloth/kernels/rope_embedding.py:178\u001b[0m, in \u001b[0;36minplace_rope_embedding\u001b[0;34m(Q, K, cos, sin, position_ids)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minplace_rope_embedding\u001b[39m(Q, K, cos, sin, position_ids):\n\u001b[0;32m--> 178\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[43mSlow_RoPE_Embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     K \u001b[38;5;241m=\u001b[39m Slow_RoPE_Embedding\u001b[38;5;241m.\u001b[39mapply(K, cos, sin, position_ids)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Q, K\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/unsloth/kernels/rope_embedding.py:154\u001b[0m, in \u001b[0;36mSlow_RoPE_Embedding.forward\u001b[0;34m(ctx, Q, cos, sin, position_ids)\u001b[0m\n\u001b[1;32m    152\u001b[0m half \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    153\u001b[0m RH_Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;241m-\u001b[39mQ[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, half:], Q[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :half]), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m \u001b[43mQ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\n\u001b[1;32m    155\u001b[0m Q\u001b[38;5;241m.\u001b[39maddcmul_(RH_Q, sin)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# RH_Q *= sin\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Q += RH_Q\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (75) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     output_3 = model.generate(**tokenizer([\"print forever \"] * 3, return_tensors='pt'), use_cache=False, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprint forever \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "len(tokenizer(\"print forever \" * 1000).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from star.dataset.dataset import get_train_val_test_dataset\n",
    "train, _, _ = get_train_val_test_dataset(\"mmlu_YO-NG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3139, 4550)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(len(tokenizer(t[\"Question\"]).input_ids), i) for i, t in enumerate(train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to know how to train a simple torch model????\n",
    "# want to be able to replicate star... \n",
    "# They use a 6 Billion parameter sized model, \n",
    "# so maybe I start with just tuning a llama 7.1 billion instruct? \n",
    "# The model that star works off of is a pretrained one no finetuning\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dotenv import load_dotenv\n",
    "from math import ceil\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../simple-evals/mmlu.csv\").rename(columns={\"Unnamed: 0\": \"index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'D': 3776, 'C': 3582, 'B': 3462, 'A': 3222})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(df.Answer.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../simple-evals/mmlu.csv\").rename(columns={\"Unnamed: 0\": \"Subject_question_number\"})\n",
    "df = df.reset_index()\n",
    "# performing the train, test, val split before entering the dataset. As lists\n",
    "def train_test_split(dataset, test_size, seed=0):\n",
    "    n_samples = len(dataset)\n",
    "    train_size = 1.0 - test_size\n",
    "    assert isinstance(test_size, float)\n",
    "\n",
    "    n_test = ceil(test_size * n_samples)\n",
    "    n_train = int(train_size * n_samples)\n",
    "\n",
    "    generator = np.random.default_rng(seed)\n",
    "    permutation = generator.permutation(len(dataset))\n",
    "    test_indices = permutation[:n_test]\n",
    "    train_indices = permutation[n_test : (n_test + n_train)]\n",
    "    return [dataset[i] for i in train_indices], [dataset[j] for j in test_indices]\n",
    "raw_dataset = df.to_dict(\"records\")\n",
    "# raw_dataset = df.to_records()\n",
    "train_dataset, split_test = train_test_split(raw_dataset, 0.2)\n",
    "val_dataset, test_datset = train_test_split(split_test, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmlu_dataset = load_dataset(\"csv\", data_files=\"../../simple-evals/mmlu.csv\")\n",
    "# assert isinstance(mmlu_dataset, DatasetDict)\n",
    "# mmlu_dataset = mmlu_dataset.rename_column(\"Unnamed: 0\", \"index\")\n",
    "# mmlu_dataset = mmlu_dataset[\"train\"].train_test_split(test_size=0.2, seed=0)\n",
    "# split_test = mmlu_dataset[\"test\"].train_test_split(test_size=0.5, seed=0)\n",
    "# mmlu_dataset[\"val\"] = split_test[\"train\"]\n",
    "# mmlu_dataset[\"test\"] = split_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from star.utils.utils import QUERY_TEMPLATE_MULTICHOICE, SYSTEM_PROMPTED_TEMPLATE, ANSWER_PATTERN_MULTICHOICE, ANSWER_PATTERN, MULTILINGUAL_ANSWER_PATTERN_TEMPLATE, MULTILINGUAL_ANSWER_REGEXES, normalize_response, normalize_extracted_answer\n",
    "# class FormatDict(dict):\n",
    "#     def __missing__(self, key):\n",
    "#         return '{' + key + '}'\n",
    "\n",
    "# QUERY_TEMPLATE_MULTICHOICE = QUERY_TEMPLATE_MULTICHOICE.format_map(FormatDict(Preamble_To_Question=\"Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.384 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"temp_model_dpo\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = torch.bfloat16,\n",
    "#     # load_in_4bit = load_in_4bit,\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from star.dataset.dataset import GenerationCollateReturn, get_generation_collate_fn\n",
    "# @dataclass\n",
    "# class GenerationCollateReturn:\n",
    "#     indices: List[int]\n",
    "#     input_ids: torch.Tensor\n",
    "#     attention_mask: torch.Tensor\n",
    "#     input_strs: List[str]\n",
    "#     subjects: List[str]\n",
    "#     answers: List[str]\n",
    "#     def to(self, device):\n",
    "#         def tree_map(dict_obj):\n",
    "#             for key, val in dict_obj.items():\n",
    "#                 if isinstance(val, torch.Tensor):\n",
    "#                     dict_obj[key] = val.to(device)\n",
    "#                 elif isinstance(val, Dict):\n",
    "#                     tree_map(val)\n",
    "#         tree_map(self.__dict__)\n",
    "#         return self\n",
    "#     def __getitem__(self, i):\n",
    "#         return GenerationCollateReturn(indices=[self.indices[i]], \n",
    "#                                        input_ids=self.input_ids[[i]],\n",
    "#                                        attention_mask=self.attention_mask[[i]],\n",
    "#                                        input_strs=[self.input_strs[i]],\n",
    "#                                        subjects=[self.subjects[i]],\n",
    "#                                        answers=[self.answers[i]],)\n",
    "\n",
    "# def generation_collate_fn(examples):\n",
    "#     # print(examples)\n",
    "#     indices = []\n",
    "#     subjects = []\n",
    "#     answers = []\n",
    "#     input_strs = []\n",
    "#     for example in examples:\n",
    "#         indices.append(example[\"index\"])\n",
    "#         formated_question = QUERY_TEMPLATE_MULTICHOICE.format(**example)\n",
    "#         conversation = {\n",
    "#             \"system_message\": \"You are a helpful assistant\",\n",
    "#             # \"system_message\": \"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are a helpful assistant\",\n",
    "#             # \"system_message\": \"Cutting Knowledge Date: December 2023\\nToday Date: 23 July 2024\\n\\nYou are a helpful assistant\",\n",
    "#             \"user_message\": formated_question}\n",
    "#         input_str = SYSTEM_PROMPTED_TEMPLATE.format(**conversation) # + \"Let's think through this carefully, step by step.\"\n",
    "#         input_strs.append(input_str)\n",
    "#         subjects.append(example[\"Subject\"])\n",
    "#         answers.append(example[\"Answer\"])\n",
    "#     # inputs = tokenizer.apply_chat_template(input_strs, add_generation_prompt=True, tokenize=True, padding=True, return_tensors='pt')\n",
    "#     tokenized_conversations = tokenizer(text=input_strs, padding=True, return_tensors='pt')\n",
    "#     input_ids = tokenized_conversations[\"input_ids\"]\n",
    "#     attention_mask = tokenized_conversations[\"attention_mask\"]\n",
    "#     collate_return = GenerationCollateReturn(indices, input_ids, attention_mask, input_strs, subjects, answers)\n",
    "#     return collate_return\n",
    "generation_collate_fn = get_generation_collate_fn(tokenizer, system_message=\"You are a helpful assistant\", preamble_to_question=\"Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\")\n",
    "from typing import Dict\n",
    "# assert isinstance(mmlu_dataset, DatasetDict)\n",
    "dl = DataLoader(train_dataset, batch_size=64, collate_fn=generation_collate_fn, num_workers=0, pin_memory=True, pin_memory_device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model)\n",
    "# indices = []\n",
    "# prompted_questions = []\n",
    "# answers = []\n",
    "# raw_responses = []\n",
    "# scores = []\n",
    "# with torch.no_grad():\n",
    "#     for batch_number, data in enumerate(tqdm(dl)):\n",
    "#         indices.append([])\n",
    "#         prompted_questions.append([])\n",
    "#         answers.append([])\n",
    "#         raw_responses.append([])\n",
    "#         scores.append([])\n",
    "#         print(data)\n",
    "\n",
    "#         for num_responses in range(2):\n",
    "#             # if batch_number == 0:\n",
    "#             #     continue\n",
    "#             # if batch_number == 4:\n",
    "#             #     break\n",
    "#             # break\n",
    "#             assert isinstance(data, GenerationCollateReturn)\n",
    "#             data.to(\"cuda\")\n",
    "#             print(f\"{data.input_ids.shape=}\")\n",
    "\n",
    "#             outputs = model.generate(input_ids=data.input_ids, attention_mask=data.attention_mask, max_new_tokens=1024, use_cache=True, do_sample=True, temperature=1.0, top_p=0.7) #  , temperature=1.0 min_p ???\n",
    "#             print(f\"{outputs.shape=}\")\n",
    "#             for i, output in enumerate(tokenizer.batch_decode(outputs[:, data.input_ids.shape[1]:])):\n",
    "#                 if len(scores[-1]) <= i:\n",
    "#                     scores[-1].append([])\n",
    "#                     raw_responses[-1].append([])\n",
    "#                     answers[-1].append([])\n",
    "\n",
    "#                 try:\n",
    "#                     raw_answer = output[:output.index(\"<|eot_id|>\") + len(\"<|eot_id|>\")]\n",
    "#                     response_text = normalize_response(raw_answer)\n",
    "#                     extracted_answer = None\n",
    "#                     for answer_regex in MULTILINGUAL_ANSWER_REGEXES:\n",
    "#                         regex = MULTILINGUAL_ANSWER_PATTERN_TEMPLATE.format(answer_regex)\n",
    "#                         match = re.search(regex, response_text)\n",
    "#                         if match:\n",
    "#                             extracted_answer = normalize_extracted_answer(match.group(1))\n",
    "#                             break\n",
    "#                     score = 1.0 if extracted_answer == data.answers[i] else 0.0\n",
    "#                     answers[-1][i].append(extracted_answer)\n",
    "#                     raw_responses[-1][i].append(raw_answer)\n",
    "#                     scores[-1][i].append(score)\n",
    "#                 except Exception as e:\n",
    "#                     print(e)\n",
    "#                     answers[-1][i].append(None)\n",
    "#                     raw_responses[-1][i].append(None)\n",
    "#                     scores[-1][i].append(0.0)\n",
    "#                     pass\n",
    "#                 # print(extracted_answer)\n",
    "#                 # print(data.answers[i])\n",
    "#                 # print(score)\n",
    "#                 # print(raw_answer)\n",
    "#             for i, input_str in enumerate(data.input_strs):\n",
    "#                 if len(prompted_questions[-1]) <= i:\n",
    "#                     prompted_questions[-1].append([])\n",
    "#                     indices[-1].append([])\n",
    "#                 prompted_questions[-1][i].append(input_str)\n",
    "#                 indices[-1][i].append(data.indices[i])\n",
    "#             # [print(output.shape) for output in outputs]\n",
    "#             # [print(tokenizer.decode(output)) for output in outputs]\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump({\n",
    "#     \"data\":data,\n",
    "#     \"indices\": indices,\n",
    "#     \"prompted_questions\": prompted_questions,\n",
    "#     \"answers\": answers,\n",
    "#     \"raw_responses\": raw_responses,\n",
    "#     \"scores\": scores,\n",
    "# }, open(\"temp_dpo_output.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(i for l in scores[0] for i in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = pickle.load(open(\"temp_base_output.pkl\", 'rb')) # temp_base_output temp_dpo_output\n",
    "# data, indices, prompted_questions, answers, raw_responses, scores = [dic[k] for k in [\"data\", \"indices\", \"prompted_questions\", \"answers\", \"raw_responses\", \"scores\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from star.dataset.dataset import DPODataset, DPOCollateReturn, get_dpo_collate_fn\n",
    "# class DPODataset(Dataset):\n",
    "#     internal_data: list # list of tuples, giving the prompted qeustions, the answers, the raw responses, and the indices\n",
    "#     def __init__(self, data, indices, prompted_questions, answers, raw_responses, scores):\n",
    "#         # filter to only the ones where the scores don't agree on some examples. \n",
    "#         self.internal_data = []\n",
    "#         for i, scores_for_one_question in enumerate(scores):\n",
    "#             if 0 < sum(scores_for_one_question) < len(scores_for_one_question):\n",
    "#                 # mark this as contrastive examples.\n",
    "#                 for correct_index in [j for j, s in enumerate(scores_for_one_question) if s == 1.0]:\n",
    "#                     for incorrect_index in [j for j, s in enumerate(scores_for_one_question) if s == 0.0]:\n",
    "#                         if raw_responses[i][correct_index] is None:\n",
    "#                             continue\n",
    "#                         if raw_responses[i][incorrect_index] is None:\n",
    "#                             continue\n",
    "#                         self.internal_data.append(\n",
    "#                             (indices[i][0], data.subjects[i], prompted_questions[i][0], \n",
    "#                              (answers[i][correct_index], scores[i][correct_index], raw_responses[i][correct_index]), \n",
    "#                              (answers[i][incorrect_index], scores[i][incorrect_index], raw_responses[i][incorrect_index]))\n",
    "#                         )\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.internal_data)\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.internal_data[idx]\n",
    "    \n",
    "# @dataclass\n",
    "# class DPOCollateReturn:\n",
    "#     indices: List[int]\n",
    "#     input_ids: torch.Tensor\n",
    "#     attention_mask: torch.Tensor\n",
    "#     labels: torch.Tensor\n",
    "#     prompted_questions: List[str]\n",
    "#     model_answers: List[str]\n",
    "#     raw_responses: list[tuple[str, str]]\n",
    "#     subjects: List[str]\n",
    "#     def to(self, device):\n",
    "#         def tree_map(dict_obj):\n",
    "#             for key, val in dict_obj.items():\n",
    "#                 if isinstance(val, torch.Tensor):\n",
    "#                     dict_obj[key] = val.to(device)\n",
    "#                 elif isinstance(val, Dict):\n",
    "#                     tree_map(val)\n",
    "#         tree_map(self.__dict__)\n",
    "#         return self\n",
    "# # raw_responses[0][0][4]\n",
    "# # tokenizer.decode(tokenizer(raw_responses[0][1][4]).input_ids, skip_special_tokens=True)\n",
    "\n",
    "# # <|eot_id|> = 128009\n",
    "# # <|finetune_right_pad_id|> = 128004\n",
    "\n",
    "# # concat the input_ids with the tokenized raw_responses, and an eot_id token, then padd the sequences with finetune_right_pad_id on the left.\n",
    "# # and ensure the masks have 0 on those tokens.\n",
    "# def dpo_collate_fn(examples):\n",
    "#     subjects = []\n",
    "#     inputs_to_combine = []\n",
    "#     indices = []\n",
    "#     prompted_questions = []\n",
    "#     model_answers = []\n",
    "#     for example in examples:\n",
    "#         indices.append(example[0])\n",
    "#         subjects.append(example[1])\n",
    "#         prompted_question = example[2]\n",
    "#         prompted_questions.append(prompted_question)\n",
    "#         raw_response_correct = example[3][2]\n",
    "#         raw_response_incorrect = example[4][2]\n",
    "#         model_answers.extend([\n",
    "#             example[3][0],\n",
    "#             example[4][0],\n",
    "#         ])\n",
    "#         inputs_to_combine.extend([ # prompted_question\n",
    "#             [tokenizer(prompted_question, add_special_tokens=False).input_ids, tokenizer(raw_response_correct, add_special_tokens=False).input_ids],\n",
    "#             [tokenizer(prompted_question, add_special_tokens=False).input_ids, tokenizer(raw_response_incorrect, add_special_tokens=False).input_ids]\n",
    "#         ])\n",
    "\n",
    "#     inputs_to_pad = [sum(l, []) for l in inputs_to_combine]\n",
    "#     inputs_padded = []\n",
    "#     max_len_input = max(len(l) for l in inputs_to_pad) \n",
    "#     attention_mask = torch.ones((len(inputs_to_pad), max_len_input), dtype=torch.float32)\n",
    "#     for i, input_to_pad in enumerate(inputs_to_pad):\n",
    "#         inputs_padded.append([128004] * (max_len_input - len(input_to_pad)) + input_to_pad)\n",
    "#         attention_mask[i, :-len(input_to_pad)] = 0\n",
    "#     input_ids = torch.tensor(inputs_padded)\n",
    "#     labels = torch.full_like(input_ids, fill_value=-100)\n",
    "#     for i, input_to_combine in enumerate(inputs_to_combine):\n",
    "#         labels[i,-len(input_to_combine[-1]):] = input_ids[i, -len(input_to_combine[-1]):] # take only the response portion of the answer from the labels\n",
    "#         # print(tokenizer.decode(labels[i,-len(input_to_combine[-1]):]))\n",
    "\n",
    "#     return DPOCollateReturn(\n",
    "#         indices,\n",
    "#         input_ids, \n",
    "#         attention_mask,\n",
    "#         labels,\n",
    "#         prompted_questions,\n",
    "#         model_answers,\n",
    "#         raw_responses,\n",
    "#         subjects\n",
    "#     )\n",
    "dpo_collate_fn = get_dpo_collate_fn(tokenizer)\n",
    "\n",
    "dpo_dataset = DPODataset(data, indices[0], prompted_questions[0], answers[0], raw_responses[0], scores[0])\n",
    "dpo_dataloader = DataLoader(dpo_dataset, batch_size=8, collate_fn=dpo_collate_fn, num_workers=0, pin_memory=True, pin_memory_device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 888])\n",
      "torch.Size([10, 787])\n"
     ]
    }
   ],
   "source": [
    "for i, dpo_data in enumerate(dpo_dataloader):\n",
    "    print(dpo_data.input_ids.shape)\n",
    "    # print(dpo_data.input_ids.device) # cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.384 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "ref_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "def get_nlls_per_batch_element(model, input_ids, attention_mask, labels):\n",
    "    model_output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # need to pass in labels else the computation is wrong???\n",
    "    logits = model_output.logits\n",
    "    # log_softmax_logits = logits.log_softmax(-1)# .__dict__.keys()\n",
    "    # new_labels = labels.detach().clone()\n",
    "    # new_labels[new_labels==-100] = 0\n",
    "    # print(log_softmax_logits[:, -5:, 100:105])\n",
    "    # print(logits.shape)\n",
    "\n",
    "    nlls_per_token = torch.nn.functional.cross_entropy(logits.transpose(-1,-2)[:,:,:-1], labels[:, 1:], reduction='none')\n",
    "    # print(model_output.loss)\n",
    "    # print(nlls_per_token.sum() / (labels!=-100).sum())\n",
    "    # nlls_per_token = torch.where(labels[...,None] != -100, log_softmax_logits[torch.arange(len(labels))[:,None,None],torch.arange(len(labels[0]))[None,:,None], new_labels[..., None]], torch.zeros_like(labels[...,None], dtype=log_softmax_logits.dtype))\n",
    "    # print(nlls_per_token)\n",
    "    return nlls_per_token.flatten(1).sum(1)\n",
    "ref_model = FastLanguageModel.for_training(ref_model)\n",
    "model = FastLanguageModel.for_training(model)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    # use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "from torch.optim.adamw import AdamW\n",
    "# optim = AdamW(params=model.parameters(), lr=0.0002, weight_decay=0.01)\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "max_steps = 20\n",
    "warmup = 5\n",
    "lr = 0.00002\n",
    "weight_decay = 0.01\n",
    "optim = AdamW(model.parameters(), lr, weight_decay=weight_decay)\n",
    "scheduler = LambdaLR(optim, lambda step: (step+1)/warmup if warmup >= (step+1) else 1 - (step+1-warmup)/ (max_steps-warmup))\n",
    "\n",
    "# when given the sum of negative log likelihoods for both the reference model and the training model, we frame the dpo objective:\n",
    "# minimize this: - log sigmoid of (Beta * (   (nll_train_sum(winning) - nll_ref_sum(winning)) \n",
    "#                                           - (nll_train_sum(losing) - nll_ref_sum(losing))     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_example_loss tensor([0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931],\n",
      "       device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([-0., -0., -0., -0., -0., -0., -0., -0.], device='cuda:0')\n",
      "per_example_reward_losing tensor([-0., -0., -0., -0., -0., -0., -0., -0.], device='cuda:0')\n",
      "loss tensor(6.4196, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(0., device='cuda:0')\n",
      "reward_losing tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 8.000000000000001e-06\n",
      "per_example_loss tensor([0.5818, 0.6606, 0.6805, 0.7756, 0.5091, 0.5601, 0.4146, 0.7622],\n",
      "       device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([0.2188, 0.2117, 0.4434, 0.2418, 0.8324, 1.2965, 1.2410, 0.7215],\n",
      "       device='cuda:0')\n",
      "per_example_reward_losing tensor([-0.0179,  0.1454,  0.4180,  0.4004,  0.4227,  1.0100,  0.5750,  0.8551],\n",
      "       device='cuda:0')\n",
      "loss tensor(6.3998, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(0.6509, device='cuda:0')\n",
      "reward_losing tensor(0.4761, device='cuda:0')\n",
      "lr 1.2e-05\n",
      "per_example_loss tensor([0.2349, 0.4718, 0.8225, 1.1702, 0.3442, 0.4195, 0.1149, 1.0528],\n",
      "       device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([1.3311, 1.2043, 2.2848, 1.4088, 3.5781, 4.3977, 4.3372, 3.1314],\n",
      "       device='cuda:0')\n",
      "per_example_reward_losing tensor([2.3010e-03, 6.9836e-01, 2.5287e+00, 2.2074e+00, 2.6887e+00, 3.7461e+00,\n",
      "        2.2311e+00, 3.7551e+00], device='cuda:0')\n",
      "loss tensor(6.3372, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(2.7092, device='cuda:0')\n",
      "reward_losing tensor(2.2322, device='cuda:0')\n",
      "lr 1.6000000000000003e-05\n",
      "per_example_loss tensor([0.0265, 0.2848, 0.9353, 1.8834, 0.0947, 0.1991, 0.0045, 2.7843],\n",
      "       device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([ 3.5641,  3.3488,  5.4289,  3.7061,  9.6746, 10.6006, 11.0348,  7.5598],\n",
      "       device='cuda:0')\n",
      "per_example_reward_losing tensor([-0.0542,  2.2388,  5.8659,  5.4245,  7.3658,  9.0881,  5.6384, 10.2803],\n",
      "       device='cuda:0')\n",
      "loss tensor(6.2108, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(6.8647, device='cuda:0')\n",
      "reward_losing tensor(5.7309, device='cuda:0')\n",
      "lr 2e-05\n",
      "per_example_loss tensor([1.9705e-05, 1.2743e-01, 7.0498e-01, 5.6939e+00, 7.7982e-04, 4.1554e-03,\n",
      "        3.1400e-08, 7.3506e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([10.7970,  9.9801, 15.5811, 10.2162, 30.3121, 31.6867, 34.7589, 22.5473],\n",
      "       device='cuda:0')\n",
      "per_example_reward_losing tensor([-0.0376,  7.9843, 15.6046, 15.9067, 23.1561, 26.2055, 17.4824, 29.8973],\n",
      "       device='cuda:0')\n",
      "loss tensor(5.7891, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(20.7349, device='cuda:0')\n",
      "reward_losing tensor(17.0249, device='cuda:0')\n",
      "lr 1.866666666666667e-05\n",
      "per_example_loss tensor([8.4127e-14, 1.3198e-05, 2.3578e-01, 1.5470e+01, 1.0165e-05, 4.4878e-07,\n",
      "        3.7459e-16, 8.7034e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([30.1295, 24.3359, 43.0506, 22.9107, 75.6948, 85.3502, 82.1656, 55.3699],\n",
      "       device='cuda:0')\n",
      "per_example_reward_losing tensor([2.3047e-02, 1.3101e+01, 4.1726e+01, 3.8380e+01, 6.4198e+01, 7.0733e+01,\n",
      "        4.6645e+01, 6.4073e+01], device='cuda:0')\n",
      "loss tensor(4.8270, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(52.3759, device='cuda:0')\n",
      "reward_losing tensor(42.3599, device='cuda:0')\n",
      "lr 1.7333333333333336e-05\n",
      "per_example_loss tensor([2.8960e-16, 2.5939e-06, 8.1127e-02, 1.7753e+01, 8.1628e-06, 9.6567e-08,\n",
      "        3.5377e-17, 8.1390e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([35.7174, 27.3807, 49.6875, 25.0672, 86.8115, 99.6573, 91.8082, 62.7934],\n",
      "       device='cuda:0')\n",
      "per_example_reward_losing tensor([-6.0657e-02,  1.4518e+01,  4.7217e+01,  4.2820e+01,  7.5096e+01,\n",
      "         8.3504e+01,  5.3928e+01,  7.0932e+01], device='cuda:0')\n",
      "loss tensor(4.5993, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(59.8654, device='cuda:0')\n",
      "reward_losing tensor(48.4942, device='cuda:0')\n",
      "lr 1.6000000000000003e-05\n",
      "per_example_loss tensor([8.1627e-17, 7.9343e-06, 5.6297e-02, 1.9247e+01, 1.4687e-06, 2.9156e-08,\n",
      "        9.6380e-19, 7.6286e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([ 37.0592,  27.0334,  47.7564,  19.2219,  89.9437, 100.9661,  94.1063,\n",
      "         62.2850], device='cuda:0')\n",
      "per_example_reward_losing tensor([1.4804e-02, 1.5289e+01, 4.4908e+01, 3.8469e+01, 7.6513e+01, 8.3616e+01,\n",
      "        5.2623e+01, 6.9913e+01], device='cuda:0')\n",
      "loss tensor(4.6014, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(59.7965, device='cuda:0')\n",
      "reward_losing tensor(47.6681, device='cuda:0')\n",
      "lr 1.466666666666667e-05\n",
      "per_example_loss tensor([3.8304e-17, 1.1735e-05, 5.7803e-02, 1.9667e+01, 5.5792e-07, 1.3408e-08,\n",
      "        1.2268e-19, 8.3792e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([ 37.7621,  27.1004,  47.7080,  16.2633,  92.4884, 102.9089,  95.9524,\n",
      "         62.2057], device='cuda:0')\n",
      "per_example_reward_losing tensor([-3.8870e-02,  1.5747e+01,  4.4886e+01,  3.5930e+01,  7.8089e+01,\n",
      "         8.4781e+01,  5.2408e+01,  7.0585e+01], device='cuda:0')\n",
      "loss tensor(4.5861, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(60.2986, device='cuda:0')\n",
      "reward_losing tensor(47.7986, device='cuda:0')\n",
      "lr 1.3333333333333337e-05\n",
      "per_example_loss tensor([8.3974e-18, 9.6008e-06, 4.7661e-02, 1.9979e+01, 3.3901e-07, 9.7462e-09,\n",
      "        2.3150e-20, 9.2709e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "per_example_reward_winning tensor([ 39.3141,  28.2654,  51.3689,  16.9215,  97.4371, 107.5764,  99.9406,\n",
      "         65.0424], device='cuda:0')\n",
      "per_example_reward_losing tensor([-4.5532e-03,  1.6712e+01,  4.8349e+01,  3.6901e+01,  8.2540e+01,\n",
      "         8.9130e+01,  5.4728e+01,  7.4313e+01], device='cuda:0')\n",
      "loss tensor(4.4969, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_winning tensor(63.2333, device='cuda:0')\n",
      "reward_losing tensor(50.3336, device='cuda:0')\n",
      "lr 1.2e-05\n"
     ]
    }
   ],
   "source": [
    "for _ in range(max_steps // 2):\n",
    "    for i, dpo_data in enumerate(dpo_dataloader):\n",
    "        dpo_data.to('cuda')\n",
    "        input_ids = dpo_data.input_ids\n",
    "        attention_mask = dpo_data.attention_mask\n",
    "        labels = dpo_data.labels\n",
    "        with torch.amp.autocast('cuda', dtype = torch.bfloat16): # type: ignore\n",
    "            with torch.no_grad():\n",
    "                nll_ref_sum = get_nlls_per_batch_element(ref_model, input_ids, attention_mask, labels)\n",
    "            nll_model_sum = get_nlls_per_batch_element(model, input_ids, attention_mask, labels)\n",
    "            # print(\"reference_log_probs\", nll_ref_sum)\n",
    "            # print(\"model_log_probs\", nll_model_sum)\n",
    "            log_ratios = - (nll_model_sum - nll_ref_sum) # negation to make them log p_model(y | x) - log p_ref(y|x)\n",
    "            beta = 0.1\n",
    "            # I want to learning rate on the log_ratios for the losing outcomes to be fractions of what the learning rate is for the winning outcomes.\n",
    "            # to reduce the unlearning of language that I think the model is doing...\n",
    "            winning_log_ratios = log_ratios[0::2]\n",
    "            lr_for_losing = 1 # 0.5\n",
    "            # trick from PLAID repo\n",
    "            losing_log_ratios = log_ratios[1::2] # * lr_for_losing - (1-lr_for_losing) * log_ratios[1::2].detach()\n",
    "            losing_log_ratios = torch.lerp(log_ratios[1::2].detach(), log_ratios[1::2], torch.full_like(log_ratios[1::2], lr_for_losing))\n",
    "            per_example_loss = - torch.nn.functional.logsigmoid(beta * (winning_log_ratios - losing_log_ratios))\n",
    "            print(\"per_example_loss\", per_example_loss)\n",
    "            with torch.no_grad():\n",
    "                print(\"per_example_reward_winning\", beta * log_ratios[0::2].detach())\n",
    "                print(\"per_example_reward_losing\", beta * log_ratios[1::2].detach())\n",
    "                reward_winning = beta * log_ratios[0::2].mean().detach()\n",
    "                reward_losing = beta * log_ratios[1::2].mean().detach()\n",
    "            def at_least_above_starting_likelihood(reward_winning):\n",
    "                starting_reward_winning = 195\n",
    "                if reward_winning > starting_reward_winning:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return (reward_winning - starting_reward_winning) ** 2\n",
    "\n",
    "            # loss = per_example_loss.mean() # + at_least_above_starting_likelihood(reward_winning) * nll_model_sum[0::2].sum() / (labels[0::2] != -100).sum() # 195 is the target winning_log_ratio, we don't want it to change significantly, just for  the losing one to decrease,\n",
    "            loss = nll_model_sum[0::2].sum() / (labels[0::2] != -100).sum() # this is just the nll of the winning\n",
    "            optim.zero_grad()\n",
    "            print(\"loss\", loss)\n",
    "            print(\"reward_winning\", reward_winning)\n",
    "            print(\"reward_losing\", reward_losing)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            print(\"lr\", scheduler.get_lr()[-1])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" 2e-6\\nper_example_loss tensor([0.1769, 0.7056, 0.7860, 1.1896, 0.2740, 0.3690, 0.0771, 1.3104],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.3200, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(3.2745, device='cuda:0')\\nreward_losing tensor(2.7618, device='cuda:0')\\nlr 1.0666666666666667e-06\\nper_example_loss tensor([0.0753, 0.7510, 0.9100, 1.5381, 0.1998, 0.2968, 0.0171, 1.9630],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.2692, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(4.9464, device='cuda:0')\\nreward_losing tensor(4.2506, device='cuda:0')\\nlr 9.333333333333333e-07\\nper_example_loss tensor([0.2209, 0.5862, 0.7866, 1.0701, 0.2874, 0.4070, 0.0800, 1.1888],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.3356, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(2.7613, device='cuda:0')\\nreward_losing tensor(2.2308, device='cuda:0')\\nlr 8e-07\\nper_example_loss tensor([0.1217, 0.3439, 0.8070, 1.2528, 0.2339, 0.2943, 0.0383, 1.4951],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.2976, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(4.0110, device='cuda:0')\\nreward_losing tensor(3.2348, device='cuda:0')\\nlr 6.666666666666667e-07\\nper_example_loss tensor([0.1034, 0.5052, 0.8666, 1.5091, 0.2532, 0.4188, 0.0463, 1.3931],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.2959, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(4.0657, device='cuda:0')\\nreward_losing tensor(3.4538, device='cuda:0')\\nlr 5.333333333333335e-07\\nper_example_loss tensor([0.2068, 0.8919, 0.7838, 1.0834, 0.3545, 0.3607, 0.0928, 1.1233],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.3332, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(2.8400, device='cuda:0')\\nreward_losing tensor(2.3961, device='cuda:0')\\nlr 3.999999999999999e-07\\nper_example_loss tensor([0.1341, 0.5705, 0.7952, 1.2489, 0.2377, 0.3495, 0.0418, 1.5869],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.3052, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(3.7612, device='cuda:0')\\nreward_losing tensor(3.1260, device='cuda:0')\\nlr 2.666666666666666e-07\\nper_example_loss tensor([0.0904, 0.6373, 0.7040, 1.5092, 0.2177, 0.2739, 0.0275, 1.9109],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.2821, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(4.5223, device='cuda:0')\\nreward_losing tensor(3.8237, device='cuda:0')\\nlr 1.333333333333333e-07\\nper_example_loss tensor([0.1403, 1.0152, 0.7521, 1.1772, 0.2628, 0.4361, 0.0416, 1.5009],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.3115, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(3.5555, device='cuda:0')\\nreward_losing tensor(3.0404, device='cuda:0')\\nlr 0.0\\nper_example_loss tensor([0.1647, 0.5987, 0.6951, 1.2873, 0.2678, 0.2757, 0.0629, 1.2808],\\n       device='cuda:0', grad_fn=<NegBackward0>)\\nloss tensor(6.3163, device='cuda:0', grad_fn=<DivBackward0>)\\nreward_winning tensor(3.3954, device='cuda:0')\\nreward_losing tensor(2.7630, device='cuda:0')\\nlr -1.333333333333333e-07\\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" lr 2e-5\n",
    "per_example_loss tensor([0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.4196, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(0., device='cuda:0')\n",
    "reward_losing tensor(0., device='cuda:0')\n",
    "/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
    "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
    "lr 8.000000000000001e-06\n",
    "per_example_loss tensor([0.3729, 0.8844, 0.6936, 0.9513, 0.3934, 0.5777, 0.2208, 1.1831],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.3758, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(1.4396, device='cuda:0')\n",
    "reward_losing tensor(1.2477, device='cuda:0')\n",
    "/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
    "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
    "lr 1.2e-05\n",
    "per_example_loss tensor([0.2886, 0.7477, 0.7138, 1.0217, 0.4901, 0.4463, 0.1894, 1.1161],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.3563, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(2.0798, device='cuda:0')\n",
    "reward_losing tensor(1.7981, device='cuda:0')\n",
    "lr 1.6000000000000003e-05\n",
    "per_example_loss tensor([0.0327, 0.4647, 1.0329, 1.7709, 0.0926, 0.1801, 0.0047, 2.4679],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.2211, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(6.5264, device='cuda:0')\n",
    "reward_losing tensor(5.4418, device='cuda:0')\n",
    "lr 2e-05\n",
    "per_example_loss tensor([2.2541e-05, 7.5633e-02, 6.3770e-01, 5.7448e+00, 6.8654e-04, 3.6790e-03,\n",
    "        3.6067e-08, 7.3011e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(5.7938, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(20.5793, device='cuda:0')\n",
    "reward_losing tensor(16.7867, device='cuda:0')\n",
    "lr 1.866666666666667e-05\n",
    "per_example_loss tensor([6.3413e-14, 9.9693e-06, 2.3529e-01, 1.5682e+01, 8.9800e-06, 2.6246e-07,\n",
    "        4.1075e-16, 8.3276e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(4.8036, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(53.1437, device='cuda:0')\n",
    "reward_losing tensor(42.9655, device='cuda:0')\n",
    "lr 1.7333333333333336e-05\n",
    "per_example_loss tensor([3.5968e-16, 3.2037e-06, 9.6646e-02, 1.7802e+01, 9.3569e-06, 7.8808e-08,\n",
    "        3.4973e-17, 8.2159e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(4.6007, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(59.8182, device='cuda:0')\n",
    "reward_losing tensor(48.5294, device='cuda:0')\n",
    "lr 1.6000000000000003e-05\n",
    "per_example_loss tensor([8.8611e-17, 6.3471e-06, 5.6361e-02, 1.9219e+01, 1.4521e-06, 2.5486e-08,\n",
    "        1.0330e-18, 7.5853e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(4.6112, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(59.4730, device='cuda:0')\n",
    "reward_losing tensor(47.3086, device='cuda:0')\n",
    "lr 1.466666666666667e-05\n",
    "per_example_loss tensor([4.2993e-17, 1.1542e-05, 6.2179e-02, 1.9579e+01, 6.1631e-07, 1.8443e-08,\n",
    "        1.2576e-19, 8.2817e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(4.5878, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(60.2411, device='cuda:0')\n",
    "reward_losing tensor(47.7949, device='cuda:0')\n",
    "lr 1.3333333333333337e-05\n",
    "per_example_loss tensor([7.7578e-18, 1.3197e-05, 5.7235e-02, 1.9767e+01, 3.0771e-07, 1.1345e-08,\n",
    "        2.4984e-20, 9.3163e+00], device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(4.5038, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(63.0057, device='cuda:0')\n",
    "reward_losing tensor(50.1548, device='cuda:0')\n",
    "lr 1.2e-05\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" 2e-6\n",
    "per_example_loss tensor([0.1769, 0.7056, 0.7860, 1.1896, 0.2740, 0.3690, 0.0771, 1.3104],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.3200, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(3.2745, device='cuda:0')\n",
    "reward_losing tensor(2.7618, device='cuda:0')\n",
    "lr 1.0666666666666667e-06\n",
    "per_example_loss tensor([0.0753, 0.7510, 0.9100, 1.5381, 0.1998, 0.2968, 0.0171, 1.9630],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.2692, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(4.9464, device='cuda:0')\n",
    "reward_losing tensor(4.2506, device='cuda:0')\n",
    "lr 9.333333333333333e-07\n",
    "per_example_loss tensor([0.2209, 0.5862, 0.7866, 1.0701, 0.2874, 0.4070, 0.0800, 1.1888],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.3356, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(2.7613, device='cuda:0')\n",
    "reward_losing tensor(2.2308, device='cuda:0')\n",
    "lr 8e-07\n",
    "per_example_loss tensor([0.1217, 0.3439, 0.8070, 1.2528, 0.2339, 0.2943, 0.0383, 1.4951],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.2976, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(4.0110, device='cuda:0')\n",
    "reward_losing tensor(3.2348, device='cuda:0')\n",
    "lr 6.666666666666667e-07\n",
    "per_example_loss tensor([0.1034, 0.5052, 0.8666, 1.5091, 0.2532, 0.4188, 0.0463, 1.3931],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.2959, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(4.0657, device='cuda:0')\n",
    "reward_losing tensor(3.4538, device='cuda:0')\n",
    "lr 5.333333333333335e-07\n",
    "per_example_loss tensor([0.2068, 0.8919, 0.7838, 1.0834, 0.3545, 0.3607, 0.0928, 1.1233],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.3332, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(2.8400, device='cuda:0')\n",
    "reward_losing tensor(2.3961, device='cuda:0')\n",
    "lr 3.999999999999999e-07\n",
    "per_example_loss tensor([0.1341, 0.5705, 0.7952, 1.2489, 0.2377, 0.3495, 0.0418, 1.5869],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.3052, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(3.7612, device='cuda:0')\n",
    "reward_losing tensor(3.1260, device='cuda:0')\n",
    "lr 2.666666666666666e-07\n",
    "per_example_loss tensor([0.0904, 0.6373, 0.7040, 1.5092, 0.2177, 0.2739, 0.0275, 1.9109],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.2821, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(4.5223, device='cuda:0')\n",
    "reward_losing tensor(3.8237, device='cuda:0')\n",
    "lr 1.333333333333333e-07\n",
    "per_example_loss tensor([0.1403, 1.0152, 0.7521, 1.1772, 0.2628, 0.4361, 0.0416, 1.5009],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.3115, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(3.5555, device='cuda:0')\n",
    "reward_losing tensor(3.0404, device='cuda:0')\n",
    "lr 0.0\n",
    "per_example_loss tensor([0.1647, 0.5987, 0.6951, 1.2873, 0.2678, 0.2757, 0.0629, 1.2808],\n",
    "       device='cuda:0', grad_fn=<NegBackward0>)\n",
    "loss tensor(6.3163, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "reward_winning tensor(3.3954, device='cuda:0')\n",
    "reward_losing tensor(2.7630, device='cuda:0')\n",
    "lr -1.333333333333333e-07\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"temp_model_dpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nA man who was emotionally upset due to marital problems voluntarily checked into a private medical facility for treatment. State law provided that if he tried to leave against medical advice, the hospital could hold him for 72 hours while trying to convince him to stay. During the stay, he told his medical providers that he was intensely angry at his wife, who was living with another man, and he wanted to harm her. Despite that knowledge, the hospital issued him an unrestricted weekend pass. When he got out he shot and killed his wife. The man's children sued the hospital for negligence. The hospital filed a motion to dismiss mainly because there was no privity with the decedent and no duty owed to her. Will the court grant the motion to dismiss?\\n\\nA) No, because the hospital is strictly liable for releasing patients who it knows or should know represent an unreasonable danger to third persons.\\nB) No, because the special relationship and the control between doctor and an in-patient mandates due care by the doctor to prevent a foreseeable danger to others.\\nC) Yes, because there is no duty to control the conduct of another person to prevent that person from harming others.\\nD) Yes, because the intervening acts of third parties are unforeseeable and cannot give rise to liability.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(prompted_questions[0][4][0], add_special_tokens=False).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationCollateReturn(indices=[13635, 11067, 991, 10159, 11792, 10178, 2198, 10135], input_ids=tensor([[128004, 128004, 128004,  ...,  78191, 128007,    271],\n",
      "        [128000, 128000, 128006,  ...,  78191, 128007,    271],\n",
      "        [128004, 128004, 128004,  ...,  78191, 128007,    271],\n",
      "        ...,\n",
      "        [128004, 128004, 128004,  ...,  78191, 128007,    271],\n",
      "        [128004, 128004, 128004,  ...,  78191, 128007,    271],\n",
      "        [128004, 128004, 128004,  ...,  78191, 128007,    271]]), attention_mask=tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), input_strs=[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nIn general, how do conservatives and liberals differ when it comes to defense spending?\\n\\nA) Conservatives are skeptical of increases in defense spending; liberals advocate increases in defense spending.\\nB) Conservatives favor social spending over defense spending; liberals favor defense spending over social spending.\\nC) Conservatives advocate increases in defense spending; liberals are generally skeptical of increases in defense spending.\\nD) Conservatives emphasize strong economies; liberals emphasize strong military readiness.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: \\'Answer: $LETTER\\' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nA city has adopted the following ordinance which provides:\"Section 1: It shall be unlawful for any person, group, or organization to hold a meeting of 50 persons or more in any city park without first securing a city permit;Section 2: The application shall specify the day and hours for which the permit is sought. The fee shall be $10 per hour, with a maximum fee of $50;Section 3: Permits shall be issued on a first come basis; provided that the chief of police shall deny any application if, after hearing the applicant, it is his considered judgment that the meeting would create serious traffic congestion. \"An anarchist planned to hold an antigovernment protest demonstration at a city square. Although his previous antigovernment protest rallies attracted fewer than 15 demonstrators, he decided to apply for a permit pursuant to the city ordinance. After meeting with the anarchist, the chief of police denied his permit application because he believed that the demonstration would incite the protestors and threaten imminent violence or serious disorder. On October 4, the anarchist and his fellow demonstrators staged their antigovemment protest at a city park. The rally attracted only about 17 protestors and was conducted peacefully. As the anarchist was making his final remarks to the gathering, the city police arrived at the park and arrested him and his fellow demonstrators, charging them with violating the city ordinance. In a subsequent action by the anarchist in state court challenging the constitutionality of Sections 1 and 2 of the city ordinance, the court will most likely rule these sections\\n\\nA) constitutional, because a state may regulate the communicative impact of speech related conduct.\\nB) constitutional, because the aforementioned sections are within the ambit of the state\\'s police power.\\nC) unconstitutional, because the statute is vague and overbroad.\\nD) unconstitutional, because the statute is a content based regulation ofspeech.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nOf the following atoms, which has the lowest electron affinity?\\n\\nA) F\\nB) Si\\nC) O\\nD) Ca<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: \\'Answer: $LETTER\\' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nThe attitude of many ancient elites toward the consumption of resources seems to have been:\\n\\nA) â€œa penny saved is a penny earned.\"\\nB) \"if you\\'ve got it, flaunt it.\"\\nC) \"look before you leap.\"\\nD) \"the last shall be first and the first shall be last.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nA man who was emotionally upset due to marital problems voluntarily checked into a private medical facility for treatment. State law provided that if he tried to leave against medical advice, the hospital could hold him for 72 hours while trying to convince him to stay. During the stay, he told his medical providers that he was intensely angry at his wife, who was living with another man, and he wanted to harm her. Despite that knowledge, the hospital issued him an unrestricted weekend pass. When he got out he shot and killed his wife. The man's children sued the hospital for negligence. The hospital filed a motion to dismiss mainly because there was no privity with the decedent and no duty owed to her. Will the court grant the motion to dismiss?\\n\\nA) No, because the hospital is strictly liable for releasing patients who it knows or should know represent an unreasonable danger to third persons.\\nB) No, because the special relationship and the control between doctor and an in-patient mandates due care by the doctor to prevent a foreseeable danger to others.\\nC) Yes, because there is no duty to control the conduct of another person to prevent that person from harming others.\\nD) Yes, because the intervening acts of third parties are unforeseeable and cannot give rise to liability.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nMayan city-states were NOT:\\n\\nA) ruled by autonomous dynasties.\\nB) similar in their political organization to Egyptian communities.\\nC) supported by large agricultural hinterlands.\\nD) much smaller in population than Mohanjo-daro.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nThe schedule for a music showcase includes 3 sets that are 20 minutes each and 1 set that is 40 minutes. There is a 10-minute break between each set. The total length of the music showcase is 3(20 + 10) + 40 minutes. What is the total length of the music showcase?\\n\\nA) 73 minutes\\nB) 110 minutes\\nC) 130 minutes\\nD) 210 minutes<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nWhich of the following sites has monumental earthworks with concentric ridges in a semicircle that date from around 3,500 years ago?\\n\\nA) Caral\\nB) Stonehenge\\nC) Poverty Point\\nD) Gobekli Tepe<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"], subjects=['us_foreign_policy', 'professional_law', 'college_chemistry', 'prehistory', 'professional_law', 'prehistory', 'elementary_mathematics', 'prehistory'], answers=['C', 'B', 'D', 'B', 'B', 'B', 'C', 'C'])\n"
     ]
    }
   ],
   "source": [
    "for data in dl:\n",
    "    print(data)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 899])\n",
      "['C']\n",
      "To answer this question, let's analyze the general perspectives of conservatives and liberals on defense spending.\n",
      "\n",
      "1. **Understanding Conservative Views on Defense Spending:**\n",
      "   - Generally, conservatives tend to be strong supporters of a robust military and national defense. They often believe that a strong military is essential for the country's security and global influence.\n",
      "   - Many conservatives advocate for increases in defense spending to ensure the military has the necessary resources to maintain its capabilities and protect national interests.\n",
      "   - They might be skeptical of reductions in defense spending, especially if they believe it could weaken the country's position on the global stage.\n",
      "\n",
      "2. **Understanding Liberal Views on Defense Spending:**\n",
      "   - Liberals often prioritize domestic spending and social welfare programs over defense spending. They may be more concerned with issues like healthcare, education, and economic inequality.\n",
      "   - While liberals support a strong national defense, they may be more cautious about increases in defense spending, especially if they perceive it as diverting funds from more pressing domestic needs.\n",
      "   - Liberals might advocate for more strategic or targeted increases in defense spending rather than blanket increases.\n",
      "\n",
      "Considering these perspectives, the statement that best describes the general difference between conservatives and liberals on defense spending is:\n",
      "\n",
      "**C) Conservatives advocate increases in defense spending; liberals are generally skeptical of increases in defense spending.**\n",
      "\n",
      "This option accurately reflects the typical views of conservatives favoring increases in defense spending and liberals being more cautious about such increases, often preferring to allocate funds to other areas.\n",
      "\n",
      "Answer: C<|eot_id|>\n",
      "[1.0]\n",
      "['D']\n",
      "To determine the most likely ruling of the court, we need to analyze the city ordinance in the context of First Amendment rights. The ordinance requires a permit for gatherings of 50 persons or more in city parks, with a fee structure and a provision for the chief of police to deny permits if the meeting would create serious traffic congestion.\n",
      "\n",
      "Option A is incorrect because the city ordinance is not regulating the communicative impact of speech-related conduct; it's regulating the physical gathering itself, which is a form of expressive conduct protected by the First Amendment.\n",
      "\n",
      "Option B is also incorrect because the police power is broad, but it is limited by the First Amendment. The ordinance is not within the state's police power because it burdens speech, which is protected by the First Amendment.\n",
      "\n",
      "Option C is incorrect because the ordinance is not vague or overbroad in the context of the facts given. The ordinance clearly states the conditions under which a permit must be secured and the grounds upon which the chief of police can deny a permit.\n",
      "\n",
      "Option D is the most plausible because the ordinance is a content-based regulation of speech. It singles out gatherings of 50 persons or more, which could include expressive activities like protests or demonstrations. The First Amendment generally prohibits content-based regulations of speech, unless they can be narrowly tailored to serve a compelling government interest.\n",
      "\n",
      "Answer: D<|eot_id|>\n",
      "[0.0]\n",
      "['D']\n",
      "To determine which atom has the lowest electron affinity, we need to understand what electron affinity is. Electron affinity is the energy change when an electron is added to an atom in its gaseous state. It is a measure of how strongly an atom attracts electrons.\n",
      "\n",
      "Let's analyze each option:\n",
      "\n",
      "A) Fluorine (F) has a high electron affinity because it has a strong tendency to attract electrons due to its high electronegativity and the small size of its atoms, which makes it difficult for an electron to be added.\n",
      "\n",
      "B) Silicon (Si) is a metalloid and has a relatively low electron affinity compared to the other options, but still higher than some other metals.\n",
      "\n",
      "C) Oxygen (O) has a moderate electron affinity, but it is not as high as that of fluorine due to its larger size and relatively lower electronegativity compared to fluorine.\n",
      "\n",
      "D) Calcium (Ca) is a metal with a very low electron affinity because it has a large size and a low electronegativity, making it difficult for it to attract an electron.\n",
      "\n",
      "Given the options, the atom with the lowest electron affinity would be the metal, which in this case is Calcium.\n",
      "\n",
      "Answer: D<|eot_id|>\n",
      "[1.0]\n",
      "['B']\n",
      "To answer this question, let's consider the context of ancient elites and their consumption patterns.\n",
      "\n",
      "First, option A, \"a penny saved is a penny earned,\" suggests a frugal and cautious approach to money management, which doesn't align with the typical attitude of elites who often have more than enough resources. This option doesn't fit the behavior of ancient elites.\n",
      "\n",
      "Option B, \"if you've got it, flaunt it,\" suggests an extravagant and ostentatious approach to consumption, which is consistent with the behavior of many ancient elites who often showcased their wealth and status through lavish spending and displays of luxury.\n",
      "\n",
      "Option C, \"look before you leap,\" suggests caution and careful consideration, which again doesn't match the typical behavior of elites who often engage in conspicuous consumption and luxury spending.\n",
      "\n",
      "Option D, \"the last shall be first and the first shall be last,\" is a quote from the Bible and doesn't describe a general attitude toward consumption but rather a principle of social hierarchy.\n",
      "\n",
      "Considering the above, the most fitting description of the attitude of many ancient elites toward the consumption of resources is an emphasis on flaunting their wealth and status.\n",
      "\n",
      "Answer: B<|eot_id|>\n",
      "[1.0]\n",
      "['C']\n",
      "To answer this question, let's break it down step by step:\n",
      "\n",
      "1. **Understanding the scenario**: The man, who is a patient in a hospital due to emotional distress related to marital problems, expresses a desire to harm his wife. Despite this, the hospital grants him an unrestricted weekend pass.\n",
      "\n",
      "2. **Identifying the key issue**: The key issue here is whether the hospital has a duty to prevent harm to the man's wife, a third party, given his expressed intent to harm her.\n",
      "\n",
      "3. **Considering the legal principles**:\n",
      "   - **Strict Liability (A)**: This concept usually applies to situations where the defendant's action directly causes harm, regardless of intent or negligence. While the hospital's decision to release the man might be seen as a negligent act, strict liability typically doesn't apply to negligence cases.\n",
      "   - **Special Relationship and Control (B)**: This principle might suggest a duty on the part of the hospital (or its medical staff) to ensure the man does not harm others, especially if they have a special relationship with him (e.g., a doctor-patient relationship) and have control over his actions (e.g., by holding him in the facility). However, the key issue is whether this relationship imposes a duty to control his actions to prevent harm to a third party.\n",
      "   - **Duty to Control (C)**: Generally, there is no duty to control the conduct of another person to prevent them from harming others. This principle is key in this scenario, as the hospital's duty is to treat and care for the man as a patient, not to prevent him from causing harm to third parties.\n",
      "   - **Intervening Acts of Third Parties (D)**: This principle might suggest that the hospital could argue the wife's death was caused by the man's intervening act, rather than any action or inaction by the hospital. However, this does not address the hospital's duty directly.\n",
      "\n",
      "4. **Conclusion**: Given the general principle that there is no duty to control the conduct of another to prevent harm to third parties, the hospital's action (or inaction) of releasing the man, while potentially negligent, does not create a legal duty to prevent the man from harming his wife.\n",
      "\n",
      "Answer: C<|eot_id|>\n",
      "[0.0]\n",
      "['B']\n",
      "To answer this question, let's analyze each option step by step:\n",
      "\n",
      "A) Ruled by autonomous dynasties: Mayan city-states were indeed known for their complex systems of governance, often involving powerful dynasties that ruled over the city-states. This is consistent with the way they were organized, so this option does not seem to be a characteristic that Mayan city-states were NOT known for.\n",
      "\n",
      "B) Similar in their political organization to Egyptian communities: The political organization of Mayan city-states and ancient Egyptian communities were quite different. Egyptian communities were typically organized into a centralized, monarchic system with a strong emphasis on the pharaoh's divine right to rule. Mayan city-states, on the other hand, were more decentralized and featured a complex system of governance with multiple ruling dynasties, some of which might have been more powerful than others. This option seems plausible as a characteristic that Mayan city-states were NOT similar to Egyptian communities in terms of political organization.\n",
      "\n",
      "C) Supported by large agricultural hinterlands: Mayan city-states were indeed supported by large agricultural hinterlands. The Mayans developed sophisticated agricultural systems, including terracing and irrigation, which allowed them to support large populations in their city-states. This option does not seem to be a characteristic that Mayan city-states were NOT known for.\n",
      "\n",
      "D) Much smaller in population than Mohanjo-daro: Mohanjo-daro is a city in ancient India, not directly comparable to Mayan city-states in terms of population size or geographical context. However, without specific population figures, it's challenging to definitively say that Mayan city-states were generally smaller than Mohanjo-daro. This comparison is less directly relevant to the question of what Mayan city-states were NOT known for, making this option less clear-cut.\n",
      "\n",
      "Considering the above analysis, the best answer to the question is the option that describes a characteristic that Mayan city-states were NOT similar to or known for.\n",
      "\n",
      "Answer: B<|eot_id|>\n",
      "[1.0]\n",
      "['C']\n",
      "To find the total length of the music showcase, we need to calculate the total time taken by the 3 sets that are 20 minutes each and the 1 set that is 40 minutes, and then add the time taken by the breaks.\n",
      "\n",
      "First, let's calculate the total time taken by the 3 sets that are 20 minutes each:\n",
      "3 sets * 20 minutes/set = 60 minutes\n",
      "\n",
      "Next, let's add the time taken by the 1 set that is 40 minutes:\n",
      "60 minutes + 40 minutes = 100 minutes\n",
      "\n",
      "Now, let's calculate the total time taken by the breaks:\n",
      "There are 2 breaks between the 3 sets (before the first set and after the third set), so the total break time is 2 * 10 minutes = 20 minutes\n",
      "\n",
      "Finally, let's add the total time taken by the sets and the breaks:\n",
      "100 minutes + 20 minutes = 120 minutes\n",
      "\n",
      "However, the question mentions that the total length of the music showcase is 3(20 + 10) + 40 minutes. Let's calculate this expression:\n",
      "3(20 + 10) = 3(30) = 90 minutes\n",
      "Adding the 40-minute set: 90 minutes + 40 minutes = 130 minutes\n",
      "\n",
      "Therefore, the total length of the music showcase is 130 minutes.\n",
      "\n",
      "Answer: C<|eot_id|>\n",
      "[1.0]\n",
      "['C']\n",
      "To answer this question, let's consider each option step by step:\n",
      "\n",
      "A) Caral: Caral is an archaeological site in Peru that dates back around 2,000-1,800 BCE. It is known for its large earthen mounds and ceremonial architecture, but it's not specifically known for concentric ridges in a semicircle.\n",
      "\n",
      "B) Stonehenge: Stonehenge is a prehistoric monument in England that dates back to around 3,000-2,000 BCE. It is famous for its circular arrangement of large standing stones, but it's not specifically known for monumental earthworks with concentric ridges.\n",
      "\n",
      "C) Poverty Point: Poverty Point is an archaeological site in Louisiana, USA, that dates back around 3,500-2,500 BCE. It is known for its unique earthworks, including concentric ridges in a semicircle, which are believed to have been used for ceremonial or ritual purposes.\n",
      "\n",
      "D) Gobekli Tepe: Gobekli Tepe is an archaeological site in Turkey that dates back around 11,000-10,000 years ago. It is known for its circular and oval-shaped structures, but it's not specifically known for monumental earthworks with concentric ridges.\n",
      "\n",
      "Considering the specific feature mentioned in the question, the correct answer is the site that is known for its earthworks with concentric ridges in a semicircle.\n",
      "\n",
      "Answer: C<|eot_id|>\n",
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "for i, output in enumerate(tokenizer.batch_decode(outputs[:, data.input_ids.shape[1]:])):\n",
    "    try:\n",
    "        raw_answer = output[:output.index(\"<|eot_id|>\") + len(\"<|eot_id|>\")]\n",
    "        response_text = normalize_response(raw_answer)\n",
    "        extracted_answer = None\n",
    "        for answer_regex in MULTILINGUAL_ANSWER_REGEXES:\n",
    "            regex = MULTILINGUAL_ANSWER_PATTERN_TEMPLATE.format(answer_regex)\n",
    "            match = re.search(regex, response_text)\n",
    "            if match:\n",
    "                extracted_answer = normalize_extracted_answer(match.group(1))\n",
    "                break\n",
    "        score = 1.0 if extracted_answer == data.answers[i] else 0.0\n",
    "        print([extracted_answer])# answers.append(extracted_answer)\n",
    "        print(raw_answer)# raw_responses.append(raw_answer)\n",
    "        print([score])# scores.append(score)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(None) # answers.append(None)\n",
    "        print(None) # raw_responses.append(None)\n",
    "        print(0.0) # scores.append(0.0)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump((indices, prompted_questions, answers, raw_responses, scores), open(\"temp_output_generate.pkl\", \"wb\"))\n",
    "# pickle.dump((indices, prompted_questions, answers, raw_responses, scores), open(\"temp_output_generate_post_fine_tune.pkl\", \"wb\"))\n",
    "(indices_gen, prompted_questions_gen, answers_gen, raw_responses_gen, scores_gen) = pickle.load(open(\"temp_output_generate.pkl\", \"rb\"))\n",
    "# (indices, prompted_questions, answers, raw_responses, scores) = pickle.load(open(\"temp_output_generate.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices, prompted_questions, answers, raw_responses, scores = indices_gen, prompted_questions_gen, answers_gen, raw_responses_gen, scores_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indices)):\n",
    "    if scores[i] != scores_gen[i]: # corrected 10, uncorrected 7.\n",
    "        if scores_gen[i] == 1.0:\n",
    "            print(0)\n",
    "            # print(f\"pre {scores_gen[i]:<3} -> post {scores[i]:<3}\")\n",
    "            # print(f\"question index={indices[i]} c_answer={df.iloc[indices[i]]['Answer']}\\n{prompted_questions[i]}\")\n",
    "            # assert prompted_questions[i] == prompted_questions_gen[i]\n",
    "            # print(f\"PRE {raw_responses_gen[i]}\")\n",
    "            # print(f\"POST {raw_responses[i]}\")\n",
    "            pass\n",
    "        else:\n",
    "            print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.DataFrame(data=dict(zip([\"indices\", \"prompted_questions\", \"answers\", \"raw_responses\", \"scores\"],[indices, prompted_questions, answers, raw_responses, scores]))).to_csv(\"temp_output_generate.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for rational from correct stuff\n",
    "class RationalDataset(Dataset):\n",
    "    def __init__(self, raw_responses, scores, prompted_questions):\n",
    "        # to instantiate this object I need to be able to get out the rational, which mean I need to somehow remove the answer from the generation process? Or isn't the answer regexed from the rational generation process? so I just need to reinforce the output created by the assistant to the last prompt?\n",
    "        # for understanding the dataset, I just need to do simple preprocessing, the tokenization and stuff can be done in the collate function.\n",
    "        # so, which answers do I need?\n",
    "        self.examples = []\n",
    "        for raw_response, prompted_question, score in zip(raw_responses, prompted_questions, scores):\n",
    "            if score == 1.0: # answers are already filtered to have the <|eot_id|> to be at their end\n",
    "                self.examples.append((raw_response, prompted_question))       \n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "@dataclass\n",
    "class ReinforceCollateReturn:\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor # specifically made to only reinforce the output string, till the last <|eot_id|> after the assistant. and if that didn't occur in the generation, then??? I shouldn't reinforce it...\n",
    "    labels: torch.Tensor\n",
    "    def to(self, device):\n",
    "        def tree_map(dict_obj):\n",
    "            for key, val in dict_obj.items():\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    dict_obj[key] = val.to(device)\n",
    "                elif isinstance(val, Dict):\n",
    "                    tree_map(val)\n",
    "        tree_map(self.__dict__)\n",
    "        return self\n",
    "\n",
    "def reinforce_collate_fn(examples):\n",
    "    full_question_and_responses = []\n",
    "    raw_responses = []\n",
    "    prompted_questions = []\n",
    "    for example in examples:\n",
    "        raw_response, prompted_question = example\n",
    "        raw_responses.append(raw_response)\n",
    "        prompted_questions.append(prompted_question)\n",
    "        full_question_and_response = prompted_question + raw_response\n",
    "        # print(full_question_and_response)\n",
    "        full_question_and_responses.append(full_question_and_response)\n",
    "    tokenized_conversations = tokenizer(full_question_and_responses, padding=True, return_tensors='pt')\n",
    "    input_ids = tokenized_conversations[\"input_ids\"]\n",
    "    attention_mask = tokenized_conversations[\"attention_mask\"]\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    labels = input_ids.clone()\n",
    "    labels[attention_mask!=1] = -100\n",
    "    for i in range(len(labels)):\n",
    "        end_of_header_id = 128007 # when we get the last header, we get the beggining of the response.\n",
    "        index_of_response_start = torch.where(labels[i] == end_of_header_id)[0][-1].item() + 1\n",
    "        labels[i, :index_of_response_start] = -100\n",
    "    \n",
    "    return ReinforceCollateReturn(\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        labels\n",
    "    )\n",
    "reinforce_dataset = RationalDataset(raw_responses, scores, prompted_questions)\n",
    "dl_reinforce = DataLoader(reinforce_dataset, batch_size=8, collate_fn=reinforce_collate_fn, num_workers=0, pin_memory=True, pin_memory_device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m reinforce_dataset_hf \u001b[38;5;241m=\u001b[39m HFDataset\u001b[38;5;241m.\u001b[39mfrom_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_dataset_reinforce.pk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mreinforce_dataset_hf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mid\u001b[39m)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m reinforce_dataset_hf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      3\u001b[0m reinforce_dataset_hf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mid\u001b[39m)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m reinforce_dataset_hf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      4\u001b[0m reinforce_dataset_hf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mid\u001b[39m)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m reinforce_dataset_hf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Dataset' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# from datasets import Dataset as HFDataset\n",
    "# reinforce_dataset_hf = HFDataset.from_dict({k: v.split(1, 0) for k, v in reinforce_collate_fn(reinforce_dataset.examples).__dict__.items()})\n",
    "# reinforce_dataset_hf = HFDataset.from_parquet(\"hf_dataset_reinforce.pk\")\n",
    "# reinforce_dataset_hf[\"input_ids\"] = [torch.tensor(id).long() for id in reinforce_dataset_hf[\"input_ids\"]]\n",
    "# reinforce_dataset_hf[\"attention_mask\"] = [torch.tensor(id).long() for id in reinforce_dataset_hf[\"attention_mask\"]]\n",
    "# reinforce_dataset_hf[\"labels\"] = [torch.tensor(id).long() for id in reinforce_dataset_hf[\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2903, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.for_training(model)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    # use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "# from transformers import TrainingArguments\n",
    "# from unsloth import is_bfloat16_supported\n",
    "# from trl import IterativeSFTTrainer\n",
    "# training_args = TrainingArguments(\n",
    "#     per_device_train_batch_size = 8,\n",
    "#     gradient_accumulation_steps = 1,\n",
    "#     warmup_steps = 5,\n",
    "#     # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "#     max_steps = 60,\n",
    "#     learning_rate = 2e-4,\n",
    "#     fp16 = not is_bfloat16_supported(),\n",
    "#     bf16 = is_bfloat16_supported(),\n",
    "#     logging_steps = 1,\n",
    "#     optim = \"adamw_torch\", #\"adamw_8bit\",\n",
    "#     weight_decay = 0.01,\n",
    "#     lr_scheduler_type = \"linear\",\n",
    "#     seed = 3407,\n",
    "#     output_dir = \"outputs\",\n",
    "# )\n",
    "# trainer = IterativeSFTTrainer(model=model, args=training_args, tokenizer=tokenizer)\n",
    "for data in dl_reinforce:\n",
    "    # print(data)\n",
    "    data.to(\"cuda\")\n",
    "    print(model(**data.__dict__).loss)\n",
    "    # trainer.step(**data.__dict__)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A40. Max memory = 44.384 GB.\n",
      "36.557 GB of memory reserved.\n",
      "max_steps=60, len(dl_reinforce)=11, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=4e-05 loss=0.2697512209415436\n",
      "lr=8e-05 loss=0.24127373099327087\n",
      "lr=0.00012 loss=0.34221377968788147\n",
      "lr=0.00016 loss=0.2776520550251007\n",
      "lr=0.0002 loss=0.27513203024864197\n",
      "lr=0.00019636363636363636 loss=0.24998000264167786\n",
      "lr=0.00019272727272727274 loss=0.23653851449489594\n",
      "lr=0.0001890909090909091 loss=0.15436550974845886\n",
      "lr=0.00018545454545454545 loss=0.18819601833820343\n",
      "lr=0.00018181818181818183 loss=0.20563490688800812\n",
      "lr=0.0001781818181818182 loss=0.16134823858737946\n",
      "33.31220769882202 seconds used for training.\n",
      "0.56 minutes used for training.\n",
      "Peak reserved memory = 36.557 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 82.365 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.for_training(model)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    # use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    warmup_steps = 5,\n",
    "    # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_torch\", #\"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = \"outputs\",\n",
    ")\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     train_dataset = reinforce_dataset_hf,\n",
    "#     # dataset_text_field = \"input_ids\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "#     data_collator = reinforce_collate_fn,\n",
    "#     dataset_num_proc = 2,\n",
    "#     packing = False, # Can make training 5x faster for short sequences.\n",
    "#     dataset_kwargs={'skip_prepare_dataset': True},\n",
    "#     args = training_args,\n",
    "# )\n",
    "\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "\n",
    "import time\n",
    "train_runtime = time.time()\n",
    "# trainer_stats = trainer.train() # 04:07, Epoch 0/1\n",
    "# need a scheduler for 5 warmup and linear decay.\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.adamw import AdamW\n",
    "\n",
    "max_steps = max(60, len(dl_reinforce))\n",
    "warmup = 5\n",
    "lr = 0.0002\n",
    "weight_decay = 0.01\n",
    "optim = AdamW(model.parameters(), lr, weight_decay=weight_decay)\n",
    "scheduler = LambdaLR(optim, lambda step: step/warmup if warmup >= step else 1 - (step-warmup)/ (max_steps-warmup))\n",
    "# def peft_module_casting_to_bf16(model):\n",
    "#     for name, module in model.named_modules():\n",
    "#         if isinstance(module, torch.nn.LayerNorm) or \"norm\" in name:\n",
    "#             module = module.to(torch.float32)\n",
    "#         elif any(x in name for x in [\"lm_head\", \"embed_tokens\", \"wte\", \"wpe\"]):\n",
    "#             if hasattr(module, \"weight\"):\n",
    "#                 if module.weight.dtype == torch.float32:\n",
    "#                     module = module.to(torch.bfloat16)\n",
    "# # peft_module_casting_to_bf16(model)\n",
    "print(f\"{max_steps=}, {len(dl_reinforce)=}, batch_size={dl_reinforce.batch_size}\")\n",
    "for data in dl_reinforce:\n",
    "    with torch.autocast('cuda', dtype = torch.bfloat16):\n",
    "        data.to(\"cuda\")\n",
    "        optim.zero_grad()\n",
    "        loss = model(**data.__dict__).loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        print(f\"lr={scheduler.get_lr()[-1]} loss={loss.item()}\") \n",
    "train_runtime = time.time() - train_runtime\n",
    "\n",
    "\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{train_runtime} seconds used for training.\")\n",
    "print(f\"{round(train_runtime/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'D', 'B', 'D', 'D', 'C', 'D', 'C', 'B', 'D', 'C', 'C', 'B', 'D', 'D', 'D', 'A', 'D', 'B', 'C', 'A', 'A', 'C', 'A', 'A', 'B', 'B', 'C', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'D', 'D', 'A', 'D', 'A', 'C', 'A', 'B', 'B', 'D', 'B', 'A', 'D', 'C', 'C', 'A', 'B', 'B', 'D', 'B', 'C', 'A', 'D', 'A', 'A', 'D', 'C', 'A', 'B', 'C', 'C', 'C', 'B', 'A', 'B', 'D', 'A', 'D', 'A', 'A', 'D', 'D', 'D', 'C', 'B', 'C', 'C', 'C', 'D', 'D', 'D', 'B', 'B', 'D', 'D', 'C', 'C', 'B', 'C', 'D', 'C', 'C', 'D', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'C', 'A', 'A', 'C', 'D', 'B', 'B', 'D', 'D', 'D', 'B', 'B', 'C', 'D', 'A', 'A', 'C', 'B', 'C', 'C', 'D', 'A']\n",
      "['D', 'B', 'C', 'D', 'C', 'D', 'A', 'C', 'A', 'D', 'A', 'D', 'C', 'B', 'D', 'D', 'D', 'C', 'A', 'B', 'C', 'C', 'B', 'B', 'B', 'A', 'B', 'C', 'A', 'D', 'C', 'D', 'D', 'B', 'C', 'D', 'B', 'D', 'D', 'A', 'B', 'A', 'A', 'D', 'C', 'C', 'C', 'C', 'D', 'D', 'A', 'C', 'B', 'B', 'C', None, 'D', 'C', 'B', 'C', 'D', 'A', 'B', 'C', 'B', 'D', 'D', 'D', 'C', 'D', 'C', 'D', 'C', 'D', 'A', 'D', 'B', 'A', 'B', 'A', 'C', 'B', 'B', 'D', 'B', 'B', 'B', 'A', 'B', 'C', 'A', 'B', 'D', 'C', 'D', 'A', 'C', 'D', 'D', 'C', 'C', 'D', 'D', 'C', 'C', 'D', 'A', 'D', 'C', 'D', 'C', 'B', 'A', 'A', 'C', 'A', 'A', 'C', 'C', 'B', None, 'D', 'C', 'C', 'C', 'D', 'B', 'C', 'A', 'A', 'D', 'B', 'C', 'A', 'C', 'B', 'C', 'A', 'D', 'A', 'A', 'A', 'C', 'A', 'A', 'B', 'C', 'B', 'D', 'A', 'A', 'C', 'B', 'C', 'C', 'B', 'B', 'D', 'C', 'C', 'A', 'D', 'B', 'C', 'B', 'C', 'B', 'B', 'B', 'B', 'C', 'A', 'C', 'B', 'D', 'D', 'A', 'B', 'A', 'C', 'C', 'B', 'B', None, 'C', 'B', 'B', 'D', 'D', 'D', None, 'C', 'D', 'B', 'A', 'B', 'A', 'D', 'A', 'D', 'C', 'A', 'B', 'C', 'D', 'A', 'C', 'D', 'D', 'D', 'C', 'D', 'C', 'A', 'D', 'C', 'C', 'C', 'C', 'A', 'C', 'B', 'A', 'D', 'A', 'D', 'C', 'B', 'A', 'C', 'B', 'A', 'A', 'C', 'C', 'A', 'B', 'D', 'D', 'D', 'D', 'D', 'A', 'B', 'B', 'D', 'D', 'D', 'C', 'C', 'D', 'B', 'D', 'B', 'C', 'C', 'C', 'B', 'D', 'B', 'C', 'B', 'C', 'C', 'B', 'C', 'B', 'C', 'D', 'A', 'D', 'D', 'C', 'A', 'B', 'B', 'C', 'C', 'C', 'B', 'A', None, 'B', 'D', 'C', 'C', 'A', 'C', 'D', None, None, None, 'C', 'D', 'D', 'B', 'D', 'A', 'D', 'B', 'C', 'D', 'D', 'B', 'D', 'D', 'D', None, 'C', 'C', 'C', 'D', 'D', 'C', 'C', 'D', 'B', None, 'D', 'B', 'B', 'C', 'C', 'C', 'A', 'B', 'C', 'B', 'B', 'A', 'B', 'B', 'D', 'D', 'C', 'D', 'C', 'A', 'B', 'C', 'A', 'A', 'D', 'D', 'D', 'D', 'B', 'C', 'B', 'A', 'C', 'C', 'B', 'D', 'A', 'A', 'C', None, 'D', 'A', 'C', 'B', None, 'D', 'D', 'D', 'D', 'D', 'A', 'A', 'C', 'C', 'D', 'C', 'C', 'C', 'C', 'A', 'B', 'B', 'B', 'C', 'D', 'C']\n",
      "0.6536458333333334\n"
     ]
    }
   ],
   "source": [
    "print(data.answers)\n",
    "print(answers)\n",
    "print(sum(scores) / len(scores)) \n",
    "# 0.671875 for standard prompt so far, \n",
    "# 0.625 for \"Let's think through this carefully, step by step.\" in assistant prediction.\n",
    "# 0.609375 I reprompted it with the knowledge cutoff and a date instead of just \"You are a helpful assistant.\"\n",
    "# 0.625 for \"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n\"\n",
    "# 0.640625 for Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are a helpful assistant\n",
    "# 0.625 for \"You are a helpful assistant.\"???? I wanted this to be the .67????\n",
    "# 0.671875 for \"You are a helpful assistant\" without the period, which is crazy...\n",
    "\n",
    "# 0.65625 on 128 examples now, and without finetuning at all\n",
    "# 0.6796875 finetuning on 84 correct examples lr 0.0002, weight decay 0.01, and so on.... does weight decay try to decay the weight of the non peft parameters????\n",
    "# 0.6536458333333334 on 128 * 3\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.384 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca106c7cd4a400a9298fc01e4bd400d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")\n",
    "\n",
    "model = FastLanguageModel.for_training(model)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    # use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch\", #adamw_8bit\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b9303d1896442ab8b256905ab0bcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                \\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A40. Max memory = 44.384 GB.\n",
      "5.984 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/60 00:07 < 03:43, 0.25 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.766300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 04:07, Epoch 0/1\u001b[39;00m\n",
      "File \u001b[0;32m<string>:145\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train() # 04:07, Epoch 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272.7349 seconds used for training.\n",
      "4.55 minutes used for training.\n",
      "Peak reserved memory = 11.969 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 26.967 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do you feel about migrant crime?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI'm designed to provide information and answers based on the data I've been trained on. I don't have personal feelings or opinions. Migrant crime can be a complex topic, often involving sensitive issues like immigration, national security, and societal cohesion.\\n\\nMigrant crime refers to the involvement of migrants or asylum seekers in various\"]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How do you feel about migrant crime?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment with rwkv because I want faster inference for rollouts in search of most probable sequence\n",
    "\n",
    "need to test speed of comparably sized models? or just use this small model for rational generation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "`attention_mask` was passed, but it is unused in this model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "The researchers found that the dragons were able to communicate with each other, and that they were\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_rnn = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-169m-pile\", device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n",
    "\n",
    "prompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "output = model_rnn.generate(inputs[\"input_ids\"], max_new_tokens=20)\n",
    "print(tokenizer.decode(output[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "The researchers found that the dragons were able to communicate with each other, and that they were able to communicate with each other.\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "output = model_rnn.generate(**tokenizer(prompt, return_tensors=\"pt\").to('cuda'), max_new_tokens=30)\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some DPO experiments on other datasets to ensure our DPO implementation isn't wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.384 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\", # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    # load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Alignment Handbook utils\n",
    "import os\n",
    "import re\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "from datasets.builder import DatasetGenerationError\n",
    "\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    example, tokenizer, task: Literal[\"sft\", \"generation\", \"rm\", \"dpo\"] = \"sft\", assistant_prefix=\"<|assistant|>\\n\"\n",
    "):\n",
    "    def _strip_prefix(s, pattern):\n",
    "        # Use re.escape to escape any special characters in the pattern\n",
    "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
    "\n",
    "    if task in [\"sft\", \"generation\"]:\n",
    "        messages = example[\"messages\"]\n",
    "        # We add an empty system message if there is none\n",
    "        if messages[0][\"role\"] != \"system\":\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "        example[\"text\"] = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True if task == \"generation\" else False\n",
    "        )\n",
    "    elif task == \"rm\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            chosen_messages = example[\"chosen\"]\n",
    "            rejected_messages = example[\"rejected\"]\n",
    "            # We add an empty system message if there is none\n",
    "            if chosen_messages[0][\"role\"] != \"system\":\n",
    "                chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            if rejected_messages[0][\"role\"] != \"system\":\n",
    "                rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    elif task == \"dpo\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
    "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
    "            # Insert system message\n",
    "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            else:\n",
    "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
    "            # TODO: handle case where chosen/rejected also have system messages\n",
    "            chosen_messages = example[\"chosen\"][1:]\n",
    "            rejected_messages = example[\"rejected\"][1:]\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
    "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
    "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Task {task} not supported, please ensure that the provided task is one of {['sft', 'generation', 'rm', 'dpo']}\"\n",
    "        )\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "    data_config: dict,\n",
    "    splits: List[str] = [\"train\", \"test\"],\n",
    "    shuffle: bool = True,\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads one or more datasets with varying training set proportions.\n",
    "\n",
    "    Args:\n",
    "        data_config (`DataArguments` or `dict`):\n",
    "            Dataset configuration and split proportions.\n",
    "        splits (`List[str]`, *optional*, defaults to `['train', 'test']`):\n",
    "            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.\n",
    "        shuffle (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to shuffle the training and testing/validation data.\n",
    "\n",
    "    Returns\n",
    "        [`DatasetDict`]: The dataset dictionary containing the loaded datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data_config) is dict:\n",
    "        # Structure of the input is:\n",
    "        #     dataset_mixer = {\n",
    "        #             \"dataset1\": 0.5,\n",
    "        #             \"dataset1\": 0.3,\n",
    "        #             \"dataset1\": 0.2,\n",
    "        #         }\n",
    "        dataset_mixer = data_config\n",
    "    else:\n",
    "        raise ValueError(f\"Data config {data_config} not recognized.\")\n",
    "\n",
    "    raw_datasets = mix_datasets(dataset_mixer, splits=splits, shuffle=shuffle)\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def mix_datasets(dataset_mixer: dict, splits: Optional[List[str]] = None, shuffle=True) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads and mixes datasets according to proportions specified in `dataset_mixer`.\n",
    "\n",
    "    Args:\n",
    "        dataset_mixer (`dict`):\n",
    "            Dictionary containing the dataset names and their training proportions. By default, all test proportions are 1.\n",
    "        splits (Optional[List[str]], *optional*, defaults to `None`):\n",
    "            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.\n",
    "        shuffle (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to shuffle the training and testing/validation data.\n",
    "    \"\"\"\n",
    "    raw_datasets = DatasetDict()\n",
    "    raw_train_datasets = []\n",
    "    raw_val_datasets = []\n",
    "    fracs = []\n",
    "    for ds, frac in dataset_mixer.items():\n",
    "        fracs.append(frac)\n",
    "        for split in splits:\n",
    "            try:\n",
    "                # Try first if dataset on a Hub repo\n",
    "                dataset = load_dataset(ds, split=split)\n",
    "            except DatasetGenerationError:\n",
    "                # If not, check local dataset\n",
    "                dataset = load_from_disk(os.path.join(ds, split))\n",
    "\n",
    "            if \"train\" in split:\n",
    "                raw_train_datasets.append(dataset)\n",
    "            elif \"test\" in split:\n",
    "                raw_val_datasets.append(dataset)\n",
    "            else:\n",
    "                raise ValueError(f\"Split type {split} not recognized as one of test or train.\")\n",
    "\n",
    "    if any(frac < 0 for frac in fracs):\n",
    "        raise ValueError(\"Dataset fractions cannot be negative.\")\n",
    "\n",
    "    if len(raw_train_datasets) > 0:\n",
    "        train_subsets = []\n",
    "        for dataset, frac in zip(raw_train_datasets, fracs):\n",
    "            train_subset = dataset.select(range(int(frac * len(dataset))))\n",
    "            train_subsets.append(train_subset)\n",
    "        if shuffle:\n",
    "            raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n",
    "        else:\n",
    "            raw_datasets[\"train\"] = concatenate_datasets(train_subsets)\n",
    "    # No subsampling for test datasets to enable fair comparison across models\n",
    "    if len(raw_val_datasets) > 0:\n",
    "        if shuffle:\n",
    "            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(seed=42)\n",
    "        else:\n",
    "            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)\n",
    "\n",
    "    if len(raw_datasets) == 0:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {dataset_mixer} not recognized with split {split}. Check the dataset has been correctly formatted.\"\n",
    "        )\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380e9b9ce7a1497eb3a2a90e1abef328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting comparisons with prompt template (num_proc=12):   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d290156371e94c08a535d5fbbc0fc09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting comparisons with prompt template (num_proc=12):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = get_datasets(\n",
    "    {\"HuggingFaceH4/ultrafeedback_binarized\" : 0.005}, # 0.5% sampled\n",
    "    splits = [\"train_prefs\", \"test_prefs\"],\n",
    ")\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs = {\"tokenizer\": tokenizer, \"task\": \"dpo\"},\n",
    "    num_proc = 12,\n",
    "    remove_columns = column_names,\n",
    "    desc = \"Formatting comparisons with prompt template\",\n",
    ")\n",
    "\n",
    "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
    "for split in [\"train\", \"test\"]:\n",
    "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
    "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIn this metaphor, Thoreau is suggesting that if a person doesn't conform or fit in with others around them, it might be because they have their own unique path and aspirations. He encourages the individual to follow their own internal guidance and passions, no matter how different or distant it may seem from the norm.\\n\\nThoreau's metaphors can still apply to readers today, as it highlights the importance of individuality and staying true to oneself in a world that often values conformity.<|eot_id|>\",\n",
       " 'rejected': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThoreau is comparing a person who marches to the beat of their own drummer to a band that plays at a different tempo. He suggests that if someone is falling behind their peers, it might be because they are listening to a different internal rhythm. He encourages them to follow their own internal beat rather than trying to force themselves to keep up with others. Whether or not this metaphor applies to readers today depends on their individual lifestyles and personalities. Some people may feel more comfortable following their own internal rhythm, while others may prefer to march to the beat of society's drum.<|eot_id|>\",\n",
       " 'prompt': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThoreauâ€™s metaphors are highly visual. Though theyâ€™re clever and original, they arenâ€™t far-fetched. Thoreau takes his comparisons from nature and from other things with which he and his audience are familiar. To be sure you understand Thoreauâ€™s figures of speech, paraphrase the following metaphor. Do his metaphors apply to readers today?\\n\\nâ€œIf a man does not keep pace with his companions, perhaps it is because he hears a different drummer. Let him step to the music which he hears, however measured or far away.â€<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 26 Jul 2024\\n'\n",
      " '\\n'\n",
      " '<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Describe a possible solution to the environmental issue of air '\n",
      " 'pollution.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n')\n",
      "('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 26 Jul 2024\\n'\n",
      " '\\n'\n",
      " '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'One of the most effective solutions to the environmental issue of air '\n",
      " 'pollution is promoting and investing in renewable energy sources. '\n",
      " 'Traditional energy sources like coal and oil produce large amounts of '\n",
      " 'greenhouse gases that contribute greatly to air pollution. Renewable energy '\n",
      " 'sources like wind, solar, and hydropower sources do not produce greenhouse '\n",
      " 'gases. They use clean energy sources that do not harm the environment and do '\n",
      " 'not contribute to air pollution. \\n'\n",
      " '\\n'\n",
      " 'Another solution is improving our public transportation systems. Encouraging '\n",
      " 'individuals to use public transportation, cycling, walking, or carpooling '\n",
      " 'instead of their personal vehicles can greatly reduce emissions. This is '\n",
      " 'especially helpful in urban areas where traffic congestion and subsequent '\n",
      " 'air pollution is a common issue. \\n'\n",
      " '\\n'\n",
      " 'Reducing the use of single-use plastics can also be a significant step in '\n",
      " 'reducing air pollution. The production, transportation, and disposal of '\n",
      " 'plastic products contributes a large amount to greenhouse gases and air '\n",
      " 'pollution. Using reusable bags, bottles, and containers can greatly reduce '\n",
      " 'the amount of plastic waste and in turn, pollution.\\n'\n",
      " '\\n'\n",
      " 'Implementing stricter regulations and penalties on industries that heavily '\n",
      " 'pollute can also be a solution. Cleaner production practices can be '\n",
      " 'enforced, and those who fail to comply can be penalized, raising awareness '\n",
      " 'of the consequences of air pollution.\\n'\n",
      " '\\n'\n",
      " 'Finally, increasing research and development in clean technologies can lead '\n",
      " 'to innovative solutions to air pollution. This includes technologies that '\n",
      " 'remove pollutants from the air, more efficient energy storage systems for '\n",
      " 'renewable energy, and more effective waste management systems.\\n'\n",
      " '\\n'\n",
      " 'These solutions, when implemented in a systematic manner, can go a long way '\n",
      " 'in reducing air pollution and protecting our environment.<|eot_id|>')\n",
      "('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 26 Jul 2024\\n'\n",
      " '\\n'\n",
      " '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " \"Hello! As a helpful and respectful assistant, I'm here to provide you with \"\n",
      " 'information and answers that are safe, accurate, and positive. I understand '\n",
      " \"the importance of addressing environmental issues, and I'm happy to help you \"\n",
      " 'explore possible solutions to air pollution.\\n'\n",
      " '\\n'\n",
      " 'Air pollution is a pressing environmental issue that affects not only the '\n",
      " 'environment but also human health. It can cause respiratory problems, '\n",
      " 'cardiovascular diseases, and even cancer. To address this issue, there are '\n",
      " 'several possible solutions that can be implemented at the individual, '\n",
      " 'community, and government levels.\\n'\n",
      " '\\n'\n",
      " 'One possible solution is to use cleaner energy sources such as wind, solar, '\n",
      " 'and hydroelectric power. These sources of energy are renewable and produce '\n",
      " 'minimal air pollution compared to fossil fuels. Additionally, we can reduce '\n",
      " 'our energy consumption by using energy-efficient appliances and turning off '\n",
      " 'lights and electronics when not in use.\\n'\n",
      " '\\n'\n",
      " 'Another solution is to improve public transportation and encourage the use '\n",
      " 'of electric or hybrid vehicles. This can reduce the number of cars on the '\n",
      " 'road, which in turn can reduce air pollution. Governments can also encourage '\n",
      " 'the use of public transportation by investing in infrastructure and '\n",
      " 'providing incentives for using eco-friendly vehicles.\\n'\n",
      " '\\n'\n",
      " 'Moreover, we can reduce air pollution by reducing waste and properly '\n",
      " 'disposing of hazardous materials. We can also promote sustainable '\n",
      " 'agriculture practices that reduce the use of harmful chemicals and promote '\n",
      " 'soil health.\\n'\n",
      " '\\n'\n",
      " \"Lastly, it's important to raise awareness about air pollution and its \"\n",
      " 'effects on human health and the environment. Education is key to promoting '\n",
      " 'behavior change and encouraging individuals to take action to reduce air '\n",
      " 'pollution.\\n'\n",
      " '\\n'\n",
      " 'Overall, addressing air pollution requires a multi-faceted approach that '\n",
      " 'involves individuals, communities, and governments working together. By '\n",
      " 'implementing these possible solutions, we can work towards creating a '\n",
      " 'healthier and more sustainable future for all.<|eot_id|>')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "row = raw_datasets[\"train\"][8]\n",
    "pprint.pprint(row[\"prompt\"])\n",
    "pprint.pprint(row[\"chosen\"])\n",
    "pprint.pprint(row[\"rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Currently only supports dropout = 0\n",
    "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    # use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62788d132dd449ebbe8c69c198d16889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 5e-6,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        # do_eval=True,\n",
    "        # eval_on_start=True,\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = raw_datasets[\"train\"],\n",
    "    # eval_dataset = raw_datasets[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 1024,\n",
    "    max_prompt_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 305 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 114\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjakobbbjorner\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nethome/jbjorner3/dev/hallucination-fun/star/scratch/wandb/run-20241118_224229-va2a8460</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jakobbbjorner/huggingface/runs/va2a8460' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/jakobbbjorner/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jakobbbjorner/huggingface' target=\"_blank\">https://wandb.ai/jakobbbjorner/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jakobbbjorner/huggingface/runs/va2a8460' target=\"_blank\">https://wandb.ai/jakobbbjorner/huggingface/runs/va2a8460</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='83' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 83/114 11:47 < 04:30, 0.11 it/s, Epoch 2.14/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-274.151184</td>\n",
       "      <td>-323.857513</td>\n",
       "      <td>-0.050662</td>\n",
       "      <td>-0.322402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-379.080048</td>\n",
       "      <td>-389.128784</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>-0.154219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>0.035881</td>\n",
       "      <td>0.049972</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>-393.368744</td>\n",
       "      <td>-463.013794</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>-0.024824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>-0.002527</td>\n",
       "      <td>-0.045335</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.042807</td>\n",
       "      <td>-394.607178</td>\n",
       "      <td>-347.304077</td>\n",
       "      <td>-0.290850</td>\n",
       "      <td>-0.362606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681700</td>\n",
       "      <td>-0.005103</td>\n",
       "      <td>-0.029131</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.024028</td>\n",
       "      <td>-305.079865</td>\n",
       "      <td>-302.711487</td>\n",
       "      <td>-0.094871</td>\n",
       "      <td>-0.117073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.701600</td>\n",
       "      <td>-0.071465</td>\n",
       "      <td>-0.056546</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.014919</td>\n",
       "      <td>-519.314026</td>\n",
       "      <td>-563.393616</td>\n",
       "      <td>-0.306572</td>\n",
       "      <td>-0.253723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>0.027589</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.012236</td>\n",
       "      <td>-409.270630</td>\n",
       "      <td>-337.999756</td>\n",
       "      <td>-0.397736</td>\n",
       "      <td>-0.397742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>-0.012338</td>\n",
       "      <td>-0.020446</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>-336.084534</td>\n",
       "      <td>-250.110245</td>\n",
       "      <td>-0.091475</td>\n",
       "      <td>-0.031448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>-0.007598</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.011904</td>\n",
       "      <td>-292.233032</td>\n",
       "      <td>-419.987183</td>\n",
       "      <td>-0.060607</td>\n",
       "      <td>-0.244648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.728100</td>\n",
       "      <td>-0.022511</td>\n",
       "      <td>0.045232</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.067743</td>\n",
       "      <td>-358.177155</td>\n",
       "      <td>-489.047852</td>\n",
       "      <td>0.042983</td>\n",
       "      <td>-0.162099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.708400</td>\n",
       "      <td>-0.010098</td>\n",
       "      <td>0.019565</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.029663</td>\n",
       "      <td>-348.526215</td>\n",
       "      <td>-364.755676</td>\n",
       "      <td>-0.132899</td>\n",
       "      <td>-0.161549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>-0.063459</td>\n",
       "      <td>-0.074602</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>-414.567017</td>\n",
       "      <td>-518.648804</td>\n",
       "      <td>0.433654</td>\n",
       "      <td>0.260409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>-0.067110</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>-408.732086</td>\n",
       "      <td>-367.277954</td>\n",
       "      <td>-0.406494</td>\n",
       "      <td>-0.217080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.686400</td>\n",
       "      <td>-0.000580</td>\n",
       "      <td>-0.015290</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>-275.953278</td>\n",
       "      <td>-377.604004</td>\n",
       "      <td>-0.009360</td>\n",
       "      <td>-0.282456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>-0.050218</td>\n",
       "      <td>-0.058247</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>-404.189789</td>\n",
       "      <td>-334.633789</td>\n",
       "      <td>-0.085196</td>\n",
       "      <td>-0.065744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.702900</td>\n",
       "      <td>-0.085048</td>\n",
       "      <td>-0.067873</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.017175</td>\n",
       "      <td>-498.848755</td>\n",
       "      <td>-511.758789</td>\n",
       "      <td>0.129482</td>\n",
       "      <td>0.056219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.734900</td>\n",
       "      <td>-0.094348</td>\n",
       "      <td>-0.013095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.081252</td>\n",
       "      <td>-416.032593</td>\n",
       "      <td>-470.297516</td>\n",
       "      <td>-0.026527</td>\n",
       "      <td>-0.131530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>-0.135923</td>\n",
       "      <td>-0.149622</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>-526.823120</td>\n",
       "      <td>-574.717773</td>\n",
       "      <td>-0.099857</td>\n",
       "      <td>-0.202293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>-0.136402</td>\n",
       "      <td>-0.151120</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.014718</td>\n",
       "      <td>-376.553650</td>\n",
       "      <td>-373.435303</td>\n",
       "      <td>-0.320239</td>\n",
       "      <td>-0.263057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.041527</td>\n",
       "      <td>-0.043656</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.085183</td>\n",
       "      <td>-440.280121</td>\n",
       "      <td>-402.471527</td>\n",
       "      <td>-0.069526</td>\n",
       "      <td>0.049746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>-0.085495</td>\n",
       "      <td>-0.136273</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.050778</td>\n",
       "      <td>-459.334778</td>\n",
       "      <td>-421.447357</td>\n",
       "      <td>-0.227842</td>\n",
       "      <td>-0.075135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.715500</td>\n",
       "      <td>-0.148252</td>\n",
       "      <td>-0.108487</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.039764</td>\n",
       "      <td>-377.908325</td>\n",
       "      <td>-340.920074</td>\n",
       "      <td>0.057733</td>\n",
       "      <td>-0.074885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.712300</td>\n",
       "      <td>-0.154849</td>\n",
       "      <td>-0.119371</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.035479</td>\n",
       "      <td>-394.784027</td>\n",
       "      <td>-411.969482</td>\n",
       "      <td>0.184284</td>\n",
       "      <td>0.053342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>-0.011991</td>\n",
       "      <td>-0.026076</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>-220.269577</td>\n",
       "      <td>-248.277039</td>\n",
       "      <td>-0.404857</td>\n",
       "      <td>-0.542563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>-0.050902</td>\n",
       "      <td>-0.133576</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.082674</td>\n",
       "      <td>-387.088959</td>\n",
       "      <td>-507.810364</td>\n",
       "      <td>0.031773</td>\n",
       "      <td>-0.171120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>-0.179840</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.011995</td>\n",
       "      <td>-464.705627</td>\n",
       "      <td>-447.499451</td>\n",
       "      <td>-0.030083</td>\n",
       "      <td>-0.081224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>-0.088703</td>\n",
       "      <td>-0.181205</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.092503</td>\n",
       "      <td>-448.454010</td>\n",
       "      <td>-489.573334</td>\n",
       "      <td>0.036651</td>\n",
       "      <td>-0.084497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.684700</td>\n",
       "      <td>-0.227147</td>\n",
       "      <td>-0.245869</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>-392.538574</td>\n",
       "      <td>-441.606812</td>\n",
       "      <td>0.233673</td>\n",
       "      <td>0.102971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.716900</td>\n",
       "      <td>-0.160302</td>\n",
       "      <td>-0.129706</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.030596</td>\n",
       "      <td>-513.491821</td>\n",
       "      <td>-451.430725</td>\n",
       "      <td>-0.237104</td>\n",
       "      <td>-0.265431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>-0.206885</td>\n",
       "      <td>-0.273152</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.066267</td>\n",
       "      <td>-426.122803</td>\n",
       "      <td>-509.725067</td>\n",
       "      <td>0.004781</td>\n",
       "      <td>-0.144470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.726700</td>\n",
       "      <td>-0.199693</td>\n",
       "      <td>-0.134750</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.064943</td>\n",
       "      <td>-303.510406</td>\n",
       "      <td>-295.073181</td>\n",
       "      <td>-0.143769</td>\n",
       "      <td>-0.161042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>-0.276446</td>\n",
       "      <td>-0.275589</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-464.866516</td>\n",
       "      <td>-330.263916</td>\n",
       "      <td>-0.160320</td>\n",
       "      <td>0.096564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>-0.089472</td>\n",
       "      <td>-0.160234</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.070762</td>\n",
       "      <td>-403.014069</td>\n",
       "      <td>-374.901520</td>\n",
       "      <td>-0.513849</td>\n",
       "      <td>-0.512193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>-0.242843</td>\n",
       "      <td>-0.256997</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>-455.504852</td>\n",
       "      <td>-388.096466</td>\n",
       "      <td>-0.314802</td>\n",
       "      <td>-0.314831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.645200</td>\n",
       "      <td>-0.089386</td>\n",
       "      <td>-0.196637</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.107251</td>\n",
       "      <td>-317.710022</td>\n",
       "      <td>-399.514832</td>\n",
       "      <td>0.251790</td>\n",
       "      <td>0.128740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.639400</td>\n",
       "      <td>-0.172780</td>\n",
       "      <td>-0.288106</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.115325</td>\n",
       "      <td>-472.016724</td>\n",
       "      <td>-502.516541</td>\n",
       "      <td>-0.080321</td>\n",
       "      <td>-0.425385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.715900</td>\n",
       "      <td>-0.329994</td>\n",
       "      <td>-0.287318</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.042677</td>\n",
       "      <td>-415.796814</td>\n",
       "      <td>-366.368500</td>\n",
       "      <td>0.137256</td>\n",
       "      <td>0.131423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.616300</td>\n",
       "      <td>-0.130951</td>\n",
       "      <td>-0.300060</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.169109</td>\n",
       "      <td>-368.590057</td>\n",
       "      <td>-377.935730</td>\n",
       "      <td>0.037028</td>\n",
       "      <td>0.083008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>-0.363598</td>\n",
       "      <td>-0.427785</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.064187</td>\n",
       "      <td>-437.126892</td>\n",
       "      <td>-419.339417</td>\n",
       "      <td>-0.258002</td>\n",
       "      <td>-0.445171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.589300</td>\n",
       "      <td>-0.310121</td>\n",
       "      <td>-0.538692</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>-393.984711</td>\n",
       "      <td>-390.609863</td>\n",
       "      <td>-0.145919</td>\n",
       "      <td>-0.193694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.627700</td>\n",
       "      <td>-0.274229</td>\n",
       "      <td>-0.417467</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.143238</td>\n",
       "      <td>-455.715942</td>\n",
       "      <td>-392.016418</td>\n",
       "      <td>-0.165207</td>\n",
       "      <td>-0.168235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>-0.246467</td>\n",
       "      <td>-0.477590</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.231123</td>\n",
       "      <td>-425.385986</td>\n",
       "      <td>-526.766602</td>\n",
       "      <td>-0.268051</td>\n",
       "      <td>-0.296044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>-0.311339</td>\n",
       "      <td>-0.464767</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.153428</td>\n",
       "      <td>-272.223267</td>\n",
       "      <td>-226.559387</td>\n",
       "      <td>-0.431026</td>\n",
       "      <td>-0.426297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>-0.173092</td>\n",
       "      <td>-0.471768</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.298676</td>\n",
       "      <td>-314.812958</td>\n",
       "      <td>-259.808502</td>\n",
       "      <td>-0.205494</td>\n",
       "      <td>-0.101388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.593500</td>\n",
       "      <td>-0.220958</td>\n",
       "      <td>-0.459930</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.238972</td>\n",
       "      <td>-455.513062</td>\n",
       "      <td>-465.042114</td>\n",
       "      <td>0.161485</td>\n",
       "      <td>0.005671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.616500</td>\n",
       "      <td>-0.248822</td>\n",
       "      <td>-0.432886</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.184065</td>\n",
       "      <td>-481.791504</td>\n",
       "      <td>-496.701569</td>\n",
       "      <td>-0.088560</td>\n",
       "      <td>-0.069651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>-0.260758</td>\n",
       "      <td>-0.435537</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.174780</td>\n",
       "      <td>-329.241821</td>\n",
       "      <td>-455.651337</td>\n",
       "      <td>0.187148</td>\n",
       "      <td>-0.261297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.769700</td>\n",
       "      <td>-0.455244</td>\n",
       "      <td>-0.322831</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.132412</td>\n",
       "      <td>-399.354980</td>\n",
       "      <td>-489.504944</td>\n",
       "      <td>-0.014672</td>\n",
       "      <td>-0.148347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.577400</td>\n",
       "      <td>-0.436641</td>\n",
       "      <td>-0.713040</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.276399</td>\n",
       "      <td>-381.912109</td>\n",
       "      <td>-479.484955</td>\n",
       "      <td>-0.211231</td>\n",
       "      <td>-0.401036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.618000</td>\n",
       "      <td>-0.290580</td>\n",
       "      <td>-0.496633</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.206053</td>\n",
       "      <td>-263.791077</td>\n",
       "      <td>-233.580414</td>\n",
       "      <td>-0.094721</td>\n",
       "      <td>-0.256855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>-0.379290</td>\n",
       "      <td>-0.403745</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.024455</td>\n",
       "      <td>-392.799042</td>\n",
       "      <td>-415.341064</td>\n",
       "      <td>-0.130330</td>\n",
       "      <td>-0.214501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>-0.217669</td>\n",
       "      <td>-0.318854</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.101185</td>\n",
       "      <td>-360.788757</td>\n",
       "      <td>-414.215363</td>\n",
       "      <td>0.183719</td>\n",
       "      <td>0.020330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.569400</td>\n",
       "      <td>-0.321589</td>\n",
       "      <td>-0.632055</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.310466</td>\n",
       "      <td>-430.440857</td>\n",
       "      <td>-431.140930</td>\n",
       "      <td>-0.126287</td>\n",
       "      <td>-0.116852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>-0.634727</td>\n",
       "      <td>-0.747659</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.112932</td>\n",
       "      <td>-397.521973</td>\n",
       "      <td>-530.528442</td>\n",
       "      <td>-0.164826</td>\n",
       "      <td>-0.213904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>-0.443781</td>\n",
       "      <td>-0.790703</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.346922</td>\n",
       "      <td>-394.281067</td>\n",
       "      <td>-490.736511</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>-0.170369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.615500</td>\n",
       "      <td>-0.451294</td>\n",
       "      <td>-0.638308</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.187014</td>\n",
       "      <td>-422.488312</td>\n",
       "      <td>-490.565063</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>-0.026416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.640100</td>\n",
       "      <td>-0.378325</td>\n",
       "      <td>-0.529314</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.150989</td>\n",
       "      <td>-296.321136</td>\n",
       "      <td>-297.377289</td>\n",
       "      <td>0.013990</td>\n",
       "      <td>0.016063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>-0.760079</td>\n",
       "      <td>-0.799327</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.039247</td>\n",
       "      <td>-579.813538</td>\n",
       "      <td>-547.340576</td>\n",
       "      <td>-0.047039</td>\n",
       "      <td>-0.064421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>-0.535404</td>\n",
       "      <td>-0.715514</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.180110</td>\n",
       "      <td>-319.515747</td>\n",
       "      <td>-290.352631</td>\n",
       "      <td>-0.293651</td>\n",
       "      <td>-0.216856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>-0.761424</td>\n",
       "      <td>-1.069698</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.308275</td>\n",
       "      <td>-441.683350</td>\n",
       "      <td>-361.566132</td>\n",
       "      <td>-0.155939</td>\n",
       "      <td>-0.168119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>-0.492426</td>\n",
       "      <td>-0.952632</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.460206</td>\n",
       "      <td>-370.931915</td>\n",
       "      <td>-373.323425</td>\n",
       "      <td>0.175038</td>\n",
       "      <td>0.005710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>-0.514542</td>\n",
       "      <td>-0.702220</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.187677</td>\n",
       "      <td>-433.404480</td>\n",
       "      <td>-312.829437</td>\n",
       "      <td>-0.264555</td>\n",
       "      <td>0.056747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>-0.709086</td>\n",
       "      <td>-1.181640</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.472554</td>\n",
       "      <td>-371.096252</td>\n",
       "      <td>-362.378540</td>\n",
       "      <td>-0.158520</td>\n",
       "      <td>-0.117868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.661100</td>\n",
       "      <td>-0.696611</td>\n",
       "      <td>-0.797861</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.101250</td>\n",
       "      <td>-316.280060</td>\n",
       "      <td>-444.529297</td>\n",
       "      <td>0.371114</td>\n",
       "      <td>-0.051734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>-0.422893</td>\n",
       "      <td>-0.483762</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.060869</td>\n",
       "      <td>-561.132324</td>\n",
       "      <td>-396.917053</td>\n",
       "      <td>-0.037754</td>\n",
       "      <td>0.091314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.572948</td>\n",
       "      <td>-0.579350</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>-370.623444</td>\n",
       "      <td>-344.495056</td>\n",
       "      <td>-0.013561</td>\n",
       "      <td>-0.051308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.665900</td>\n",
       "      <td>-0.708819</td>\n",
       "      <td>-0.799411</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.090592</td>\n",
       "      <td>-521.503418</td>\n",
       "      <td>-453.194397</td>\n",
       "      <td>-0.259940</td>\n",
       "      <td>-0.083210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.563800</td>\n",
       "      <td>-0.381271</td>\n",
       "      <td>-0.678869</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.297597</td>\n",
       "      <td>-346.413330</td>\n",
       "      <td>-367.731384</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>-0.017528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>-0.387988</td>\n",
       "      <td>-0.891804</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.503817</td>\n",
       "      <td>-430.961609</td>\n",
       "      <td>-435.400116</td>\n",
       "      <td>-0.293556</td>\n",
       "      <td>-0.224184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>-0.409559</td>\n",
       "      <td>-1.056810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647251</td>\n",
       "      <td>-407.932068</td>\n",
       "      <td>-486.229706</td>\n",
       "      <td>-0.034613</td>\n",
       "      <td>-0.159539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>-0.483517</td>\n",
       "      <td>-1.015774</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.532257</td>\n",
       "      <td>-514.713989</td>\n",
       "      <td>-533.738159</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>-0.173034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>-0.736230</td>\n",
       "      <td>-1.041909</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.305679</td>\n",
       "      <td>-569.475708</td>\n",
       "      <td>-520.008667</td>\n",
       "      <td>-0.271725</td>\n",
       "      <td>-0.168249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.679300</td>\n",
       "      <td>-0.631773</td>\n",
       "      <td>-0.706725</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.074952</td>\n",
       "      <td>-409.441101</td>\n",
       "      <td>-449.388855</td>\n",
       "      <td>0.013101</td>\n",
       "      <td>-0.046314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>-0.404645</td>\n",
       "      <td>-0.924291</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.519646</td>\n",
       "      <td>-256.983429</td>\n",
       "      <td>-354.405762</td>\n",
       "      <td>-0.016226</td>\n",
       "      <td>-0.285837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.570800</td>\n",
       "      <td>-0.475701</td>\n",
       "      <td>-0.805184</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.329483</td>\n",
       "      <td>-361.061462</td>\n",
       "      <td>-384.575684</td>\n",
       "      <td>-0.068661</td>\n",
       "      <td>0.005312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.517600</td>\n",
       "      <td>-0.499111</td>\n",
       "      <td>-0.932547</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.433436</td>\n",
       "      <td>-351.455261</td>\n",
       "      <td>-296.520294</td>\n",
       "      <td>0.150230</td>\n",
       "      <td>0.140958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.551400</td>\n",
       "      <td>-0.744688</td>\n",
       "      <td>-1.154025</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.409338</td>\n",
       "      <td>-434.493347</td>\n",
       "      <td>-465.710114</td>\n",
       "      <td>-0.038291</td>\n",
       "      <td>-0.173521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>-0.704664</td>\n",
       "      <td>-1.173185</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.468521</td>\n",
       "      <td>-497.800110</td>\n",
       "      <td>-419.253326</td>\n",
       "      <td>-0.026119</td>\n",
       "      <td>-0.032368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.601900</td>\n",
       "      <td>-0.825783</td>\n",
       "      <td>-1.132557</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.306774</td>\n",
       "      <td>-477.946106</td>\n",
       "      <td>-408.286011</td>\n",
       "      <td>-0.247174</td>\n",
       "      <td>-0.228426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.635300</td>\n",
       "      <td>-0.767820</td>\n",
       "      <td>-0.920984</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.153165</td>\n",
       "      <td>-327.325653</td>\n",
       "      <td>-477.489319</td>\n",
       "      <td>0.216637</td>\n",
       "      <td>-0.308996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.593700</td>\n",
       "      <td>-0.679778</td>\n",
       "      <td>-0.917286</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.237509</td>\n",
       "      <td>-425.125000</td>\n",
       "      <td>-513.867920</td>\n",
       "      <td>0.034056</td>\n",
       "      <td>-0.176328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One must patch the DPO Trainer first!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.384 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"unsloth/zephyr-sft-bnb-4bit\", # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "# )\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Alignment Handbook utils\n",
    "import os\n",
    "import re\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "from datasets.builder import DatasetGenerationError\n",
    "\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    example, tokenizer, task: Literal[\"sft\", \"generation\", \"rm\", \"dpo\"] = \"sft\", assistant_prefix=\"<|assistant|>\\n\"\n",
    "):\n",
    "    def _strip_prefix(s, pattern):\n",
    "        # Use re.escape to escape any special characters in the pattern\n",
    "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
    "\n",
    "    if task in [\"sft\", \"generation\"]:\n",
    "        messages = example[\"messages\"]\n",
    "        # We add an empty system message if there is none\n",
    "        if messages[0][\"role\"] != \"system\":\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "        example[\"text\"] = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True if task == \"generation\" else False\n",
    "        )\n",
    "    elif task == \"rm\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            chosen_messages = example[\"chosen\"]\n",
    "            rejected_messages = example[\"rejected\"]\n",
    "            # We add an empty system message if there is none\n",
    "            if chosen_messages[0][\"role\"] != \"system\":\n",
    "                chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            if rejected_messages[0][\"role\"] != \"system\":\n",
    "                rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    elif task == \"dpo\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
    "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
    "            # Insert system message\n",
    "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            else:\n",
    "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
    "            # TODO: handle case where chosen/rejected also have system messages\n",
    "            chosen_messages = example[\"chosen\"][1:]\n",
    "            rejected_messages = example[\"rejected\"][1:]\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
    "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
    "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Task {task} not supported, please ensure that the provided task is one of {['sft', 'generation', 'rm', 'dpo']}\"\n",
    "        )\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "    data_config: dict,\n",
    "    splits: List[str] = [\"train\", \"test\"],\n",
    "    shuffle: bool = True,\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads one or more datasets with varying training set proportions.\n",
    "\n",
    "    Args:\n",
    "        data_config (`DataArguments` or `dict`):\n",
    "            Dataset configuration and split proportions.\n",
    "        splits (`List[str]`, *optional*, defaults to `['train', 'test']`):\n",
    "            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.\n",
    "        shuffle (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to shuffle the training and testing/validation data.\n",
    "\n",
    "    Returns\n",
    "        [`DatasetDict`]: The dataset dictionary containing the loaded datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data_config) is dict:\n",
    "        # Structure of the input is:\n",
    "        #     dataset_mixer = {\n",
    "        #             \"dataset1\": 0.5,\n",
    "        #             \"dataset1\": 0.3,\n",
    "        #             \"dataset1\": 0.2,\n",
    "        #         }\n",
    "        dataset_mixer = data_config\n",
    "    else:\n",
    "        raise ValueError(f\"Data config {data_config} not recognized.\")\n",
    "\n",
    "    raw_datasets = mix_datasets(dataset_mixer, splits=splits, shuffle=shuffle)\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def mix_datasets(dataset_mixer: dict, splits: Optional[List[str]] = None, shuffle=True) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads and mixes datasets according to proportions specified in `dataset_mixer`.\n",
    "\n",
    "    Args:\n",
    "        dataset_mixer (`dict`):\n",
    "            Dictionary containing the dataset names and their training proportions. By default, all test proportions are 1.\n",
    "        splits (Optional[List[str]], *optional*, defaults to `None`):\n",
    "            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.\n",
    "        shuffle (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to shuffle the training and testing/validation data.\n",
    "    \"\"\"\n",
    "    raw_datasets = DatasetDict()\n",
    "    raw_train_datasets = []\n",
    "    raw_val_datasets = []\n",
    "    fracs = []\n",
    "    for ds, frac in dataset_mixer.items():\n",
    "        fracs.append(frac)\n",
    "        for split in splits:\n",
    "            try:\n",
    "                # Try first if dataset on a Hub repo\n",
    "                dataset = load_dataset(ds, split=split)\n",
    "            except DatasetGenerationError:\n",
    "                # If not, check local dataset\n",
    "                dataset = load_from_disk(os.path.join(ds, split))\n",
    "\n",
    "            if \"train\" in split:\n",
    "                raw_train_datasets.append(dataset)\n",
    "            elif \"test\" in split:\n",
    "                raw_val_datasets.append(dataset)\n",
    "            else:\n",
    "                raise ValueError(f\"Split type {split} not recognized as one of test or train.\")\n",
    "\n",
    "    if any(frac < 0 for frac in fracs):\n",
    "        raise ValueError(\"Dataset fractions cannot be negative.\")\n",
    "\n",
    "    if len(raw_train_datasets) > 0:\n",
    "        train_subsets = []\n",
    "        for dataset, frac in zip(raw_train_datasets, fracs):\n",
    "            train_subset = dataset.select(range(int(frac * len(dataset))))\n",
    "            train_subsets.append(train_subset)\n",
    "        if shuffle:\n",
    "            raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n",
    "        else:\n",
    "            raw_datasets[\"train\"] = concatenate_datasets(train_subsets)\n",
    "    # No subsampling for test datasets to enable fair comparison across models\n",
    "    if len(raw_val_datasets) > 0:\n",
    "        if shuffle:\n",
    "            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(seed=42)\n",
    "        else:\n",
    "            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)\n",
    "\n",
    "    if len(raw_datasets) == 0:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {dataset_mixer} not recognized with split {split}. Check the dataset has been correctly formatted.\"\n",
    "        )\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from star.utils.utils import GeneratedSamples\n",
    "\n",
    "# preload_data_for_train = \"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/train_dpo_english_2024-11-07/12-28-14_0/train_0/generated.csv\"\n",
    "# preload_data_for_val = \"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/train_dpo_english_2024-11-13/23-30-22/val_0/generated.csv\"\n",
    "preload_data_for_train = \"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/train_dpo_english_2024-11-19/02-57-11_0/train_0/generated.csv\"\n",
    "preload_data_for_val = \"/nethome/jbjorner3/dev/hallucination-fun/star/star_runs/train_dpo_english_2024-11-19/02-57-11_0/val_0/generated.csv\"\n",
    "def get_sample_completitions_from_csv(csv_file):\n",
    "    data_in_lists = pd.read_csv(csv_file).replace(np.nan, None).to_dict('list')\n",
    "    data_in_lists = {str(k): v for k, v in data_in_lists.items()}\n",
    "    sampled_completitions = GeneratedSamples(**data_in_lists) \n",
    "    return sampled_completitions\n",
    "sampled_completitions_train = get_sample_completitions_from_csv(csv_file=preload_data_for_train)\n",
    "sampled_completitions_val = get_sample_completitions_from_csv(csv_file=preload_data_for_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 111406\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 14513\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from star.dataset.dataset import DPODataset\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "def get_dpo_hf_dataset_from_samples(samples: GeneratedSamples):\n",
    "    ds = DPODataset(subjects=samples.subjects,\n",
    "            indices=samples.indices,\n",
    "            prompted_questions=samples.prompted_questions,\n",
    "            answers=samples.model_answers,\n",
    "            raw_responses=samples.raw_responses,\n",
    "            scores=samples.scores)\n",
    "    list_of_mappings = [{\"prompt\": ex[2],\n",
    "                         \"chosen\": ex[3][2],\n",
    "                         \"rejected\": ex[4][2]} for ex in ds]\n",
    "    return HFDataset.from_list(list_of_mappings)\n",
    "    \n",
    "dsdict = DatasetDict()\n",
    "dsdict['train'] = get_dpo_hf_dataset_from_samples(sampled_completitions_train)\n",
    "dsdict['val'] = get_dpo_hf_dataset_from_samples(sampled_completitions_val)\n",
    "\n",
    "print(dsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "samples = sampled_completitions_train\n",
    "ds = DPODataset(subjects=samples.subjects,\n",
    "            indices=samples.indices,\n",
    "            prompted_questions=samples.prompted_questions,\n",
    "            answers=samples.model_answers,\n",
    "            raw_responses=samples.raw_responses,\n",
    "            scores=samples.scores)\n",
    "c = Counter(d[0] for d in ds.internal_data)\n",
    "# ds.internal_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111406"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({991: 25, 9290: 25, 13682: 25, 522: 25, 1226: 25, 11319: 25, 11800: 25, 313: 25, 1816: 25, 11969: 25, 6056: 25, 6956: 25, 2626: 25, 10055: 25, 10139: 25, 4855: 25, 9049: 25, 2804: 25, 12479: 25, 689: 25, 12059: 25, 890: 25, 11093: 25, 647: 25, 2557: 25, 1547: 25, 13114: 25, 10143: 25, 5639: 25, 8846: 25, 9328: 25, 4223: 25, 5640: 25, 9199: 25, 11830: 25, 4852: 25, 6736: 25, 10806: 25, 1344: 25, 12729: 25, 3096: 25, 11029: 25, 2701: 25, 765: 25, 2624: 25, 10539: 25, 8358: 25, 11459: 25, 2880: 25, 11147: 25, 12093: 25, 10437: 25, 12015: 25, 1759: 25, 3702: 25, 10761: 25, 10266: 25, 9725: 25, 6502: 25, 6147: 25, 5684: 25, 1814: 25, 10423: 25, 6553: 25, 2811: 25, 6902: 25, 14027: 25, 12551: 25, 11383: 25, 9117: 25, 6761: 25, 8222: 25, 12997: 25, 6063: 25, 2945: 25, 616: 25, 13080: 25, 3013: 25, 7518: 25, 12135: 25, 11436: 25, 190: 25, 13873: 25, 6395: 25, 5568: 25, 13850: 25, 1058: 25, 1924: 25, 13544: 25, 13372: 25, 6744: 25, 7046: 25, 252: 25, 5944: 25, 3531: 25, 4252: 25, 13560: 25, 9339: 25, 3014: 25, 1804: 25, 9085: 25, 13508: 25, 9526: 25, 12911: 25, 8380: 25, 9994: 25, 13260: 25, 1342: 25, 10851: 25, 6711: 25, 9271: 25, 11758: 25, 12548: 25, 9516: 25, 11110: 25, 12152: 25, 10972: 25, 11163: 25, 8669: 25, 13245: 25, 12960: 25, 1638: 25, 10612: 25, 7353: 25, 11783: 25, 9152: 25, 10675: 25, 2313: 25, 5155: 25, 895: 25, 8217: 25, 10717: 25, 10146: 25, 11143: 25, 12058: 25, 11455: 25, 10215: 25, 6933: 25, 225: 25, 10931: 25, 2111: 25, 9714: 25, 11610: 25, 4539: 25, 3809: 25, 10642: 25, 12236: 25, 2777: 25, 9348: 25, 9835: 25, 10877: 25, 2310: 25, 11935: 25, 10964: 25, 13434: 25, 11471: 25, 1830: 25, 7579: 25, 8226: 25, 11878: 25, 4155: 25, 11845: 25, 10907: 25, 3481: 25, 929: 25, 8650: 25, 12456: 25, 1082: 25, 2866: 25, 9438: 25, 3296: 25, 1981: 25, 10029: 25, 10625: 25, 12863: 25, 2140: 25, 5453: 25, 5644: 25, 1980: 25, 3170: 25, 1002: 25, 3640: 25, 11513: 25, 8709: 25, 11746: 25, 13492: 25, 11930: 25, 5024: 25, 11552: 25, 9402: 25, 11426: 25, 1053: 25, 10105: 25, 1484: 25, 12190: 25, 4607: 25, 5624: 25, 10765: 25, 12404: 25, 2087: 25, 2238: 25, 9791: 25, 1873: 25, 8707: 25, 1145: 25, 2243: 25, 10681: 25, 13895: 25, 10453: 25, 10046: 25, 12950: 25, 2822: 25, 5823: 25, 7139: 25, 8540: 25, 11909: 25, 2014: 25, 2801: 25, 8493: 25, 9393: 25, 6899: 25, 6884: 25, 11063: 25, 12964: 25, 12710: 25, 12409: 25, 862: 25, 11602: 25, 8728: 25, 2486: 25, 11078: 25, 6706: 25, 150: 25, 11867: 25, 7401: 25, 10853: 25, 5975: 25, 9380: 25, 3117: 25, 205: 25, 11498: 25, 8844: 25, 11242: 25, 1568: 25, 13555: 25, 8595: 25, 3079: 25, 7173: 25, 2969: 25, 8154: 25, 1808: 25, 11879: 25, 9237: 25, 13535: 25, 11923: 25, 643: 25, 4019: 25, 2862: 25, 2746: 25, 3025: 25, 7842: 25, 4490: 25, 13604: 25, 8819: 25, 9360: 25, 12503: 25, 5935: 25, 1292: 25, 6253: 25, 4564: 25, 621: 25, 10607: 25, 12447: 25, 6057: 25, 5632: 25, 11691: 25, 11231: 25, 12186: 25, 2299: 25, 12678: 25, 8069: 25, 1373: 25, 11988: 25, 11765: 25, 5829: 25, 12066: 25, 463: 25, 4135: 25, 11636: 25, 10786: 25, 5101: 25, 2355: 25, 9905: 25, 1333: 25, 12292: 25, 10969: 25, 7237: 25, 934: 25, 1024: 25, 3102: 25, 4147: 25, 10687: 25, 10560: 25, 10867: 25, 2012: 25, 4162: 25, 10439: 25, 13022: 25, 4170: 25, 12669: 25, 6549: 25, 8449: 25, 1848: 25, 12470: 25, 4076: 25, 3867: 25, 5621: 25, 2049: 25, 8735: 25, 7252: 25, 13334: 25, 11281: 25, 943: 25, 2877: 25, 5811: 25, 6835: 25, 11594: 25, 1894: 25, 7474: 25, 1536: 25, 11018: 25, 5957: 25, 11316: 25, 1727: 25, 11832: 25, 9251: 25, 11512: 25, 1737: 25, 9215: 25, 2307: 25, 12187: 25, 1399: 25, 1301: 25, 9887: 25, 11411: 25, 4862: 25, 11413: 25, 8147: 25, 1834: 25, 11729: 25, 2937: 25, 6894: 25, 2775: 25, 4710: 25, 12375: 25, 11959: 25, 3527: 25, 5893: 25, 11473: 25, 13713: 25, 1637: 25, 100: 25, 4090: 25, 10488: 25, 3544: 25, 8954: 25, 5691: 25, 4658: 25, 6167: 25, 13516: 25, 1925: 25, 4193: 25, 6298: 25, 4054: 25, 7715: 25, 657: 25, 11210: 25, 10558: 25, 11873: 25, 1336: 25, 13547: 25, 0: 25, 12561: 25, 3051: 25, 6573: 25, 11028: 25, 12790: 25, 1100: 25, 10114: 25, 3895: 25, 8979: 25, 3388: 25, 6847: 25, 7742: 25, 4741: 25, 8727: 25, 13510: 25, 1728: 25, 4104: 25, 4357: 25, 9700: 25, 6799: 25, 8838: 25, 4141: 25, 10305: 25, 4095: 25, 6636: 25, 10654: 25, 7671: 25, 11819: 25, 6288: 25, 11560: 25, 8866: 25, 6780: 25, 12098: 25, 10249: 25, 13314: 25, 3214: 25, 3496: 25, 10054: 25, 10490: 25, 8766: 25, 9721: 25, 12248: 25, 330: 25, 11340: 25, 1431: 25, 9169: 25, 1424: 25, 12008: 25, 8343: 25, 4125: 25, 5940: 25, 10500: 25, 10513: 25, 13741: 25, 7383: 25, 8739: 25, 12439: 25, 11388: 25, 4230: 25, 6876: 25, 13377: 25, 10540: 25, 605: 25, 8667: 25, 56: 25, 10443: 25, 7352: 25, 8793: 25, 8221: 25, 13717: 25, 6300: 25, 2646: 25, 3722: 25, 13363: 25, 9888: 25, 11420: 25, 10817: 25, 12025: 25, 5249: 25, 2568: 25, 5826: 25, 173: 25, 11355: 25, 6665: 25, 3172: 25, 931: 25, 2098: 25, 9688: 25, 1015: 25, 1781: 25, 8671: 25, 11330: 25, 3113: 25, 13364: 25, 9120: 25, 2870: 25, 4248: 25, 4523: 25, 13274: 25, 13003: 25, 1472: 25, 6715: 25, 2890: 25, 1354: 25, 10658: 25, 5553: 25, 311: 25, 5858: 25, 6381: 25, 992: 25, 9627: 25, 11790: 25, 13370: 25, 1174: 25, 3293: 25, 2667: 25, 11590: 25, 5143: 25, 8397: 25, 10963: 25, 2547: 25, 10262: 25, 10295: 25, 600: 25, 4606: 25, 9621: 25, 11980: 25, 3892: 25, 12332: 25, 6090: 25, 6500: 25, 12882: 25, 11946: 25, 7194: 25, 3345: 25, 13027: 25, 8983: 25, 12004: 25, 1402: 25, 11285: 25, 4304: 25, 6020: 25, 5434: 25, 2024: 25, 6631: 25, 10723: 25, 9747: 25, 11525: 25, 10049: 25, 12104: 25, 407: 25, 10152: 25, 11957: 25, 12134: 25, 3409: 25, 8643: 25, 12682: 25, 8884: 25, 12754: 25, 8967: 25, 11495: 25, 3077: 25, 13573: 25, 4740: 25, 268: 25, 12163: 25, 1540: 25, 11204: 25, 11770: 25, 302: 25, 10544: 25, 11847: 25, 10180: 25, 4418: 25, 314: 25, 6083: 25, 10800: 25, 4190: 25, 7990: 25, 6076: 25, 6652: 25, 8784: 25, 10920: 25, 8824: 25, 10467: 25, 10274: 25, 12108: 25, 2181: 25, 6358: 25, 2986: 25, 11051: 25, 5019: 25, 10051: 25, 7462: 25, 11061: 25, 9044: 25, 499: 25, 13783: 25, 13626: 25, 2022: 25, 4063: 25, 9759: 25, 8822: 25, 11740: 25, 11100: 25, 955: 25, 12106: 25, 11329: 25, 5039: 25, 526: 25, 6679: 25, 9719: 25, 8896: 25, 3749: 25, 12629: 25, 9777: 25, 11176: 25, 5407: 25, 1869: 25, 6725: 25, 5921: 25, 1807: 24, 8581: 24, 4546: 24, 855: 24, 5901: 24, 218: 24, 13329: 24, 11979: 24, 8690: 24, 11583: 24, 3625: 24, 8934: 24, 440: 24, 1541: 24, 6184: 24, 11401: 24, 13054: 24, 9275: 24, 5277: 24, 3094: 24, 12442: 24, 7652: 24, 2464: 24, 8575: 24, 65: 24, 13625: 24, 12081: 24, 1048: 24, 2781: 24, 7912: 24, 47: 24, 2036: 24, 11817: 24, 11591: 24, 9788: 24, 2003: 24, 4774: 24, 1849: 24, 8231: 24, 3988: 24, 2035: 24, 5777: 24, 6589: 24, 8304: 24, 11732: 24, 13755: 24, 11193: 24, 3853: 24, 10590: 24, 6225: 24, 3850: 24, 3289: 24, 9029: 24, 4270: 24, 6770: 24, 3419: 24, 11718: 24, 6136: 24, 3798: 24, 12801: 24, 12718: 24, 13140: 24, 5779: 24, 6068: 24, 13644: 24, 11368: 24, 12974: 24, 8259: 24, 11916: 24, 722: 24, 6290: 24, 29: 24, 9086: 24, 11606: 24, 6192: 24, 6387: 24, 10361: 24, 726: 24, 13057: 24, 11950: 24, 12908: 24, 1634: 24, 11628: 24, 1903: 24, 10683: 24, 9545: 24, 8781: 24, 13443: 24, 6412: 24, 11137: 24, 10889: 24, 4228: 24, 2892: 24, 4832: 24, 5522: 24, 2113: 24, 1013: 24, 9910: 24, 4870: 24, 8340: 24, 1890: 24, 4033: 24, 11962: 24, 642: 24, 4683: 24, 6140: 24, 5339: 24, 10987: 24, 11076: 24, 4581: 24, 1949: 24, 1874: 24, 10506: 24, 8965: 24, 11079: 24, 12517: 24, 10714: 24, 1805: 24, 2148: 24, 9762: 24, 398: 24, 10316: 24, 7069: 24, 13405: 24, 10621: 24, 8510: 24, 7076: 24, 9985: 24, 6526: 24, 2341: 24, 8825: 24, 11876: 24, 5501: 24, 9878: 24, 11381: 24, 13703: 24, 7791: 24, 4334: 24, 3064: 24, 4385: 24, 69: 24, 543: 24, 576: 24, 6320: 24, 3188: 24, 2495: 24, 1662: 24, 800: 24, 2642: 24, 4840: 24, 13212: 24, 6201: 24, 2346: 24, 2891: 24, 13469: 24, 2383: 24, 11191: 24, 1605: 24, 8854: 24, 1988: 24, 7264: 24, 11339: 24, 10996: 24, 1021: 24, 9294: 24, 11346: 24, 5579: 24, 5593: 24, 5659: 24, 10622: 24, 10091: 24, 1603: 24, 4733: 24, 619: 24, 10700: 24, 3688: 24, 4627: 24, 9204: 24, 11064: 24, 9093: 24, 13181: 24, 2583: 24, 11430: 24, 11843: 24, 12057: 24, 808: 24, 7999: 24, 13404: 24, 10326: 24, 1260: 24, 1602: 24, 1920: 24, 9353: 24, 9742: 24, 11171: 24, 1649: 24, 11555: 24, 9097: 24, 12401: 24, 11053: 24, 10840: 24, 13059: 24, 10880: 24, 12591: 24, 2484: 24, 4371: 24, 4049: 24, 1896: 24, 8310: 24, 6723: 24, 8428: 24, 50: 24, 10505: 24, 10693: 24, 9012: 24, 6714: 24, 10756: 24, 10747: 24, 10416: 24, 2170: 24, 9787: 24, 10264: 24, 11596: 24, 8095: 24, 1236: 24, 2007: 24, 10979: 24, 6879: 24, 2577: 24, 4838: 24, 13543: 24, 1747: 24, 8351: 24, 6829: 24, 7005: 24, 135: 24, 9520: 24, 9661: 24, 8295: 24, 1040: 24, 10652: 24, 9967: 24, 11949: 24, 3871: 24, 6784: 24, 4712: 24, 10882: 24, 12127: 24, 4351: 24, 2184: 24, 1612: 24, 12952: 24, 633: 24, 2772: 24, 5401: 24, 11387: 24, 2900: 24, 11181: 24, 6164: 24, 12158: 24, 1855: 24, 12415: 24, 13078: 24, 4765: 24, 10771: 24, 3801: 24, 9144: 24, 8555: 24, 11902: 24, 12132: 24, 2713: 24, 6247: 24, 12042: 24, 7934: 24, 9870: 24, 10082: 24, 12310: 24, 5998: 24, 11981: 24, 11841: 24, 6331: 24, 1671: 24, 10287: 24, 12901: 24, 5311: 24, 2901: 24, 4150: 24, 7945: 24, 12651: 24, 13331: 24, 342: 24, 12738: 24, 7254: 24, 11544: 24, 1700: 24, 2638: 24, 8693: 24, 13231: 24, 13702: 24, 8282: 24, 6528: 24, 10812: 24, 4068: 24, 11931: 24, 11864: 24, 10792: 24, 7514: 24, 10661: 24, 10239: 24, 1878: 24, 3946: 24, 8895: 24, 4861: 24, 7592: 24, 12372: 24, 9397: 24, 9848: 24, 835: 24, 12932: 24, 1641: 24, 11223: 24, 10947: 24, 9686: 24, 10001: 24, 12229: 24, 1012: 24, 11674: 24, 12205: 24, 5031: 24, 13481: 24, 1335: 24, 6825: 24, 3376: 24, 9743: 24, 5141: 24, 10494: 24, 13457: 24, 11422: 24, 13339: 24, 9078: 24, 2570: 24, 628: 24, 11168: 24, 11160: 24, 11038: 24, 10958: 24, 6046: 24, 1148: 24, 9512: 24, 1515: 24, 9454: 24, 1900: 24, 12489: 24, 6772: 24, 13237: 24, 1552: 24, 6488: 24, 11158: 24, 10187: 24, 8562: 24, 5448: 24, 2263: 24, 3811: 24, 8625: 24, 9207: 24, 13016: 24, 282: 24, 11684: 24, 9323: 24, 12360: 24, 12980: 24, 4622: 24, 741: 24, 6374: 24, 10358: 24, 3348: 24, 1313: 24, 9444: 24, 7391: 24, 1199: 24, 9135: 24, 11416: 24, 3053: 24, 11739: 24, 2567: 24, 9909: 24, 8814: 24, 12054: 24, 369: 24, 6838: 24, 9217: 24, 109: 24, 1037: 24, 5166: 24, 10464: 24, 12994: 24, 10140: 24, 9857: 24, 10835: 24, 4225: 24, 3263: 24, 12022: 24, 8695: 24, 383: 24, 9977: 24, 11481: 24, 4168: 24, 4426: 24, 4635: 24, 2987: 24, 1766: 24, 11162: 24, 9564: 24, 11694: 24, 4053: 24, 9858: 24, 9041: 24, 10580: 24, 11332: 24, 7025: 24, 12935: 24, 11303: 24, 5591: 24, 10345: 24, 11412: 24, 2789: 24, 11947: 24, 3135: 24, 8961: 24, 12162: 24, 5943: 24, 2936: 24, 568: 24, 4210: 24, 756: 24, 1147: 24, 6373: 24, 8796: 24, 9595: 24, 10677: 24, 9267: 24, 2318: 24, 6370: 24, 1463: 24, 8347: 24, 10823: 24, 12051: 24, 8628: 24, 10697: 24, 9234: 24, 596: 24, 1418: 24, 3238: 24, 2462: 24, 2812: 24, 11428: 24, 10888: 24, 14037: 24, 2188: 24, 11649: 24, 5803: 24, 10103: 24, 936: 24, 7500: 24, 4080: 24, 365: 24, 6743: 24, 12129: 24, 388: 24, 4339: 24, 422: 24, 4563: 24, 9183: 24, 2459: 24, 2301: 24, 10948: 24, 4649: 24, 11556: 24, 13612: 24, 10478: 24, 9042: 24, 1607: 24, 198: 24, 13776: 24, 2598: 24, 11574: 24, 3851: 24, 12791: 24, 7445: 24, 11664: 24, 5178: 24, 5156: 24, 1330: 24, 12339: 24, 409: 24, 5817: 24, 3206: 24, 4895: 24, 12937: 24, 13757: 24, 124: 24, 5959: 24, 8689: 24, 8782: 24, 8658: 24, 12534: 24, 1020: 24, 1581: 24, 11114: 24, 9156: 24, 5569: 24, 12173: 24, 11266: 24, 7043: 24, 12843: 24, 6073: 24, 7869: 24, 9399: 24, 8930: 24, 5815: 24, 3509: 24, 4509: 24, 3207: 24, 2152: 24, 6620: 24, 11580: 24, 598: 24, 3613: 24, 10541: 24, 300: 24, 12892: 24, 651: 24, 9059: 24, 7770: 24, 6860: 24, 420: 24, 13126: 24, 8501: 24, 5015: 24, 10890: 24, 3843: 24, 900: 24, 6915: 24, 10742: 24, 3032: 24, 6718: 24, 10552: 24, 687: 24, 5922: 24, 4786: 24, 12538: 24, 12073: 24, 10767: 24, 5363: 24, 11586: 24, 322: 24, 11627: 24, 11953: 24, 10251: 24, 4142: 24, 2015: 24, 2600: 24, 9863: 24, 12349: 24, 11112: 24, 3653: 24, 4676: 24, 7627: 24, 11541: 24, 7519: 24, 1716: 24, 8523: 24, 4538: 24, 952: 24, 12707: 24, 5037: 24, 1823: 24, 8238: 24, 13448: 24, 10385: 24, 84: 24, 6097: 24, 5751: 24, 11037: 24, 11057: 24, 13769: 24, 9897: 24, 11044: 24, 6523: 24, 12543: 24, 2678: 24, 12744: 24, 12461: 24, 11187: 24, 10598: 24, 12705: 24, 2133: 24, 5895: 24, 8532: 24, 2241: 24, 10852: 24, 9556: 24, 4746: 24, 9577: 24, 11354: 24, 9960: 24, 4964: 24, 12060: 24, 438: 24, 4062: 24, 8981: 24, 14011: 24, 12381: 24, 6854: 24, 11662: 24, 4432: 24, 2994: 24, 9129: 24, 13962: 24, 10365: 24, 9999: 24, 4189: 24, 11834: 24, 6165: 24, 8021: 24, 10410: 24, 8653: 24, 7001: 24, 307: 24, 6465: 24, 10077: 24, 4175: 24, 3804: 24, 10624: 24, 8337: 24, 5598: 24, 9133: 24, 9500: 24, 2527: 24, 11399: 24, 8225: 24, 8396: 24, 16: 24, 10463: 24, 2575: 24, 11081: 24, 10263: 24, 10635: 24, 6418: 24, 7147: 24, 9911: 24, 4402: 24, 7483: 24, 12955: 24, 11722: 24, 6396: 24, 4180: 24, 10199: 24, 11895: 24, 13088: 24, 2878: 24, 13025: 24, 12965: 24, 2763: 24, 12191: 24, 12118: 24, 11468: 24, 5400: 24, 1144: 24, 1358: 24, 12747: 24, 11482: 24, 13375: 24, 935: 24, 812: 24, 3353: 24, 4731: 24, 7318: 24, 4514: 24, 8088: 24, 11395: 24, 12826: 24, 7179: 24, 11853: 24, 10069: 24, 2291: 24, 2380: 24, 6582: 24, 1705: 24, 3533: 24, 97: 24, 2618: 24, 2295: 24, 12686: 24, 9592: 24, 9325: 24, 9378: 24, 10719: 24, 10196: 24, 1839: 24, 11438: 24, 1230: 24, 5046: 24, 10716: 24, 4995: 24, 10568: 24, 9625: 24, 4368: 24, 10036: 24, 10760: 24, 8252: 24, 12049: 24, 10918: 24, 9382: 24, 10644: 24, 3597: 24, 5728: 24, 11497: 24, 387: 24, 6587: 24, 11077: 24, 939: 24, 204: 24, 1141: 24, 5071: 24, 2190: 24, 5204: 24, 11446: 24, 11337: 24, 8683: 24, 1647: 24, 1382: 24, 11483: 24, 12923: 24, 10102: 24, 6186: 24, 2000: 24, 10224: 24, 13420: 24, 1846: 24, 13128: 24, 10995: 24, 11898: 24, 2473: 24, 3181: 24, 5083: 24, 692: 24, 5491: 24, 9947: 24, 798: 24, 11743: 24, 9589: 24, 2736: 24, 8534: 24, 7707: 24, 11945: 24, 13784: 24, 11201: 24, 9210: 24, 6995: 24, 707: 24, 9281: 24, 3839: 24, 9423: 24, 2639: 24, 11198: 24, 11240: 24, 11402: 24, 462: 24, 832: 24, 6624: 24, 10512: 24, 8915: 24, 7090: 24, 5469: 24, 3090: 24, 12154: 24, 8999: 24, 9823: 24, 4769: 24, 10631: 24, 6133: 24, 8834: 24, 8741: 24, 5504: 24, 185: 24, 11796: 24, 10008: 24, 11972: 24, 2308: 24, 10942: 24, 1359: 24, 444: 24, 9109: 24, 3575: 24, 1252: 24, 341: 24, 4593: 24, 10425: 24, 8583: 24, 13439: 24, 2889: 24, 10722: 24, 9070: 24, 10985: 24, 4576: 24, 9892: 24, 3218: 24, 9547: 24, 3049: 24, 2571: 24, 4276: 24, 6786: 24, 3570: 24, 6530: 24, 13514: 24, 13659: 24, 7137: 24, 335: 24, 4968: 24, 7337: 24, 13155: 24, 1851: 24, 460: 24, 10932: 24, 4296: 24, 4132: 24, 1800: 24, 5243: 24, 6722: 24, 965: 24, 6275: 24, 12037: 24, 2006: 24, 6925: 24, 3493: 24, 5471: 24, 6342: 24, 13409: 24, 11154: 24, 9980: 24, 9490: 24, 1414: 24, 4504: 24, 5032: 24, 20: 24, 4530: 24, 3767: 24, 9494: 24, 12255: 24, 13371: 24, 1881: 24, 11547: 24, 6726: 24, 10981: 24, 6598: 24, 10831: 24, 99: 24, 7743: 24, 1435: 24, 5383: 24, 1870: 24, 10762: 24, 3215: 24, 10523: 24, 10357: 24, 9503: 24, 10986: 24, 13571: 24, 11320: 24, 13335: 24, 4177: 24, 1667: 24, 1305: 24, 11380: 24, 940: 24, 9226: 24, 431: 24, 11939: 24, 2251: 24, 10906: 24, 2043: 24, 3777: 24, 10396: 24, 5642: 24, 1741: 24, 6372: 24, 13387: 24, 8506: 24, 13458: 24, 7082: 24, 6978: 24, 5534: 24, 12636: 24, 12194: 24, 4128: 24, 6668: 24, 8236: 24, 10075: 24, 4707: 24, 3483: 24, 11090: 24, 13213: 24, 234: 24, 10454: 24, 12915: 24, 11648: 24, 7324: 24, 4340: 24, 6694: 24, 12650: 24, 3322: 24, 3026: 24, 12043: 24, 11528: 24, 10770: 24, 11238: 24, 1736: 24, 2121: 24, 8899: 24, 4083: 24, 12052: 24, 10576: 24, 5675: 24, 3674: 24, 6868: 24, 4165: 24, 10126: 24, 1288: 24, 6493: 24, 7463: 24, 12038: 24, 4916: 24, 5063: 24, 10301: 24, 10657: 24, 11427: 24, 10524: 24, 8987: 24, 10020: 24, 11737: 24, 4743: 24, 6619: 24, 93: 24, 13661: 24, 7024: 24, 10975: 24, 762: 24, 5396: 24, 4696: 24, 9187: 24, 4331: 24, 12076: 24, 9327: 24, 6047: 24, 7222: 24, 7890: 24, 624: 24, 10058: 24, 12814: 24, 514: 24, 6517: 24, 5165: 24, 7578: 24, 8157: 24, 10827: 24, 12462: 24, 1150: 24, 8328: 24, 2542: 24, 106: 24, 11047: 24, 10883: 24, 11508: 24, 9340: 24, 8964: 24, 13184: 24, 6095: 24, 12077: 24, 9842: 24, 3478: 24, 1532: 24, 4492: 24, 13017: 24, 4801: 24, 4691: 24, 1267: 24, 9200: 24, 13330: 24, 10926: 24, 8308: 24, 8921: 24, 456: 24, 6857: 24, 10350: 24, 8929: 24, 1139: 24, 8944: 24, 4066: 24, 5492: 24, 5127: 24, 5842: 24, 8513: 24, 3115: 24, 12772: 24, 8411: 24, 3947: 24, 2103: 24, 12677: 24, 2439: 24, 10516: 24, 12962: 24, 10796: 24, 11156: 24, 384: 24, 3766: 24, 11277: 24, 13938: 24, 1043: 24, 5801: 24, 10871: 24, 1704: 24, 11024: 24, 11747: 24, 9349: 24, 5438: 24, 451: 24, 10557: 24, 5514: 24, 2232: 24, 11856: 24, 8482: 24, 1599: 24, 11858: 24, 11706: 24, 10866: 24, 11584: 24, 13517: 24, 10844: 24, 7575: 24, 4727: 24, 3350: 24, 8631: 24, 11670: 24, 4805: 24, 10573: 24, 8479: 24, 3927: 24, 12306: 24, 10784: 24, 8263: 24, 6791: 24, 11349: 24, 6361: 24, 11518: 24, 1156: 24, 39: 24, 10347: 24, 2059: 24, 11669: 24, 4139: 24, 4091: 24, 4247: 24, 699: 24, 11690: 24, 2933: 24, 5764: 24, 3864: 24, 10374: 24, 2607: 24, 5913: 24, 6892: 24, 2521: 24, 9032: 24, 11296: 24, 10729: 24, 9002: 24, 8569: 24, 10427: 24, 4253: 24, 11205: 24, 6334: 24, 9548: 24, 11393: 24, 3890: 24, 6313: 24, 3354: 24, 554: 24, 11546: 24, 984: 24, 10587: 24, 9918: 24, 2009: 24, 6430: 24, 4621: 24, 8254: 24, 4684: 24, 10701: 24, 7235: 24, 2595: 24, 6432: 24, 10130: 24, 110: 24, 6145: 24, 1153: 24, 4931: 24, 8755: 24, 51: 24, 492: 24, 6537: 24, 9400: 24, 11855: 24, 11452: 24, 3065: 24, 11763: 24, 1343: 24, 2664: 24, 13629: 24, 4664: 24, 10150: 24, 11215: 24, 11792: 21, 10178: 21, 10135: 21, 13688: 21, 11254: 21, 11014: 21, 12394: 21, 6992: 21, 10862: 21, 5195: 21, 1976: 21, 10440: 21, 4336: 21, 6042: 21, 3502: 21, 1821: 21, 12886: 21, 10383: 21, 5157: 21, 6756: 21, 10209: 21, 4753: 21, 12486: 21, 5271: 21, 12549: 21, 9957: 21, 7002: 21, 8601: 21, 4461: 21, 6601: 21, 11554: 21, 8772: 21, 7160: 21, 7951: 21, 5139: 21, 9163: 21, 1915: 21, 6127: 21, 10599: 21, 435: 21, 3683: 21, 10944: 21, 8320: 21, 7843: 21, 7155: 21, 10887: 21, 8218: 21, 6153: 21, 9244: 21, 2691: 21, 8362: 21, 2453: 21, 7030: 21, 5723: 21, 7991: 21, 8454: 21, 11658: 21, 3182: 21, 12998: 21, 11211: 21, 12329: 21, 9013: 21, 11614: 21, 333: 21, 11366: 21, 10785: 21, 12727: 21, 8433: 21, 10444: 21, 5620: 21, 745: 21, 5476: 21, 2228: 21, 13431: 21, 12189: 21, 8957: 21, 9515: 21, 8719: 21, 239: 21, 6111: 21, 9153: 21, 13792: 21, 8376: 21, 11942: 21, 6704: 21, 10798: 21, 4010: 21, 11642: 21, 3997: 21, 8888: 21, 4776: 21, 6062: 21, 12834: 21, 9249: 21, 7287: 21, 10083: 21, 11883: 21, 2516: 21, 8033: 21, 4867: 21, 13001: 21, 192: 21, 186: 21, 10566: 21, 4507: 21, 4586: 21, 9094: 21, 12434: 21, 8325: 21, 7104: 21, 6912: 21, 13131: 21, 11328: 21, 3043: 21, 1259: 21, 8451: 21, 244: 21, 5556: 21, 2315: 21, 3692: 21, 11974: 21, 3124: 21, 1369: 21, 405: 21, 8764: 21, 14022: 21, 6856: 21, 9682: 21, 8319: 21, 3538: 21, 4350: 21, 4657: 21, 3648: 21, 103: 21, 9899: 21, 4881: 21, 3002: 21, 12981: 21, 12244: 21, 2482: 21, 12105: 21, 11592: 21, 2225: 21, 1290: 21, 3923: 21, 9130: 21, 4871: 21, 11445: 21, 11138: 21, 9006: 21, 8322: 21, 1887: 21, 2751: 21, 12653: 21, 11123: 21, 443: 21, 9599: 21, 4134: 21, 6846: 21, 12167: 21, 12304: 21, 294: 21, 5181: 21, 2604: 21, 8372: 21, 3965: 21, 11050: 21, 11289: 21, 9847: 21, 5631: 21, 2128: 21, 11484: 21, 8099: 21, 4706: 21, 2602: 21, 2579: 21, 13332: 21, 10775: 21, 10577: 21, 5662: 21, 1796: 21, 1600: 21, 10791: 21, 2257: 21, 1240: 21, 12473: 21, 4094: 21, 4075: 21, 11245: 21, 8168: 21, 11496: 21, 5439: 21, 10830: 21, 5864: 21, 10269: 21, 12321: 21, 11415: 21, 7039: 21, 6525: 21, 13937: 21, 11506: 21, 807: 21, 519: 21, 13900: 21, 13039: 21, 1566: 21, 3920: 21, 1922: 21, 4610: 21, 13169: 21, 2037: 21, 10939: 21, 11098: 21, 5746: 21, 12824: 21, 13860: 21, 49: 21, 9270: 21, 11761: 21, 1731: 21, 6699: 21, 12136: 21, 165: 21, 6155: 21, 10955: 21, 6427: 21, 377: 21, 5456: 21, 4004: 21, 2617: 21, 6211: 21, 4734: 21, 11184: 21, 7694: 21, 12123: 21, 5292: 21, 8757: 21, 10184: 21, 7937: 21, 1811: 21, 482: 21, 9118: 21, 7925: 21, 5828: 21, 3331: 21, 9616: 21, 12514: 21, 10597: 21, 5824: 21, 12579: 21, 1867: 21, 670: 21, 12146: 21, 9058: 21, 6905: 21, 191: 21, 12871: 21, 6093: 21, 8269: 21, 11805: 21, 12895: 21, 7361: 21, 6639: 21, 11600: 21, 8726: 21, 9387: 21, 12675: 21, 6625: 21, 6682: 21, 497: 21, 10715: 21, 4121: 21, 8989: 21, 10017: 21, 3260: 21, 6058: 21, 8644: 21, 11323: 21, 10859: 21, 11279: 21, 4188: 21, 13286: 21, 10530: 21, 3802: 21, 824: 21, 8830: 21, 4067: 21, 9764: 21, 4913: 21, 5743: 21, 174: 21, 10609: 21, 11494: 21, 8243: 21, 9585: 21, 64: 21, 11462: 21, 681: 21, 10656: 21, 183: 21, 6015: 21, 1934: 21, 7320: 21, 614: 21, 11502: 21, 12841: 21, 1457: 21, 8572: 21, 491: 21, 10381: 21, 7916: 21, 1994: 21, 12511: 21, 4687: 21, 4256: 21, 10050: 21, 3515: 21, 9376: 21, 5638: 21, 10211: 21, 12945: 21, 6664: 21, 1984: 21, 3464: 21, 3158: 21, 4319: 21, 5714: 21, 2637: 21, 10108: 21, 10006: 21, 1268: 21, 8593: 21, 2136: 21, 1815: 21, 13416: 21, 5098: 21, 10495: 21, 3495: 21, 12110: 21, 6889: 21, 12161: 21, 8769: 21, 5526: 21, 11716: 21, 3863: 21, 2668: 21, 10052: 21, 9446: 21, 6461: 21, 10040: 21, 11660: 21, 10663: 21, 13546: 21, 2064: 21, 2523: 21, 10891: 21, 2805: 21, 6484: 21, 5530: 21, 818: 21, 7648: 21, 6803: 21, 7000: 21, 2612: 21, 1114: 21, 13201: 21, 8939: 21, 2053: 21, 4788: 21, 6508: 21, 12488: 21, 7670: 21, 8305: 21, 10157: 21, 8818: 21, 11372: 21, 6025: 21, 5116: 21, 11291: 21, 1901: 21, 5423: 21, 3112: 21, 10399: 21, 10650: 21, 125: 21, 8230: 21, 4959: 21, 6292: 21, 6680: 21, 1062: 21, 11910: 21, 11324: 21, 918: 21, 529: 21, 11914: 21, 11846: 21, 2492: 21, 2349: 21, 4153: 21, 11779: 21, 4631: 21, 10349: 21, 427: 21, 13421: 21, 10968: 21, 686: 21, 10970: 21, 11753: 21, 11065: 21, 870: 21, 3665: 21, 1745: 21, 13272: 21, 3477: 21, 11022: 21, 11667: 21, 7265: 21, 8199: 21, 1863: 21, 428: 21, 6845: 21, 5282: 21, 8928: 21, 853: 21, 11252: 21, 11505: 21, 13209: 21, 12865: 21, 12590: 21, 4280: 21, 13945: 21, 4179: 21, 9580: 21, 2584: 21, 4724: 21, 2731: 21, 7190: 21, 11833: 21, 783: 21, 10705: 21, 6554: 21, 10563: 21, 13737: 21, 8654: 21, 3000: 21, 13803: 21, 7309: 21, 7558: 21, 10610: 21, 12269: 21, 9492: 21, 7395: 21, 1408: 21, 11247: 21, 1526: 21, 1183: 21, 6475: 21, 3995: 21, 12454: 21, 2329: 21, 10280: 21, 6938: 21, 11756: 21, 12215: 21, 3047: 21, 8034: 21, 11748: 21, 11877: 21, 4697: 21, 5982: 21, 169: 21, 12715: 21, 2531: 21, 12652: 21, 7717: 21, 4196: 21, 11456: 21, 2265: 21, 9161: 21, 4620: 21, 2212: 21, 4660: 21, 11568: 21, 1857: 21, 10360: 21, 10268: 21, 8700: 21, 12033: 21, 1780: 21, 10555: 21, 1289: 21, 550: 21, 11129: 21, 12475: 21, 9518: 21, 11797: 21, 1906: 21, 4102: 21, 12512: 21, 3078: 21, 11550: 21, 11363: 21, 12419: 21, 1126: 21, 1654: 21, 8919: 21, 9239: 21, 13179: 21, 5979: 21, 8079: 21, 997: 21, 841: 21, 789: 21, 3433: 21, 12063: 21, 9623: 21, 2623: 21, 8744: 21, 10941: 21, 12103: 21, 11569: 21, 11054: 21, 10768: 21, 12649: 21, 3770: 21, 13029: 21, 4250: 21, 8382: 21, 2562: 21, 11440: 21, 8745: 21, 1642: 21, 10679: 21, 4048: 21, 12689: 21, 6449: 21, 13762: 21, 10836: 21, 4764: 21, 320: 21, 3808: 21, 8968: 21, 6821: 21, 12045: 21, 5442: 21, 13809: 21, 2297: 21, 1775: 21, 92: 21, 12874: 21, 6751: 21, 3908: 21, 11219: 21, 10485: 21, 7577: 21, 2016: 21, 6804: 21, 6250: 21, 6113: 21, 9077: 21, 5580: 21, 11097: 21, 9890: 21, 5281: 21, 3144: 21, 4006: 21, 3071: 21, 10759: 21, 13424: 21, 13124: 21, 5527: 21, 13290: 21, 3039: 21, 9596: 21, 7219: 21, 13019: 21, 5590: 21, 11027: 21, 11431: 21, 3279: 21, 6828: 21, 8880: 21, 11417: 21, 3036: 21, 11244: 21, 1677: 21, 10202: 21, 3134: 21, 5807: 21, 6166: 21, 12782: 21, 11780: 21, 2544: 21, 4830: 21, 6329: 21, 1711: 21, 1400: 21, 7339: 21, 12859: 21, 2156: 21, 4944: 21, 11851: 21, 10232: 21, 10746: 21, 10838: 21, 11589: 21, 19: 21, 6305: 21, 167: 21, 2773: 21, 8253: 21, 2321: 21, 3630: 21, 12930: 21, 6683: 21, 6922: 21, 2274: 21, 13250: 21, 3333: 21, 4632: 21, 1563: 21, 6277: 21, 12717: 21, 9757: 21, 1439: 21, 466: 21, 11343: 21, 2271: 21, 2734: 21, 12810: 21, 2092: 21, 8611: 21, 9181: 21, 11071: 21, 8909: 21, 5671: 21, 196: 21, 11153: 21, 7610: 21, 4680: 21, 8773: 21, 10879: 21, 1744: 21, 1868: 21, 11679: 21, 10994: 21, 11085: 21, 7334: 21, 10038: 21, 12071: 21, 8635: 21, 1595: 21, 10600: 21, 3612: 21, 11647: 21, 5831: 21, 1491: 21, 12752: 21, 10684: 21, 11461: 21, 4715: 21, 1441: 21, 3343: 21, 13303: 21, 6039: 21, 675: 21, 13268: 21, 4140: 21, 4847: 21, 12251: 21, 13060: 21, 569: 21, 1639: 21, 7049: 21, 14000: 21, 9280: 21, 8101: 21, 513: 21, 12712: 21, 11781: 21, 1591: 21, 8412: 21, 10074: 21, 6125: 21, 3913: 21, 12516: 21, 5853: 21, 11485: 21, 10167: 21, 1143: 21, 11850: 21, 8184: 21, 111: 21, 11250: 21, 12531: 21, 2849: 21, 1116: 21, 12778: 21, 12253: 21, 8710: 21, 9362: 21, 2433: 21, 9833: 21, 1192: 21, 8678: 21, 6383: 21, 1257: 21, 5030: 21, 6591: 21, 12656: 21, 8390: 21, 9568: 21, 10097: 21, 4320: 21, 6257: 21, 12849: 21, 13693: 21, 10991: 21, 13969: 21, 2530: 21, 5698: 21, 9125: 21, 7274: 21, 4511: 21, 10895: 21, 1756: 21, 12625: 21, 12325: 21, 5205: 21, 11335: 21, 3330: 21, 8875: 21, 4497: 21, 1074: 21, 8377: 21, 6496: 21, 5542: 21, 6710: 21, 10763: 21, 2511: 21, 13021: 21, 13822: 21, 9298: 21, 210: 21, 4243: 21, 8213: 21, 13695: 21, 12880: 21, 2711: 21, 5148: 21, 9902: 21, 3396: 21, 4169: 21, 13432: 21, 11643: 21, 2556: 21, 6189: 21, 4335: 21, 6138: 21, 11180: 21, 11772: 21, 2786: 21, 12872: 21, 10777: 21, 10850: 21, 12150: 21, 6217: 21, 232: 21, 3631: 21, 4281: 21, 12611: 21, 9648: 21, 45: 21, 819: 21, 11066: 21, 6939: 21, 9534: 21, 3106: 21, 5695: 21, 11500: 21, 5503: 21, 5600: 21, 12102: 21, 6928: 21, 6408: 21, 8794: 21, 9403: 21, 12939: 21, 10676: 21, 9300: 21, 4737: 21, 11985: 21, 9122: 21, 6183: 21, 3825: 21, 11595: 21, 4544: 21, 10118: 21, 3176: 21, 6279: 21, 13754: 21, 11479: 21, 12396: 21, 11437: 21, 1347: 21, 10507: 21, 12109: 21, 10417: 21, 11447: 21, 1776: 21, 6614: 21, 11048: 21, 10956: 21, 12012: 21, 8279: 21, 10773: 21, 10856: 21, 5735: 21, 4672: 21, 567: 21, 8551: 21, 6654: 21, 11650: 21, 942: 21, 1955: 21, 9443: 21, 11249: 21, 13859: 21, 2519: 21, 7343: 21, 7597: 21, 6006: 21, 6254: 21, 570: 21, 1990: 21, 5665: 21, 12452: 21, 4811: 21, 14036: 21, 3003: 21, 11775: 21, 10923: 21, 11499: 21, 7178: 21, 2776: 21, 1172: 21, 9567: 21, 3880: 21, 7013: 21, 3152: 21, 12889: 21, 10781: 21, 5146: 21, 10181: 21, 9726: 21, 9188: 21, 11680: 21, 12800: 21, 10839: 21, 9471: 21, 8061: 21, 2610: 21, 7363: 21, 5997: 21, 1364: 21, 1459: 21, 11169: 21, 6955: 21, 8383: 21, 9107: 21, 8549: 21, 1176: 21, 10145: 21, 448: 21, 11165: 21, 12078: 21, 10354: 21, 10764: 21, 9316: 21, 11696: 21, 9022: 21, 13076: 21, 4295: 21, 10864: 21, 9121: 21, 10630: 21, 8386: 21, 8649: 21, 3865: 21, 3034: 21, 5172: 21, 12006: 21, 11352: 21, 12423: 21, 7068: 21, 8668: 21, 1853: 21, 4701: 21, 11774: 21, 3379: 21, 176: 21, 1341: 21, 2924: 21, 3227: 21, 11425: 21, 9956: 21, 13096: 21, 286: 21, 5069: 21, 1836: 21, 2501: 21, 13075: 21, 12795: 21, 12020: 21, 12121: 21, 749: 21, 5305: 21, 1225: 21, 11373: 21, 9434: 21, 10520: 21, 3983: 21, 11035: 21, 10220: 21, 12117: 21, 10807: 21, 4815: 21, 9868: 21, 8733: 21, 11911: 21, 9703: 21, 2552: 21, 3221: 21, 2033: 21, 12839: 21, 1265: 21, 13926: 21, 6385: 21, 669: 21, 5377: 21, 11182: 21, 11088: 21, 1206: 21, 2631: 21, 1004: 21, 12144: 21, 10434: 21, 12124: 21, 13142: 21, 8246: 21, 1018: 21, 10678: 21, 13710: 21, 2328: 21, 4086: 21, 7525: 21, 11726: 21, 7248: 21, 11776: 21, 12905: 21, 8173: 21, 1303: 21, 2429: 21, 6401: 21, 5443: 21, 1010: 21, 9990: 21, 6913: 21, 9900: 21, 9126: 21, 11122: 21, 7188: 21, 2322: 21, 9582: 21, 6442: 21, 615: 21, 8354: 21, 10667: 21, 8145: 21, 684: 21, 9532: 21, 5962: 21, 10808: 21, 1242: 21, 22: 21, 10219: 21, 1666: 21, 6402: 21, 8284: 21, 11872: 21, 10874: 21, 11216: 21, 4841: 21, 2546: 21, 5648: 21, 10465: 21, 8655: 21, 1611: 21, 6350: 21, 7003: 21, 4797: 21, 3329: 21, 5121: 21, 5487: 21, 6924: 21, 10588: 21, 980: 21, 1876: 21, 5445: 21, 573: 21, 7486: 21, 6324: 21, 1961: 21, 4503: 21, 1398: 21, 11973: 21, 4119: 21, 10270: 21, 12850: 21, 3067: 21, 10045: 21, 1944: 21, 9219: 21, 11900: 21, 3202: 21, 7461: 21, 10041: 21, 9050: 21, 3381: 21, 10261: 21, 12634: 21, 11787: 21, 25: 21, 1883: 21, 1555: 21, 8239: 21, 152: 21, 13183: 21, 1549: 21, 11271: 21, 10585: 21, 10708: 21, 589: 21, 3903: 21, 4784: 21, 2435: 21, 1840: 21, 12305: 21, 9804: 21, 5054: 21, 1224: 21, 9367: 21, 7356: 21, 7140: 21, 2910: 21, 10522: 21, 8200: 21, 2611: 21, 9583: 21, 12007: 21, 13284: 21, 12661: 21, 6301: 21, 6399: 21, 6678: 21, 13164: 21, 3310: 21, 11351: 21, 127: 21, 10741: 21, 11457: 21, 10899: 21, 10223: 21, 10086: 21, 6555: 21, 11635: 21, 13014: 21, 13711: 21, 6130: 21, 6676: 21, 13687: 21, 8526: 21, 11844: 21, 7762: 21, 9879: 21, 2935: 21, 11141: 21, 6918: 21, 3521: 21, 7415: 21, 1893: 21, 6265: 21, 13616: 21, 8908: 21, 2030: 21, 13023: 21, 11989: 21, 11678: 21, 2233: 21, 5789: 21, 3949: 21, 11055: 21, 11307: 21, 3937: 21, 8867: 21, 1169: 21, 2460: 21, 254: 21, 11818: 21, 11489: 21, 5771: 21, 10619: 21, 6543: 21, 11605: 21, 4605: 21, 8421: 21, 10651: 21, 9778: 21, 5589: 21, 2176: 21, 1918: 21, 8464: 21, 13028: 21, 11464: 21, 2887: 21, 1355: 21, 9789: 21, 6836: 21, 10551: 21, 5167: 21, 4161: 21, 13414: 21, 10164: 21, 1964: 21, 11671: 21, 7154: 21, 1008: 21, 5459: 21, 995: 21, 9061: 21, 8485: 21, 2980: 21, 11904: 21, 1207: 21, 1132: 21, 10976: 21, 10696: 21, 5462: 21, 6797: 21, 5933: 21, 11442: 21, 4752: 21, 12968: 21, 574: 21, 8554: 21, 1712: 21, 11248: 21, 7233: 21, 12075: 21, 10813: 21, 1528: 21, 11901: 21, 12354: 21, 7381: 21, 10910: 21, 241: 21, 6693: 21, 13275: 21, 9884: 21, 2988: 21, 13234: 21, 3761: 21, 5744: 21, 8468: 21, 9238: 21, 11616: 21, 10935: 21, 1899: 21, 2729: 21, 1888: 21, 4985: 21, 6335: 21, 12927: 21, 6521: 21, 12317: 21, 1039: 20, 2143: 20, 1152: 20, 9602: 20, 13727: 20, 1051: 20, 10595: 20, 1458: 20, 3052: 20, 4284: 20, 6657: 20, 1096: 20, 4590: 20, 5574: 20, 5494: 20, 1465: 20, 79: 20, 4814: 20, 4271: 20, 4254: 20, 1134: 20, 10606: 20, 10780: 20, 4377: 20, 11839: 20, 2458: 20, 4408: 20, 2774: 20, 4592: 20, 1447: 20, 3119: 20, 10481: 20, 4255: 20, 933: 20, 13889: 20, 4780: 20, 1137: 20, 5634: 20, 1445: 20, 10363: 20, 98: 20, 1872: 20, 2740: 20, 6013: 20, 292: 20, 4808: 20, 2971: 20, 3482: 20, 4483: 20, 5602: 20, 4366: 20, 4466: 20, 3069: 20, 4238: 20, 4773: 20, 9408: 20, 12243: 20, 2275: 20, 4828: 20, 5241: 20, 3155: 20, 4837: 20, 1395: 20, 9301: 20, 1179: 20, 3932: 20, 4434: 20, 4475: 20, 1401: 20, 2456: 20, 2754: 20, 5564: 20, 1178: 20, 36: 18, 61: 18, 648: 18, 11264: 18, 13894: 18, 1411: 18, 9550: 18, 1182: 18, 4843: 18, 1081: 18, 10732: 18, 14: 18, 4716: 18, 1717: 18, 4526: 18, 4326: 18, 930: 18, 5582: 18, 442: 18, 1241: 18, 1050: 18, 4513: 18, 1620: 18, 3030: 18, 3089: 18, 6623: 18, 5509: 18, 1454: 18, 1749: 18, 6897: 18, 4848: 18, 12659: 18, 12453: 18, 1069: 18, 3059: 18, 10387: 18, 11364: 18, 2455: 18, 3010: 18, 947: 18, 2830: 18, 10376: 18, 4793: 18, 2097: 18, 4817: 18, 1396: 18, 1275: 18, 6944: 18, 10234: 18, 211: 18, 11890: 18, 2463: 18, 2502: 18, 7656: 18, 4374: 18, 1171: 18, 3138: 18, 9256: 18, 1138: 18, 12091: 18, 4459: 18, 13401: 18, 2485: 18, 1149: 18, 928: 18, 958: 18, 4864: 18, 4457: 18, 2191: 18, 4267: 18, 4363: 18, 57: 18, 2874: 18, 3266: 16, 11273: 16, 11899: 16, 13257: 16, 8268: 16, 11623: 16, 10278: 16, 5250: 16, 5977: 16, 847: 16, 5348: 16, 6882: 16, 4428: 16, 7064: 16, 12992: 16, 1653: 16, 12111: 16, 429: 16, 5114: 16, 6729: 16, 7300: 16, 4693: 16, 7019: 16, 11542: 16, 11624: 16, 8956: 16, 3192: 16, 11672: 16, 4874: 16, 1550: 16, 2017: 16, 6491: 16, 5776: 16, 2032: 16, 10531: 16, 6223: 16, 13726: 16, 11570: 16, 12046: 16, 8557: 16, 7495: 16, 13893: 16, 10535: 16, 3842: 16, 12691: 16, 1090: 16, 608: 16, 12314: 16, 4342: 16, 8163: 16, 267: 16, 541: 16, 10160: 16, 10063: 16, 8763: 16, 12392: 16, 9930: 16, 1443: 16, 12192: 16, 108: 16, 6721: 16, 10638: 16, 8686: 16, 3854: 16, 5990: 16, 8169: 16, 8798: 16, 8384: 16, 5463: 16, 2138: 16, 793: 16, 6798: 16, 12465: 16, 6822: 16, 4785: 16, 7928: 16, 7266: 16, 6861: 16, 6733: 16, 2013: 16, 4536: 16, 11259: 16, 13208: 16, 9309: 16, 11036: 16, 12252: 16, 9179: 16, 11034: 16, 6436: 16, 8365: 16, 7611: 16, 12464: 16, 1668: 16, 4722: 16, 11023: 16, 1437: 16, 10637: 16, 11976: 16, 381: 16, 14040: 16, 13627: 16, 11773: 16, 4133: 16, 10173: 16, 7310: 16, 8795: 16, 425: 16, 8086: 16, 13847: 16, 1982: 16, 9923: 16, 3389: 16, 3234: 16, 11403: 16, 10047: 16, 2019: 16, 12857: 16, 611: 16, 6920: 16, 9614: 16, 12947: 16, 3416: 16, 8496: 16, 4770: 16, 5749: 16, 5781: 16, 5725: 16, 8858: 16, 7256: 16, 10680: 16, 8075: 16, 13609: 16, 8409: 16, 4447: 16, 10953: 16, 9525: 16, 2095: 16, 13653: 16, 12842: 16, 9941: 16, 10686: 16, 8561: 16, 13228: 16, 5843: 16, 13601: 16, 5452: 16, 10608: 16, 10296: 16, 7301: 16, 3470: 16, 11010: 16, 4892: 16, 10978: 16, 13862: 16, 5651: 16, 11293: 16, 481: 16, 5027: 16, 9138: 16, 4498: 16, 4579: 16, 6156: 16, 9711: 16, 3912: 16, 9388: 16, 9702: 16, 3276: 16, 13877: 16, 8904: 16, 13226: 16, 5478: 16, 9653: 16, 7020: 16, 4879: 16, 10602: 16, 6268: 16, 2200: 16, 10854: 16, 8725: 16, 3147: 16, 5248: 16, 12311: 16, 10793: 16, 1360: 16, 10754: 16, 3566: 16, 12609: 16, 5291: 16, 12991: 16, 5578: 16, 7755: 16, 8216: 16, 3050: 16, 3445: 16, 8703: 16, 12615: 16, 6089: 16, 13865: 16, 12364: 16, 4224: 16, 11714: 16, 1066: 16, 11612: 16, 10583: 16, 13950: 16, 4640: 16, 8476: 16, 7142: 16, 2641: 16, 11463: 16, 8387: 16, 11522: 16, 4594: 16, 6236: 16, 8327: 16, 1220: 16, 7084: 16, 10904: 16, 10098: 16, 13699: 16, 10984: 16, 8189: 16, 10031: 16, 2359: 16, 1909: 16, 6497: 16, 3657: 16, 3298: 16, 3592: 16, 6814: 16, 4868: 16, 11322: 16, 8202: 16, 2656: 16, 71: 16, 10962: 16, 2506: 16, 9808: 16, 8918: 16, 2062: 16, 13190: 16, 8032: 16, 6900: 16, 12398: 16, 3337: 16, 11634: 16, 4806: 16, 10752: 16, 7354: 16, 13349: 16, 12067: 16, 9654: 16, 2091: 16, 7897: 16, 12088: 16, 6612: 16, 12450: 16, 7539: 16, 10613: 16, 4863: 16, 4771: 16, 12079: 16, 1997: 16, 9202: 16, 1339: 16, 12131: 16, 871: 16, 11233: 16, 4397: 16, 74: 16, 6974: 16, 9018: 16, 5460: 16, 12238: 16, 13427: 16, 11424: 16, 10240: 16, 7788: 16, 9754: 16, 10022: 16, 11347: 16, 4906: 16, 7903: 16, 5214: 16, 13050: 16, 8747: 16, 11666: 16, 12458: 16, 3446: 16, 9222: 16, 5450: 16, 2860: 16, 1703: 16, 4517: 16, 10537: 16, 9522: 16, 6119: 16, 9224: 16, 4330: 16, 1379: 16, 1943: 16, 10974: 16, 10930: 16, 2072: 16, 9728: 16, 9659: 16, 7845: 16, 143: 16, 9005: 16, 10026: 16, 3848: 16, 3909: 16, 3359: 16, 12864: 16, 12337: 16, 10688: 16, 7981: 16, 11125: 16, 8665: 16, 6106: 16, 12224: 16, 10875: 16, 9948: 16, 12765: 16, 11685: 16, 11325: 16, 5244: 16, 3173: 16, 4705: 16, 10474: 16, 13081: 16, 12647: 16, 3751: 16, 12806: 16, 1470: 16, 11429: 16, 12827: 16, 11727: 16, 12807: 16, 3620: 16, 9619: 16, 6141: 16, 6878: 16, 4738: 16, 2576: 16, 6638: 16, 266: 16, 1092: 16, 1229: 16, 11378: 16, 3479: 16, 4016: 16, 3080: 16, 9016: 16, 12053: 16, 623: 16, 12380: 16, 1319: 16, 7183: 16, 3230: 16, 4416: 16, 2512: 16, 13772: 16, 631: 16, 12116: 16, 5309: 16, 13108: 16, 12331: 16, 8759: 16, 911: 16, 11052: 16, 572: 16, 5472: 16, 5435: 16, 1729: 16, 2477: 16, 8142: 16, 184: 16, 10884: 16, 9350: 16, 6976: 16, 12769: 16, 11813: 16, 8879: 16, 8969: 16, 13145: 16, 946: 16, 11932: 16, 4860: 16, 8406: 16, 8171: 16, 12670: 16, 11713: 16, 11318: 16, 4508: 16, 1798: 16, 1809: 16, 5898: 16, 9063: 16, 1353: 16, 13593: 16, 11218: 16, 11731: 16, 3691: 16, 116: 16, 9419: 16, 12645: 16, 9733: 16, 6007: 16, 13423: 16, 1837: 16, 4787: 16, 13868: 16, 6019: 16, 9735: 16, 7721: 16, 4876: 16, 5411: 16, 7741: 16, 524: 16, 10720: 16, 12622: 16, 6204: 16, 9213: 16, 11087: 16, 2069: 16, 5424: 16, 1754: 16, 5713: 16, 10682: 16, 9581: 16, 12711: 16, 337: 16, 13984: 16, 7738: 16, 10738: 16, 2634: 16, 1295: 16, 10789: 16, 1789: 16, 265: 16, 2249: 16, 2164: 16, 3414: 16, 10352: 16, 9748: 16, 5507: 16, 12435: 16, 2153: 16, 4677: 16, 13352: 16, 6131: 16, 8373: 16, 5455: 16, 6507: 16, 10470: 16, 2284: 16, 5664: 16, 12367: 16, 548: 16, 2335: 16, 12029: 16, 11954: 16, 708: 16, 257: 16, 261: 16, 8313: 16, 3567: 16, 6728: 16, 2106: 16, 2054: 16, 8787: 16, 1031: 16, 5124: 16, 4516: 16, 13884: 16, 13204: 16, 539: 16, 1483: 16, 12083: 16, 8419: 16, 13152: 16, 10519: 16, 12149: 16, 9242: 16, 11357: 16, 10028: 16, 10245: 16, 5934: 16, 11631: 16, 1264: 16, 12851: 16, 8833: 16, 10389: 16, 8273: 16, 12840: 16, 8371: 16, 11752: 16, 6333: 16, 6862: 16, 4285: 16, 12291: 16, 4761: 16, 13694: 16, 12970: 16, 10570: 16, 11009: 16, 751: 16, 2616: 16, 1640: 16, 8317: 16, 10903: 16, 11094: 16, 7408: 16, 6287: 16, 11149: 16, 8605: 16, 7269: 16, 13065: 16, 4052: 16, 2005: 16, 1945: 16, 4961: 16, 8550: 16, 10739: 16, 3220: 16, 6226: 16, 4214: 16, 278: 16, 11390: 16, 1279: 16, 9289: 16, 3618: 16, 12126: 16, 10586: 16, 2625: 16, 9099: 16, 12887: 16, 10718: 16, 5994: 16, 13110: 16, 13816: 16, 4084: 16, 4794: 16, 11263: 16, 12357: 16, 9265: 16, 12351: 16, 7908: 16, 6719: 16, 13521: 16, 5687: 16, 12147: 16, 10662: 16, 11075: 16, 2836: 16, 13602: 16, 12201: 16, 9663: 16, 1596: 16, 1884: 16, 12756: 16, 10335: 16, 8405: 16, 12017: 16, 11007: 16, 10200: 16, 848: 16, 3916: 16, 43: 16, 8558: 16, 3860: 16, 1710: 16, 4464: 16, 9814: 16, 4300: 16, 5327: 16, 12794: 16, 4665: 16, 8450: 16, 8916: 16, 1189: 16, 5393: 16, 2480: 16, 8993: 16, 13707: 16, 6444: 16, 11885: 16, 13153: 16, 2650: 16, 1777: 16, 11236: 16, 13941: 16, 7414: 16, 201: 16, 12279: 16, 3941: 16, 10584: 16, 2613: 16, 3366: 16, 10655: 16, 5334: 16, 3453: 16, 1028: 16, 9410: 16, 11618: 16, 11256: 16, 11557: 16, 1663: 16, 11333: 16, 11936: 16, 12408: 16, 7232: 16, 8737: 16, 4088: 16, 877: 16, 1739: 16, 1444: 16, 1565: 16, 274: 16, 7057: 16, 2319: 16, 2104: 16, 9691: 16, 5477: 16, 9827: 16, 6337: 16, 10429: 16, 5474: 16, 3469: 16, 13340: 16, 3602: 16, 7704: 16, 275: 16, 11418: 16, 3971: 16, 8196: 16, 7225: 16, 13174: 16, 9361: 16, 4345: 16, 13677: 16, 3001: 16, 10921: 16, 4002: 16, 11423: 16, 8862: 16, 9995: 16, 12140: 16, 9792: 16, 13045: 16, 3271: 16, 13632: 16, 11199: 16, 6084: 16, 1135: 16, 6160: 16, 170: 16, 3316: 16, 10034: 16, 59: 16, 4293: 16, 1856: 16, 734: 16, 1864: 16, 11720: 16, 13606: 16, 10845: 16, 5813: 16, 6188: 16, 4501: 16, 3109: 16, 4138: 16, 714: 16, 10388: 16, 6306: 16, 974: 16, 10666: 16, 303: 16, 9230: 16, 9657: 16, 8495: 16, 10733: 16, 13210: 16, 4195: 16, 12451: 16, 4456: 16, 2354: 16, 5255: 16, 8166: 16, 10332: 16, 9644: 16, 11903: 16, 1897: 16, 13351: 16, 9320: 16, 3229: 16, 2090: 16, 11521: 16, 4031: 16, 5575: 16, 2673: 16, 13709: 16, 2747: 16, 3253: 16, 10190: 16, 12414: 16, 10828: 16, 2573: 16, 4024: 16, 179: 16, 11089: 16, 769: 16, 7567: 16, 4431: 16, 8010: 16, 13446: 16, 1993: 16, 436: 16, 10945: 16, 12377: 16, 8064: 16, 6243: 16, 10633: 16, 2855: 16, 7854: 16, 1661: 16, 5539: 16, 2690: 16, 12904: 16, 9269: 16, 748: 16, 10002: 16, 5645: 16, 11257: 16, 4908: 16, 8582: 16, 4682: 16, 8897: 16, 2550: 16, 13971: 16, 12732: 16, 4061: 16, 8360: 16, 2844: 16, 10302: 16, 9650: 16, 1482: 16, 5108: 16, 6739: 16, 1101: 16, 7038: 16, 8812: 16, 138: 16, 9089: 16, 13814: 16, 8444: 16, 6104: 16, 13619: 16, 3199: 16, 13959: 16, 852: 16, 11070: 16, 13338: 16, 11142: 16, 12207: 16, 12323: 16, 7666: 16, 1197: 16, 9617: 16, 10950: 16, 11178: 16, 8505: 16, 8876: 16, 10740: 16, 2596: 16, 13444: 16, 5960: 16, 10794: 16, 4496: 16, 3627: 16, 3782: 16, 5924: 16, 11798: 16, 12068: 16, 447: 16, 4668: 16, 6875: 16, 2966: 16, 3719: 16, 433: 16, 12861: 16, 9088: 16, 1371: 16, 4265: 16, 10815: 16, 12065: 16, 11206: 16, 12799: 16, 2703: 16, 2467: 16, 1261: 16, 1614: 16, 10959: 16, 1025: 16, 202: 16, 4747: 16, 8958: 16, 9511: 16, 12822: 16, 2324: 16, 10966: 16, 11015: 16, 11993: 16, 6010: 16, 12502: 16, 2929: 16, 1835: 16, 2060: 16, 1692: 16, 8478: 16, 13211: 16, 5707: 16, 11111: 16, 11831: 16, 8868: 16, 5869: 16, 4559: 16, 4107: 16, 11757: 16, 5045: 16, 3929: 16, 13557: 16, 1578: 16, 11228: 16, 11861: 16, 10019: 16, 9508: 16, 12832: 16, 5586: 16, 5750: 16, 7313: 16, 1765: 16, 11971: 16, 2580: 16, 5704: 16, 11617: 16, 12027: 16, 5755: 16, 10271: 16, 13437: 16, 9744: 16, 13040: 16, 9476: 16, 6237: 16, 13486: 16, 231: 16, 12345: 16, 12341: 16, 4714: 16, 2149: 16, 1546: 16, 5790: 16, 9184: 16, 13530: 16, 13278: 16, 8843: 16, 1656: 16, 13187: 16, 8598: 16, 6818: 16, 1859: 16, 9246: 16, 4603: 16, 10525: 16, 13725: 16, 3948: 16, 4176: 16, 8266: 16, 2685: 16, 6197: 16, 2051: 16, 5596: 16, 2086: 16, 8840: 16, 1875: 16, 8648: 16, 9015: 16, 13090: 16, 1597: 16, 3185: 16, 2767: 16, 4783: 16, 10011: 16, 6109: 16, 836: 16, 2697: 16, 6207: 16, 4208: 16, 10406: 16, 5489: 16, 5112: 16, 7620: 16, 3363: 16, 12731: 16, 2388: 16, 199: 16, 3901: 16, 11862: 16, 3900: 16, 12074: 16, 6745: 16, 5483: 16, 188: 16, 3921: 16, 6079: 16, 10769: 16, 11226: 16, 5804: 16, 2592: 16, 4171: 16, 6863: 16, 6632: 16, 3961: 16, 2507: 16, 10392: 16, 8326: 16, 4666: 16, 11881: 16, 13308: 16, 391: 16, 319: 16, 6778: 16, 11558: 16, 5669: 16, 8244: 16, 6231: 16, 2662: 16, 5160: 16, 5622: 16, 2821: 16, 11661: 16, 3964: 16, 10489: 16, 4730: 16, 612: 16, 6560: 16, 11863: 16, 12768: 16, 11435: 16, 6173: 16, 8385: 16, 1625: 16, 4846: 16, 4851: 16, 11080: 16, 12780: 16, 10820: 16, 4648: 16, 5149: 16, 11470: 16, 10814: 16, 7998: 16, 6945: 16, 3884: 16, 13173: 16, 3285: 16, 6458: 16, 5566: 16, 3085: 16, 6386: 16, 8164: 16, 164: 16, 10529: 16, 7109: 16, 1329: 16, 11246: 16, 3324: 16, 9679: 16, 7771: 16, 2353: 16, 13118: 16, 7757: 16, 9970: 16, 12343: 16, 3610: 16, 5942: 16, 1422: 16, 1523: 16, 4109: 16, 9175: 16, 11419: 16, 7393: 16, 8734: 16, 4275: 16, 10380: 16, 10216: 16, 9295: 16, 6457: 16, 1820: 16, 12444: 16, 10934: 16, 12039: 16, 3551: 16, 375: 16, 2465: 16, 12568: 16, 1487: 16, 8071: 16, 9017: 16, 1533: 16, 2397: 16, 5877: 16, 12256: 16, 3759: 16, 134: 16, 8459: 16, 13718: 16, 11475: 16, 13031: 16, 10162: 16, 10327: 16, 7859: 16, 2500: 16, 8724: 16, 9756: 16, 4494: 16, 1468: 16, 7153: 16, 9689: 16, 3874: 16, 6440: 16, 1929: 16, 13532: 16, 5117: 16, 9810: 16, 10788: 16, 2220: 16, 11654: 16, 11905: 16, 4246: 16, 5606: 16, 12278: 16, 2651: 16, 9127: 16, 11398: 16, 815: 16, 11675: 16, 10961: 16, 3101: 16, 10876: 16, 10195: 16, 8685: 16, 10258: 16, 122: 16, 717: 16, 31: 16, 13704: 16, 5672: 16, 9132: 16, 11213: 16, 1885: 16, 13312: 16, 6926: 16, 11341: 16, 12021: 16, 4566: 16, 797: 16, 4001: 16, 6858: 16, 2558: 16, 13422: 16, 6690: 16, 9211: 16, 13094: 16, 9544: 16, 13067: 16, 14034: 16, 2802: 16, 3232: 16, 12933: 16, 12356: 16, 8357: 16, 11889: 16, 9212: 16, 6659: 16, 1626: 16, 4311: 16, 11995: 16, 13033: 16, 8291: 16, 11825: 16, 990: 16, 8303: 16, 5446: 16, 8853: 16, 10803: 16, 12696: 16, 1630: 16, 12587: 16, 5560: 16, 7279: 16, 8580: 16, 6118: 16, 6849: 16, 1129: 16, 6170: 16, 8426: 16, 2861: 16, 8527: 16, 8087: 16, 3976: 16, 3256: 16, 1235: 16, 41: 16, 11458: 16, 13073: 16, 8447: 16, 5939: 16, 10458: 16, 12687: 16, 11392: 16, 3116: 16, 1128: 16, 6698: 16, 10660: 16, 12295: 16, 8331: 16, 11091: 16, 6999: 16, 11116: 16, 5475: 16, 5946: 16, 1680: 16, 8545: 16, 11702: 16, 9876: 16, 13010: 16, 4470: 16, 8425: 16, 3387: 16, 713: 16, 9954: 16, 1245: 16, 9243: 16, 13610: 16, 2011: 16, 8462: 16, 293: 16, 12330: 16, 9942: 16, 13306: 16, 11812: 16, 6310: 16, 6407: 16, 6356: 16, 1493: 16, 10521: 16, 8679: 16, 10615: 16, 2884: 16, 3209: 16, 2636: 16, 10450: 16, 6760: 16, 3485: 16, 634: 16, 5003: 16, 563: 16, 4565: 16, 9507: 16, 11755: 16, 4778: 16, 13552: 16, 13856: 16, 4473: 16, 8789: 16, 8356: 16, 8986: 16, 11578: 16, 12293: 16, 3924: 16, 6499: 16, 2644: 16, 5988: 16, 10917: 16, 5454: 16, 13750: 16, 9685: 16, 1027: 16, 13285: 16, 2379: 16, 3902: 16, 12612: 16, 9979: 16, 9882: 16, 6343: 16, 8753: 16, 73: 16, 4662: 16, 7527: 16, 4274: 16, 10997: 16, 6578: 16, 7302: 16, 13035: 16, 11803: 16, 2898: 16, 3097: 16, 1588: 16, 961: 16, 2240: 16, 9194: 16, 9003: 16, 11295: 16, 6102: 16, 2488: 16, 13068: 16, 4646: 16, 1362: 16, 4136: 16, 4282: 16, 9370: 16, 12036: 16, 7491: 16, 221: 16, 327: 16, 4850: 16, 7523: 16, 3595: 16, 10868: 16, 13214: 16, 12069: 16, 13321: 16, 11917: 16, 12393: 16, 10402: 16, 2762: 16, 6332: 16, 5294: 16, 12979: 16, 8297: 16, 6540: 16, 11659: 16, 10475: 16, 2681: 16, 11760: 16, 11665: 16, 13381: 16, 10556: 16, 11692: 16, 13831: 16, 11261: 16, 2215: 16, 4634: 16, 11519: 16, 12407: 16, 13192: 16, 9895: 16, 12107: 16, 6017: 16, 13195: 16, 483: 16, 10132: 16, 975: 16, 11255: 16, 11019: 16, 13011: 16, 10743: 16, 9477: 16, 10005: 16, 9587: 16, 401: 16, 7677: 16, 10472: 16, 1505: 16, 10952: 16, 9304: 16, 4510: 16, 10643: 16, 12048: 16, 13066: 16, 310: 16, 9100: 16, 11288: 16, 12527: 16, 4617: 16, 10751: 16, 9783: 16, 1687: 16, 1720: 16, 9014: 16, 12760: 16, 6662: 16, 11107: 16, 1989: 16, 11433: 16, 731: 16, 11396: 16, 8015: 16, 8344: 16, 2693: 16, 825: 16, 11448: 16, 8570: 16, 8018: 16, 12148: 16, 8645: 16, 3307: 16, 5481: 16, 5989: 16, 9452: 16, 11460: 16, 2780: 16, 10728: 16, 5863: 16, 4945: 16, 6348: 16, 7149: 16, 5610: 16, 7453: 16, 6: 16, 5991: 16, 6472: 16, 6684: 16, 13461: 16, 2915: 16, 4443: 16, 11535: 16, 11927: 16, 4537: 16, 2227: 16, 10483: 16, 1489: 16, 6903: 16, 1534: 16, 2730: 16, 13198: 16, 3893: 16, 5955: 16, 8675: 16, 2389: 16, 113: 16, 4902: 16, 546: 16, 10698: 16, 10670: 16, 3: 16, 4796: 16, 9874: 16, 10811: 16, 6844: 16, 2944: 16, 12589: 16, 13594: 16, 6757: 16, 10247: 16, 487: 16, 2399: 16, 932: 15, 2451: 15, 4: 15, 4278: 15, 4347: 15, 7823: 15, 3153: 15, 2038: 15, 4394: 15, 1464: 15, 4360: 15, 1270: 15, 4263: 15, 4468: 15, 5433: 15, 4763: 15, 1378: 15, 4745: 15, 6908: 15, 4242: 15, 1061: 14, 2606: 14, 921: 14, 2792: 14, 4355: 14, 4409: 14, 5485: 14, 2922: 14, 3881: 14, 12212: 14, 3159: 14, 5496: 14, 1467: 14, 4462: 14, 2649: 14, 7348: 14, 4288: 14, 4349: 14, 1214: 14, 11208: 14, 4465: 14, 2303: 14, 13773: 14, 1067: 14, 28: 14, 5473: 14, 2943: 14, 1033: 14, 986: 14, 4442: 14, 3098: 14, 1366: 14, 8080: 14, 4423: 14, 4487: 14, 95: 14, 4354: 14, 1652: 14, 13: 14, 1446: 14, 1585: 14, 7763: 14, 4772: 14, 1001: 14, 6977: 14, 923: 14, 3148: 14, 2428: 14, 3164: 14, 2028: 14, 10509: 14, 2973: 14, 34: 14, 7292: 14, 1573: 14, 1115: 14, 2449: 14, 4015: 14, 10457: 14, 1858: 14, 679: 14, 1187: 14, 13838: 14, 33: 14, 2173: 14, 4313: 14, 1448: 12, 240: 12, 1011: 12, 12: 12, 4233: 12, 4420: 12, 4398: 12, 1452: 12, 4766: 12, 4405: 12, 336: 12, 10536: 12, 325: 12, 2564: 12, 4310: 12, 4346: 12, 10579: 12, 4369: 12, 4290: 12, 4356: 12, 4833: 12, 5576: 12, 4381: 12, 1054: 12, 357: 12, 4474: 12, 786: 12, 1142: 12, 4798: 12, 874: 12, 1107: 12, 13030: 12, 1034: 12, 11764: 12, 1425: 12, 1102: 12, 8076: 12, 5585: 12, 4236: 12, 3236: 10, 4749: 10, 5547: 10, 2876: 10, 3038: 10, 1941: 10, 1222: 10, 4307: 10, 10581: 10, 10487: 10, 13871: 10, 10370: 10, 10421: 10, 8: 10, 1166: 10, 901: 10, 3224: 10, 4411: 10, 11067: 9, 3624: 9, 10527: 9, 3160: 9, 7872: 9, 1708: 9, 15: 9, 13089: 9, 5067: 9, 13763: 9, 5549: 9, 11177: 9, 270: 9, 2340: 9, 7344: 9, 10548: 9, 4478: 9, 719: 9, 12510: 9, 10550: 9, 10284: 9, 3240: 9, 12569: 9, 2923: 9, 3646: 9, 7004: 9, 8050: 9, 12101: 9, 2632: 9, 11686: 9, 10957: 9, 2698: 9, 5517: 9, 13600: 9, 2630: 9, 7465: 9, 6867: 9, 13302: 9, 10341: 9, 3370: 9, 11828: 9, 6146: 9, 10832: 9, 1788: 9, 449: 9, 2918: 9, 10311: 9, 7026: 9, 7263: 9, 1508: 9, 7713: 9, 11152: 9, 9836: 9, 6557: 9, 6793: 9, 5134: 9, 1185: 9, 3023: 9, 4775: 9, 11697: 9, 8620: 9, 6338: 9, 2073: 9, 11744: 9, 1233: 9, 2085: 9, 9159: 9, 12743: 9, 343: 9, 6518: 9, 9262: 9, 816: 9, 5357: 9, 11534: 9, 6776: 9, 10911: 9, 10131: 9, 13720: 9, 2074: 9, 11301: 9, 12358: 9, 3497: 9, 4303: 9, 13666: 9, 3682: 9, 8281: 9, 12113: 9, 3128: 9, 3791: 9, 10634: 9, 6805: 9, 9484: 9, 6605: 9, 13501: 9, 11966: 9, 11759: 9, 5679: 9, 8911: 9, 13493: 9, 9241: 9, 4878: 9, 12699: 9, 1792: 9, 8441: 9, 105: 9, 4962: 9, 10922: 9, 4673: 9, 3436: 9, 11310: 9, 10724: 9, 2041: 9, 2522: 9, 7938: 9, 4671: 9, 9062: 9, 12879: 9, 2978: 9, 8474: 9, 9284: 9, 3958: 9, 13092: 9, 6516: 9, 4686: 9, 2424: 9, 827: 9, 11687: 9, 6221: 9, 9482: 9, 182: 9, 11326: 9, 3233: 9, 12168: 9, 8571: 9, 11576: 9, 10466: 9, 13987: 9, 4395: 9, 6896: 9, 2543: 9, 8865: 9, 4531: 9, 10456: 9, 1078: 9, 13854: 9, 7051: 9, 13896: 9, 8422: 9, 13167: 9, 7568: 9, 10735: 9, 10079: 9, 5260: 9, 2139: 9, 11492: 9, 4905: 9, 7642: 9, 6754: 9, 10233: 9, 8187: 9, 7413: 9, 10309: 9, 8874: 9, 12745: 9, 7129: 9, 11241: 9, 13722: 9, 7848: 9, 12999: 9, 4249: 9, 3183: 9, 1091: 9, 8995: 9, 8847: 9, 9651: 9, 13999: 9, 11: 9, 3302: 9, 8998: 9, 3213: 9, 2962: 9, 2601: 9, 4057: 9, 13367: 9, 5562: 9, 3377: 9, 5796: 9, 10441: 9, 6429: 9, 8662: 9, 9345: 9, 3107: 9, 13438: 9, 5721: 9, 9916: 9, 2761: 9, 3308: 9, 2745: 9, 6762: 9, 4191: 9, 3191: 9, 11278: 9, 4822: 9, 7650: 9, 5666: 9, 53: 9, 24: 9, 12793: 9, 5105: 9, 4032: 9, 13828: 9, 2396: 9, 12920: 9, 2586: 9, 11294: 9, 8544: 9, 9302: 9, 9514: 9, 868: 9, 1119: 9, 5185: 9, 5084: 9, 8174: 9, 13449: 9, 5528: 9, 13356: 9, 12761: 9, 9326: 9, 8530: 9, 2771: 9, 13680: 9, 8465: 9, 10639: 9, 13264: 9, 415: 9, 7385: 9, 3248: 9, 12322: 9, 4633: 9, 8181: 9, 2716: 9, 13386: 9, 8499: 9, 6336: 9, 13358: 9, 3006: 9, 1036: 9, 2796: 9, 8228: 9, 13175: 9, 7377: 9, 7959: 9, 3216: 9, 7096: 9, 2311: 9, 12175: 9, 5408: 9, 12986: 9, 9273: 9, 7157: 9, 11886: 9, 13655: 9, 1345: 9, 993: 9, 12448: 9, 2872: 9, 2314: 9, 7330: 9, 9479: 9, 8775: 9, 1824: 9, 4732: 9, 8587: 9, 13429: 9, 6951: 9, 5337: 9, 1701: 9, 12445: 9, 7358: 9, 5753: 9, 11531: 9, 9708: 9, 9201: 9, 5887: 9, 11802: 9, 9562: 9, 6294: 9, 4035: 9, 6423: 9, 9250: 9, 9609: 9, 12631: 9, 13789: 9, 11021: 9, 2387: 9, 1817: 9, 6688: 9, 5565: 9, 4711: 9, 10734: 9, 9310: 9, 8083: 9, 2992: 9, 10982: 9, 12403: 9, 8000: 9, 6534: 9, 8953: 9, 3166: 9, 3704: 9, 10491: 9, 7428: 9, 7180: 9, 8456: 9, 7953: 9, 597: 9, 13807: 9, 6024: 9, 5929: 9, 7410: 9, 8400: 9, 7758: 9, 11517: 9, 10119: 9, 3004: 9, 9845: 9, 508: 9, 10299: 9, 6134: 9, 8508: 9, 12535: 9, 10142: 9, 10062: 9, 11377: 9, 10855: 9, 3894: 9, 6420: 9, 11336: 9, 264: 9, 8596: 9, 2549: 9, 11353: 9, 2211: 9, 912: 9, 11871: 9, 137: 9, 3024: 9, 9933: 9, 7008: 9, 12476: 9, 11465: 9, 329: 9, 13706: 9, 6219: 9, 10016: 9, 12632: 9, 11865: 9, 9436: 9, 2117: 9, 12969: 9, 12218: 9, 12142: 9, 8338: 9, 5904: 9, 11754: 9, 12977: 9, 13013: 9, 7954: 9, 5113: 9, 7308: 9, 5529: 9, 2142: 9, 5719: 9, 5657: 9, 6398: 9, 4956: 9, 4044: 9, 4809: 9, 2757: 9, 12913: 9, 11108: 9, 9170: 9, 7665: 9, 10222: 9, 6345: 9, 6765: 9, 9227: 9, 6314: 9, 10929: 9, 11965: 9, 11449: 9, 10547: 9, 13116: 9, 12536: 9, 7678: 9, 12333: 9, 3035: 9, 8856: 9, 13398: 9, 13282: 9, 979: 9, 2560: 9, 2235: 9, 683: 9, 2605: 9, 7316: 9, 8210: 9, 3764: 9, 1995: 9, 9849: 9, 10003: 9, 3068: 9, 11314: 9, 8407: 9, 12040: 9, 13667: 9, 13093: 9, 9373: 9, 5722: 9, 8188: 9, 11121: 9, 10344: 9, 2864: 9, 7481: 9, 1942: 9, 8214: 9, 3201: 9, 216: 9, 281: 9, 13746: 9, 5103: 9, 7501: 9, 5252: 9, 12041: 9, 680: 9, 12085: 9, 12875: 9, 11709: 9, 6285: 9, 2419: 9, 1388: 9, 7479: 9, 13701: 9, 5775: 9, 739: 9, 2047: 9, 2926: 9, 7314: 9, 10711: 9, 11474: 9, 11290: 9, 6677: 9, 8666: 9, 8810: 9, 11657: 9, 1831: 9, 7564: 9, 13430: 9, 727: 9, 2282: 9, 12786: 9, 9067: 9, 13672: 9, 7107: 9, 5379: 9, 11286: 9, 4854: 9, 12184: 9, 11239: 9, 9861: 9, 9255: 9, 3589: 9, 3421: 9, 13648: 9, 8636: 9, 9366: 9, 13806: 9, 3876: 9, 7018: 9, 5184: 9, 166: 9, 10873: 9, 12938: 9, 10237: 9, 9519: 9, 12138: 9, 9692: 9, 12537: 9, 13875: 9, 10914: 9, 1810: 9, 13280: 9, 11074: 9, 9937: 9, 1659: 9, 6919: 9, 8170: 9, 12940: 9, 168: 9, 4108: 9, 9414: 9, 13575: 9, 13070: 9, 9426: 9, 5983: 9, 4729: 9, 6570: 9, 5008: 9, 13781: 9, 2205: 9, 243: 9, 549: 9, 1455: 9, 2100: 9, 12399: 9, 1726: 9, 11782: 9, 12495: 9, 10940: 9, 12044: 9, 9196: 9, 8211: 9, 269: 9, 2221: 9, 194: 9, 6975: 9, 2835: 9, 11700: 9, 12522: 9, 9800: 9, 3645: 9, 10484: 9, 1991: 9, 6135: 9, 4950: 9, 2317: 9, 7375: 9, 12873: 9, 2582: 9, 12641: 9, 10193: 9, 9236: 9, 5075: 9, 11940: 9, 8762: 9, 6191: 9, 5385: 9, 10117: 9, 10553: 9, 8299: 9, 11173: 9, 6066: 9, 11652: 9, 7949: 9, 13899: 9, 5418: 9, 13790: 9, 6115: 9, 913: 9, 4720: 9, 13146: 9, 207: 9, 13738: 9, 10393: 9, 646: 9, 1076: 9, 5912: 9, 518: 9, 8642: 9, 12092: 9, 5953: 9, 4713: 9, 4388: 9, 8120: 9, 8935: 9, 9543: 9, 245: 9, 12072: 9, 10334: 9, 11058: 9, 3460: 9, 4569: 9, 9389: 9, 12746: 9, 3992: 9, 3633: 9, 4096: 9, 12883: 9, 8274: 9, 13820: 9, 6865: 9, 7646: 9, 177: 9, 7332: 9, 12164: 9, 13300: 9, 12196: 9, 13259: 9, 7382: 9, 710: 9, 10007: 9, 5225: 9, 9461: 9, 11793: 9, 9026: 9, 12263: 9, 3200: 9, 8448: 9, 75: 9, 1278: 9, 6256: 9, 13953: 9, 6561: 9, 10498: 9, 9695: 9, 13127: 9, 6796: 9, 3815: 9, 10442: 9, 12459: 9, 2457: 9, 10422: 9, 5973: 9, 5306: 9, 10448: 9, 11421: 9, 5467: 9, 8424: 9, 730: 9, 9292: 9, 2621: 9, 6489: 9, 2793: 9, 9394: 9, 1871: 9, 8186: 9, 11653: 9, 5042: 9, 7113: 9, 8640: 9, 7873: 9, 4056: 9, 13456: 9, 3288: 9, 7407: 9, 13402: 9, 8410: 9, 3726: 9, 4159: 9, 3529: 9, 3454: 9, 5188: 9, 8750: 9, 8881: 9, 959: 9, 9034: 9, 13095: 9, 11852: 9, 9322: 9, 7537: 9, 9590: 9, 8604: 9, 9573: 9, 2202: 9, 9710: 9, 8245: 9, 13091: 9, 8333: 9, 899: 9, 1627: 9, 46: 9, 8359: 9, 8807: 9, 8729: 9, 13732: 9, 12607: 9, 610: 9, 5093: 9, 6533: 9, 6929: 9, 6538: 9, 3858: 9, 7696: 9, 12221: 9, 4663: 9, 7768: 9, 12552: 9, 8602: 9, 1779: 9, 9384: 9, 2620: 9, 12941: 9, 4643: 9, 5056: 9, 4572: 9, 6552: 9, 6016: 9, 6567: 9, 4886: 9, 1782: 9, 3830: 9, 7769: 9, 7032: 9, 3133: 9, 11893: 9, 12539: 9, 8414: 9, 12287: 9, 2654: 9, 4534: 9, 6278: 9, 14017: 9, 10391: 9, 13494: 9, 775: 9, 12014: 9, 10772: 9, 2982: 9, 5911: 9, 12080: 9, 6308: 9, 2250: 9, 12024: 9, 1752: 9, 5852: 9, 2814: 9, 4092: 9, 8248: 9, 4279: 9, 7904: 9, 7022: 9, 4197: 9, 7569: 9, 9483: 9, 1795: 9, 4101: 9, 1089: 9, 3083: 9, 13397: 9, 149: 9, 742: 9, 12751: 9, 5626: 9, 4884: 9, 187: 9, 8627: 9, 6911: 9, 353: 9, 6411: 9, 10894: 9, 7535: 9, 3044: 9, 2436: 9, 1718: 9, 13646: 9, 3564: 9, 12642: 9, 10993: 9, 1938: 9, 11375: 9, 3407: 9, 6322: 9, 7664: 9, 2293: 9, 5835: 9, 5740: 9, 1651: 9, 3467: 9, 14032: 9, 503: 9, 8031: 9, 9068: 9, 12262: 9, 5635: 9, 9385: 9, 26: 9, 10617: 9, 8265: 9, 10504: 9, 11175: 9, 13400: 9, 1391: 9, 7450: 9, 6479: 9, 9096: 9, 13527: 9, 2415: 9, 2661: 9, 4064: 9, 6961: 9, 8363: 9, 8624: 9, 4036: 9, 406: 9, 5060: 9, 1302: 9, 10685: 9, 3686: 9, 2193: 9, 7130: 9, 5333: 9, 1403: 9, 9260: 9, 9885: 9, 12257: 9, 9915: 9, 9115: 9, 5845: 9, 13253: 9, 7342: 9, 8746: 9, 1529: 9, 6671: 9, 8590: 9, 6763: 9, 1621: 9, 1492: 9, 4570: 9, 6952: 9, 4615: 9, 465: 9, 10664: 9, 7941: 9, 11707: 9, 9797: 9, 4723: 9, 13425: 9, 5490: 9, 11389: 9, 1678: 9, 7801: 9, 8717: 9, 3778: 9, 7787: 9, 1898: 9, 6142: 9, 7216: 9, 9981: 9, 12018: 9, 13870: 9, 11217: 9, 5984: 9, 6389: 9, 11151: 9, 12714: 9, 8872: 9, 11943: 9, 9571: 9, 4654: 9, 8670: 9, 11937: 9, 11539: 9, 3822: 9, 3203: 9, 10493: 9, 2395: 9, 1613: 9, 2645: 9, 90: 9, 13160: 9, 5693: 9, 2603: 9, 12618: 9, 5389: 9, 193: 9, 13942: 9, 5427: 9, 13970: 9, 10109: 9, 5128: 9, 12483: 9, 12967: 9, 13810: 9, 860: 9, 10191: 9, 13825: 9, 10452: 9, 3617: 9, 712: 9, 6094: 9, 4152: 9, 7: 9, 9642: 9, 8841: 9, 10111: 9, 5488: 9, 9683: 9, 13102: 9, 8195: 9, 5068: 9, 11144: 9, 7923: 9, 11598: 9, 7409: 9, 9441: 9, 12542: 9, 7914: 9, 6234: 9, 13524: 9, 9487: 9, 9991: 9, 3754: 9, 12558: 9, 7681: 9, 10572: 9, 3422: 9, 13922: 9, 11049: 9, 2194: 9, 926: 9, 10476: 9, 1543: 9, 1318: 9, 11174: 9, 5538: 9, 544: 9, 12685: 9, 12324: 9, 1721: 9, 1332: 9, 5079: 9, 13858: 9, 11611: 9, 6098: 9, 5627: 9, 2413: 9, 8483: 9, 11043: 9, 2264: 9, 10486: 9, 1672: 9, 6594: 9, 8353: 9, 10207: 9, 11829: 9, 8366: 9, 1006: 9, 9984: 9, 787: 9, 13483: 9, 6450: 9, 114: 9, 1000: 9, 1315: 9, 2841: 9, 5573: 9, 6737: 9, 2269: 9, 10397: 9, 7047: 9, 8004: 9, 12789: 9, 6885: 9, 13087: 9, 13995: 9, 2742: 9, 6823: 9, 11369: 9, 11331: 9, 13622: 9, 11379: 9, 3091: 9, 1819: 9, 8610: 9, 2904: 9, 7172: 9, 7754: 9, 5296: 9, 9051: 9, 12585: 9, 8257: 9, 9493: 9, 8059: 9, 1338: 9, 11620: 9, 6887: 9, 7821: 9, 12988: 9, 11710: 9, 1914: 9, 3269: 9, 586: 9, 9485: 9, 11493: 9, 12211: 9, 7187: 9, 9369: 9, 13459: 9, 3619: 9, 6490: 9, 3114: 9, 3752: 9, 3930: 9, 13121: 9, 12739: 9, 12576: 9, 3151: 9, 6965: 9, 496: 9, 9611: 9, 9850: 9, 1219: 9, 9496: 9, 12114: 9, 6807: 9, 2306: 9, 10938: 9, 8136: 9, 12828: 9, 6794: 9, 9928: 9, 148: 9, 12446: 9, 7767: 9, 12296: 9, 8374: 9, 5468: 9, 1933: 9, 2444: 9, 11599: 9, 5518: 9, 1979: 9, 10674: 9, 11880: 9, 2325: 9, 11874: 9, 4807: 9, 13056: 9, 12193: 9, 9693: 9, 5832: 9, 6101: 9, 10755: 9, 1046: 9, 6339: 9, 2590: 9, 1481: 9, 6304: 9, 9092: 9, 9819: 9, 625: 9, 2993: 9, 3129: 9, 6514: 9, 13130: 9, 1616: 9, 2001: 9, 13141: 9, 6480: 9, 10898: 9, 12185: 9, 2029: 9, 11317: 9, 13366: 9, 10398: 9, 13574: 9, 10460: 9, 1522: 9, 12776: 9, 13981: 9, 2078: 9, 8417: 9, 2129: 9, 6078: 9, 525: 9, 11509: 9, 5208: 9, 13764: 9, 10412: 9, 12803: 9, 11626: 9, 11192: 9, 782: 9, 1724: 9, 7247: 9, 4358: 9, 10589: 9, 10170: 9, 1773: 9, 5525: 9, 11164: 9, 11504: 9, 4089: 9, 11604: 9, 9795: 9, 6802: 9, 1806: 9, 7345: 9, 8950: 9, 1456: 9, 12683: 9, 10312: 9, 13886: 9, 5359: 9, 9751: 9, 8882: 9, 6653: 9, 7214: 9, 11734: 9, 12003: 9, 12179: 9, 10951: 9, 6993: 9, 2076: 9, 5140: 9, 3694: 9, 5017: 9, 7551: 9, 6494: 9, 6707: 9, 7185: 9, 7432: 9, 11348: 9, 8399: 9, 4831: 9, 12369: 9, 9081: 9, 6144: 9, 7655: 9, 9886: 9, 3922: 9, 1151: 9, 7440: 9, 7952: 9, 5053: 9, 6126: 9, 8835: 9, 2905: 9, 4289: 9, 11069: 9, 12166: 9, 3314: 9, 10713: 9, 11717: 9, 509: 9, 9786: 9, 10081: 9, 8306: 9, 6299: 9, 8516: 9, 2790: 9, 9982: 9, 1186: 9, 12362: 9, 3500: 9, 4616: 9, 6774: 9, 8925: 9, 3664: 9, 10805: 9, 7189: 9, 13785: 9, 2695: 9, 4555: 9, 11575: 9, 12733: 9, 8941: 9, 14015: 9, 7089: 9, 11127: 9, 635: 9, 1297: 9, 10731: 9, 6070: 9, 8345: 9, 8774: 9, 1269: 9, 5007: 9, 8416: 9, 9940: 9, 10364: 9, 6781: 9, 11274: 9, 1433: 9, 7400: 9, 11632: 9, 3311: 9, 10125: 9, 5431: 9, 11130: 9, 7599: 9, 12796: 9, 13878: 9, 6045: 9, 5706: 9, 9352: 9, 3367: 9, 8463: 9, 4467: 9, 4380: 9, 358: 9, 5206: 9, 3972: 9, 144: 9, 9811: 9, 11907: 9, 3503: 9, 13578: 9, 8401: 9, 7860: 9, 11565: 9, 1419: 9, 3198: 9, 1513: 9, 6416: 9, 7825: 9, 2760: 9, 12672: 9, 3721: 9, 8224: 9, 5762: 9, 9240: 9, 4623: 9, 4656: 9, 3536: 9, 145: 9, 11032: 9, 8804: 9, 3326: 9, 10092: 9, 10842: 9, 3628: 9, 8990: 9, 11824: 9, 8859: 9, 1740: 9, 3528: 9, 5486: 9, 4583: 9, 13328: 9, 9610: 9, 8926: 9, 8801: 9, 2145: 9, 763: 9, 3031: 9, 13686: 9, 10913: 9, 5694: 9, 3283: 9, 12466: 9, 6266: 9, 1438: 9, 3537: 9, 1783: 9, 1480: 9, 4721: 9, 2252: 9, 1861: 9, 11084: 9, 11150: 9, 6417: 9, 9997: 9, 8349: 9, 9332: 9, 12972: 9, 10282: 9, 13355: 9, 3437: 9, 13235: 9, 326: 9, 2186: 9, 7528: 9, 5036: 9, 9524: 9, 6428: 9, 11956: 9, 11344: 9, 6989: 9, 13944: 9, 3415: 9, 7205: 9, 8623: 9, 4332: 9, 9912: 9, 9641: 9, 1030: 9, 9124: 9, 6454: 9, 12506: 9, 6672: 9, 1619: 9, 10902: 9, 2643: 9, 7346: 9, 685: 9, 13734: 9, 12701: 9, 8408: 9, 9346: 9, 4186: 9, 5931: 9, 4558: 9, 2365: 9, 10: 9, 9113: 9, 8471: 9, 10371: 9, 6584: 9, 5847: 9, 1732: 9, 9505: 9, 10749: 9, 10826: 9, 3738: 9, 382: 9, 5741: 9, 14028: 9, 3351: 9, 10627: 9, 6251: 9, 11532: 9, 3768: 9, 6886: 9, 6392: 9, 2883: 9, 13509: 9, 898: 9, 12348: 9, 8082: 9, 10331: 9, 11921: 9, 4499: 9, 12198: 9, 11545: 9, 1946: 9, 1940: 9, 355: 9, 6852: 9, 12426: 9, 12681: 9, 2755: 9, 219: 9, 101: 9, 1670: 9, 5133: 9, 13580: 9, 6872: 9, 6506: 9, 2553: 9, 9634: 9, 10659: 9, 1277: 9, 5370: 9, 13101: 9, 1294: 9, 3012: 9, 12405: 9, 6018: 9, 6264: 9, 5889: 9, 4891: 9, 13668: 9, 1793: 9, 12056: 9, 10641: 9, 7692: 9, 7672: 9, 13518: 9, 10121: 9, 260: 9, 13106: 9, 10829: 9, 14021: 9, 11376: 9, 13283: 9, 12273: 9, 3557: 9, 4935: 9, 8792: 9, 6035: 9, 733: 9, 4226: 9, 6898: 9, 12281: 9, 6085: 9, 4885: 9, 7480: 9, 12560: 9, 5254: 9, 9116: 9, 7123: 9, 5451: 9, 3187: 9, 6949: 9, 5554: 9, 3783: 9, 8892: 9, 1324: 9, 2410: 9, 10206: 9, 48: 9, 4909: 9, 12100: 9, 424: 9, 12315: 9, 7810: 9, 3668: 9, 12574: 9, 8660: 9, 3765: 9, 799: 9, 6709: 9, 6218: 9, 640: 9, 9766: 9, 175: 9, 9308: 9, 11607: 9, 4158: 9, 9486: 9, 5995: 9, 5012: 9, 7511: 9, 11039: 9, 11315: 9, 5653: 9, 6597: 9, 2385: 9, 5765: 9, 6735: 9, 11835: 9, 528: 9, 668: 9, 8484: 9, 112: 9, 11681: 9, 9146: 9, 11870: 9, 6788: 9, 3715: 9, 10073: 9, 8413: 9, 5570: 9, 273: 9, 3803: 9, 5466: 9, 13902: 9, 9975: 9, 11799: 9, 11581: 9, 5414: 9, 385: 9, 1238: 9, 10238: 9, 13791: 9, 12016: 9, 2940: 9, 7017: 9, 4926: 9, 13004: 9, 6692: 9, 4244: 9, 2816: 9, 8012: 9, 10027: 9, 7550: 9, 4789: 9, 7106: 9, 3873: 9, 2653: 9, 9206: 9, 6970: 9, 13012: 9, 6303: 9, 5844: 9, 7150: 9, 3462: 9, 7192: 9, 1930: 9, 9421: 9, 10810: 9, 12633: 9, 4997: 9, 11567: 9, 7171: 9, 12575: 9, 6696: 9, 8500: 9, 12139: 9, 11227: 9, 9597: 9, 1384: 9, 12907: 9, 7965: 9, 1394: 9, 9498: 9, 6890: 9, 7040: 9, 2409: 9, 5550: 9, 2995: 9, 6282: 9, 9176: 9, 8151: 9, 7276: 9, 5930: 9, 11046: 9, 6152: 9, 3846: 9, 2461: 9, 10084: 9, 9467: 9, 1970: 9, 10629: 9, 9431: 9, 6470: 9, 18: 9, 1049: 9, 13086: 9, 10375: 9, 740: 9, 10037: 9, 295: 9, 566: 9, 9073: 9, 3572: 9, 10318: 9, 13526: 9, 1910: 9, 10699: 9, 7690: 9, 5310: 9, 12995: 9, 8984: 9, 11011: 9, 1730: 9, 655: 9, 13305: 9, 5884: 9, 6565: 9, 11124: 9, 11526: 9, 12484: 9, 13107: 9, 2468: 9, 11434: 9, 12437: 9, 3918: 9, 11788: 9, 12774: 9, 6261: 9, 7206: 9, 8197: 9, 9463: 9, 12921: 9, 89: 9, 6477: 9, 3872: 9, 5381: 9, 4021: 9, 2320: 9, 8312: 9, 6049: 9, 11733: 9, 9101: 9, 6028: 9, 6492: 9, 4328: 9, 5397: 9, 11701: 9, 1697: 9, 9774: 9, 8952: 9, 441: 9, 11385: 9, 3933: 9, 1248: 9, 1954: 9, 12757: 9, 14041: 9, 5681: 9, 13559: 9, 11801: 9, 641: 9, 5840: 9, 1079: 9, 12942: 9, 12521: 9, 11230: 9, 13723: 9, 9988: 9, 4045: 9, 13000: 9, 5293: 9, 490: 9, 9458: 9, 9060: 9, 3016: 9, 6622: 9, 5838: 9, 3548: 9, 11646: 9, 11676: 9, 6840: 9, 5859: 9, 6414: 9, 3317: 9, 200: 9, 1228: 9, 9043: 9, 13205: 9, 5791: 9, 9880: 9, 2659: 9, 3855: 9, 2045: 9, 6960: 9, 12487: 9, 9334: 9, 13881: 9, 10758: 9, 12555: 9, 5945: 9, 12959: 9, 5229: 9, 12469: 9, 6673: 9, 9750: 9, 7088: 9, 9737: 9, 11002: 9, 1650: 9, 14003: 9, 1762: 9, 1969: 9, 5004: 9, 3539: 9, 13766: 9, 828: 9, 6163: 9, 2609: 9, 11188: 9, 8622: 9, 5331: 9, 12856: 9, 11814: 9, 3789: 9, 11789: 9, 5584: 9, 7027: 9, 8183: 9, 8563: 9, 6647: 9, 8578: 9, 8639: 9, 12089: 9, 10514: 9, 7729: 9, 2539: 9, 5523: 9, 1272: 9, 12183: 9, 55: 9, 7115: 9, 13020: 9, 3584: 9, 13844: 9, 11784: 9, 7604: 9, 4688: 9, 4601: 9, 1950: 9, 13287: 9, 601: 9, 4651: 9, 6364: 9, 1538: 9, 2903: 9, 5992: 9, 154: 9, 1860: 9, 1623: 9, 4728: 9, 6747: 9, 9469: 9, 4115: 9, 9272: 9, 13074: 9, 189: 9, 6752: 9, 9171: 9, 11312: 9, 1003: 9, 7053: 9, 10373: 9, 10148: 9, 11146: 9, 2913: 9, 2168: 9, 162: 9, 4251: 9, 1088: 9, 9831: 9, 10846: 9, 5228: 9, 7935: 9, 3257: 9, 12112: 9, 3456: 9, 13990: 9, 8292: 9, 11992: 9, 4759: 9, 6269: 9, 8466: 9, 3397: 9, 13623: 9, 10156: 9, 3448: 9, 11745: 9, 10822: 9, 23: 9, 4059: 9, 11004: 9, 12767: 9, 299: 9, 3175: 9, 10564: 9, 6410: 9, 12831: 9, 9574: 9, 8016: 9, 7132: 9, 10306: 9, 552: 9, 6421: 9, 13992: 9, 5147: 9, 13322: 9, 6123: 9, 12122: 9, 8564: 9, 4639: 9, 13032: 9, 1095: 9, 9844: 9, 3583: 9, 12265: 9, 12507: 9, 12133: 9, 2361: 9, 13675: 9, 7964: 9, 1833: 9, 10378: 9, 11725: 9, 7177: 9, 7609: 9, 12547: 9, 5152: 9, 459: 9, 8680: 9, 6390: 9, 8584: 9, 10849: 9, 13279: 9, 3452: 9, 5035: 9, 13347: 9, 6697: 9, 8815: 9, 6806: 9, 11103: 9, 7815: 9, 331: 9, 12411: 9, 5916: 9, 1320: 9, 8315: 9, 4898: 9, 11404: 9, 8084: 9, 8180: 9, 7291: 9, 5537: 9, 542: 9, 4154: 9, 5391: 9, 12082: 9, 8208: 9, 10992: 9, 4799: 9, 11894: 9, 13480: 9, 12835: 9, 9969: 9, 2253: 9, 10056: 9, 259: 9, 2075: 9, 1428: 9, 11207: 9, 10988: 9, 3656: 9, 8870: 9, 13761: 9, 13716: 9, 11161: 9, 6742: 9, 10819: 9, 11987: 9, 9307: 9, 1201: 9, 602: 9, 9473: 9, 10039: 9, 4963: 9, 12436: 9, 5251: 9, 13490: 9, 4118: 9, 4212: 9, 2635: 9, 9676: 9, 3392: 9, 13821: 9, 8962: 9, 6227: 9, 12288: 9, 11961: 9, 9852: 9, 1019: 9, 12028: 9, 5577: 9, 11807: 9, 13188: 9, 8467: 9, 1738: 9, 13502: 9, 328: 9, 13745: 9, 718: 9, 2452: 9, 1972: 9, 8041: 9, 11503: 9, 7433: 9, 5362: 9, 8158: 9, 7143: 9, 905: 9, 12958: 9, 3287: 9, 9193: 9, 10538: 9, 11516: 9, 10112: 9, 5966: 9, 3769: 9, 5130: 9, 11298: 9, 759: 9, 11321: 9, 3898: 9, 5770: 9, 821: 9, 1434: 9, 11361: 9, 11637: 9, 8777: 9, 13690: 9, 1693: 9, 2490: 9, 12825: 9, 1544: 9, 2794: 9, 13150: 9, 9877: 9, 1221: 9, 12973: 9, 11394: 9, 5168: 9, 3781: 9, 1586: 9, 5259: 9, 7439: 9, 5974: 9, 6239: 9, 1111: 9, 2778: 9, 6359: 9, 7299: 9, 3852: 9, 11275: 9, 12722: 9, 2798: 9, 6967: 9, 13024: 9, 10313: 9, 12499: 9, 1158: 9, 1937: 9, 700: 9, 1306: 9, 5186: 9, 8348: 9, 13505: 9, 9660: 9, 13882: 9, 8891: 9, 11040: 9, 13273: 9, 4211: 9, 10424: 9, 4427: 8, 963: 8, 6468: 8, 4382: 8, 1193: 8, 354: 8, 6394: 8, 10543: 8, 4435: 8, 11221: 8, 2377: 8, 2930: 8, 3021: 8, 1825: 8, 7590: 8, 10303: 8, 5508: 8, 7315: 8, 1190: 8, 3127: 8, 4481: 8, 1161: 8, 2554: 8, 4413: 8, 13946: 8, 12013: 8, 4493: 8, 2408: 8, 2989: 8, 52: 8, 2: 8, 12395: 8, 77: 8, 2384: 8, 7260: 8, 1075: 8, 10390: 8, 4027: 8, 2497: 8, 4515: 8, 76: 8, 3952: 8, 2231: 8, 9365: 8, 4826: 8, 1184: 8, 11608: 8, 1124: 8, 1948: 8, 4306: 8, 1165: 8, 4445: 8, 10925: 8, 1065: 8, 2503: 8, 1905: 8, 4038: 8, 4396: 8, 10623: 8, 2538: 8, 1045: 8, 4017: 7, 2565: 7, 1109: 7, 58: 7, 5608: 7, 1415: 7, 70: 7, 4760: 7, 1173: 7, 4836: 7, 2853: 7, 1450: 7, 2287: 7, 6881: 7, 3883: 7, 4839: 7, 10601: 7, 10526: 7, 5047: 7, 5444: 7, 1121: 7, 10372: 6, 4376: 6, 4448: 6, 10438: 6, 1167: 6, 10545: 6, 5629: 6, 4384: 6, 7286: 6, 5369: 6, 2769: 6, 7325: 6, 1698: 6, 7676: 6, 957: 6, 4268: 6, 4412: 6, 68: 6, 1060: 6, 4318: 6, 3041: 6, 2494: 6, 87: 6, 3028: 6, 1127: 6, 1177: 6, 4433: 6, 1098: 6, 1113: 6, 1159: 6, 4454: 5, 5548: 5, 1886: 5, 10411: 5, 3073: 5, 4451: 5, 4273: 5, 4485: 5, 1579: 5, 7429: 5, 12600: 5, 10501: 5, 4756: 5, 2704: 5, 971: 5, 1440: 5, 1357: 5, 4856: 5, 4272: 5, 1959: 4, 4309: 4, 1413: 4, 4323: 4, 4463: 4, 12457: 4, 3149: 4, 362: 4, 1164: 4, 4406: 4, 1110: 4, 1386: 4, 1117: 4, 907: 4, 1939: 4, 4835: 3, 11306: 3, 10419: 3, 4755: 3, 13148: 3, 3508: 3, 4401: 3, 4317: 2, 1417: 2, 8077: 2, 6229: 2, 10618: 2, 985: 2, 142: 1, 2487: 1, 1170: 1, 5378: 1})\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDatasetDict({\\n    train: Dataset({\\n        features: ['prompt', 'chosen', 'rejected'],\\n        num_rows: 2351\\n    })\\n    val: Dataset({\\n        features: ['prompt', 'chosen', 'rejected'],\\n        num_rows: 323\\n    })\\n})\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['prompt', 'chosen', 'rejected'],\n",
    "        num_rows: 2351\n",
    "    })\n",
    "    val: Dataset({\n",
    "        features: ['prompt', 'chosen', 'rejected'],\n",
    "        num_rows: 323\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 64,\n",
    "#     lora_dropout = 0, # Currently only supports dropout = 0\n",
    "#     bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,  # We support rank stabilized LoRA\n",
    "#     loftq_config = None, # And LoftQ\n",
    "# )\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    # use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.384 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "ref_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from star.dataset.dataset import get_dpo_collate_fn, DPOCollateReturn\n",
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(ds, collate_fn=get_dpo_collate_fn(tokenizer), batch_size=8, pin_memory=True, pin_memory_device='cuda', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from star.model.model import UnslothModelHandler\n",
    "from star.agent.agent import Agent\n",
    "agent = Agent(UnslothModelHandler(\"unsloth/Meta-Llama-3.1-8B-Instruct\", max_seq_length, 10, 'const', None, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/13926 [00:04<18:07:49,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/13926 [00:09<18:52:06,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/13926 [00:12<15:30:02,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/13926 [00:15<13:55:21,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/13926 [00:19<14:33:13,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/13926 [00:23<14:56:36,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/13926 [00:27<15:11:14,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/13926 [00:31<14:19:44,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/13926 [00:34<13:45:45,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/13926 [00:37<13:22:28,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/13926 [00:40<12:34:22,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/13926 [00:43<12:01:20,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/13926 [00:45<11:33:54,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/13926 [00:48<11:20:52,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/13926 [00:51<10:57:56,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/13926 [00:54<11:37:40,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/13926 [00:58<12:05:53,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/13926 [01:01<12:25:18,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/13926 [01:05<13:53:40,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/13926 [01:09<13:40:52,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/13926 [01:11<12:18:03,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/13926 [01:14<11:20:03,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/13926 [01:17<11:54:54,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/13926 [01:20<12:19:46,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/13926 [01:23<12:07:35,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/13926 [01:26<11:44:55,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/13926 [01:29<11:30:25,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 28/13926 [01:33<12:02:45,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/13926 [01:36<12:25:41,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/13926 [01:41<14:04:39,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/13926 [01:46<15:44:32,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/13926 [01:51<16:38:17,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 33/13926 [01:55<16:09:59,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 34/13926 [01:59<16:39:03,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 35/13926 [02:04<16:59:30,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 36/13926 [02:08<16:34:31,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 37/13926 [02:12<16:16:49,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 38/13926 [02:18<18:32:32,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 39/13926 [02:23<18:42:19,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/13926 [02:28<18:31:53,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/13926 [02:32<18:24:50,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/13926 [02:36<16:46:54,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 43/13926 [02:39<15:10:45,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 44/13926 [02:44<16:49:26,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45/13926 [02:49<17:58:05,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 46/13926 [02:53<16:33:11,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 47/13926 [02:56<15:33:40,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 48/13926 [03:01<16:41:52,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 49/13926 [03:07<18:44:37,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOStepOutput(loss=tensor(0.6914, device='cuda:0', dtype=torch.bfloat16), reward_winning=tensor(0., device='cuda:0', dtype=torch.bfloat16), reward_losing=tensor(0., device='cuda:0', dtype=torch.bfloat16))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 49/13926 [03:13<15:14:52,  3.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(f\"{data.input_ids.shape=}, {data.indices=}, {data.subjects=}\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m losses \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdpo_train_step(model, ref_model, data, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(losses)\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/dataclasses.py:240\u001b[0m, in \u001b[0;36m_recursive_repr.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/_tensor.py:463\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    460\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/_tensor_str.py:698\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    697\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/_tensor_str.py:618\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    616\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    617\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 618\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    621\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/_tensor_str.py:350\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/_tensor_str.py:138\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.for_training(model)\n",
    "ref_model = FastLanguageModel.for_training(ref_model)\n",
    "from tqdm import tqdm\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(dl):\n",
    "        data.to('cuda')\n",
    "        assert isinstance(data, DPOCollateReturn)\n",
    "        # print(f\"{data.input_ids.shape=}, {data.indices=}, {data.subjects=}\")\n",
    "        losses = agent.dpo_train_step(model, ref_model, data, 0.1)\n",
    "        print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f28174707d84776811b500b199739dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1351a7f24442b094ba52edac2db053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/323 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 5e-7,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\",\n",
    "        do_eval=True,\n",
    "        # eval_on_start=True,\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = dsdict[\"train\"],\n",
    "    eval_dataset = dsdict[\"val\"],\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 4048,\n",
    "    max_prompt_length = 2024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,351 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 441\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjakobbbjorner\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nethome/jbjorner3/dev/hallucination-fun/star/scratch/wandb/run-20241118_205638-ba7w9a1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jakobbbjorner/huggingface/runs/ba7w9a1a' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/jakobbbjorner/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jakobbbjorner/huggingface' target=\"_blank\">https://wandb.ai/jakobbbjorner/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jakobbbjorner/huggingface/runs/ba7w9a1a' target=\"_blank\">https://wandb.ai/jakobbbjorner/huggingface/runs/ba7w9a1a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='354' max='441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [354/441 1:43:06 < 25:28, 0.06 it/s, Epoch 2.40/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-252.698883</td>\n",
       "      <td>-189.807663</td>\n",
       "      <td>-0.316110</td>\n",
       "      <td>-0.196663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-224.104630</td>\n",
       "      <td>-182.063660</td>\n",
       "      <td>-0.106550</td>\n",
       "      <td>0.032703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>-0.001264</td>\n",
       "      <td>-0.009477</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>-229.676697</td>\n",
       "      <td>-199.528503</td>\n",
       "      <td>-0.472446</td>\n",
       "      <td>-0.336905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.020279</td>\n",
       "      <td>-0.014921</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>-226.602066</td>\n",
       "      <td>-223.594391</td>\n",
       "      <td>-0.199553</td>\n",
       "      <td>-0.188639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>-0.017760</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.051641</td>\n",
       "      <td>-285.158051</td>\n",
       "      <td>-247.402222</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>0.054315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>-0.020636</td>\n",
       "      <td>-0.016202</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.004433</td>\n",
       "      <td>-197.397812</td>\n",
       "      <td>-198.725372</td>\n",
       "      <td>-0.007635</td>\n",
       "      <td>0.016201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>-0.003549</td>\n",
       "      <td>0.006615</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.010164</td>\n",
       "      <td>-304.536957</td>\n",
       "      <td>-231.492340</td>\n",
       "      <td>-0.193714</td>\n",
       "      <td>-0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>-0.000963</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>-257.848785</td>\n",
       "      <td>-212.232208</td>\n",
       "      <td>-0.159537</td>\n",
       "      <td>-0.118583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>-0.001393</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>-292.621765</td>\n",
       "      <td>-254.827118</td>\n",
       "      <td>0.061059</td>\n",
       "      <td>0.101393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.012086</td>\n",
       "      <td>-0.022074</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.034160</td>\n",
       "      <td>-299.590759</td>\n",
       "      <td>-223.199600</td>\n",
       "      <td>-0.049941</td>\n",
       "      <td>-0.005554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.001971</td>\n",
       "      <td>-198.832062</td>\n",
       "      <td>-209.470016</td>\n",
       "      <td>-0.038868</td>\n",
       "      <td>-0.169850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.699400</td>\n",
       "      <td>-0.013797</td>\n",
       "      <td>-0.004288</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.009509</td>\n",
       "      <td>-257.080322</td>\n",
       "      <td>-212.469345</td>\n",
       "      <td>-0.175803</td>\n",
       "      <td>-0.006371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>-197.901855</td>\n",
       "      <td>-233.413483</td>\n",
       "      <td>0.019909</td>\n",
       "      <td>-0.148991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>-0.007200</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.020094</td>\n",
       "      <td>-248.740082</td>\n",
       "      <td>-286.636169</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>-0.050509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.011032</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.014811</td>\n",
       "      <td>-278.037781</td>\n",
       "      <td>-238.316666</td>\n",
       "      <td>-0.123395</td>\n",
       "      <td>-0.062950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.695700</td>\n",
       "      <td>-0.013531</td>\n",
       "      <td>-0.009889</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.003642</td>\n",
       "      <td>-209.765259</td>\n",
       "      <td>-214.706329</td>\n",
       "      <td>-0.218692</td>\n",
       "      <td>-0.142099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.683200</td>\n",
       "      <td>0.021763</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.022769</td>\n",
       "      <td>-288.949554</td>\n",
       "      <td>-196.457123</td>\n",
       "      <td>-0.332561</td>\n",
       "      <td>-0.099054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.709600</td>\n",
       "      <td>-0.013121</td>\n",
       "      <td>0.017839</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.030961</td>\n",
       "      <td>-191.176193</td>\n",
       "      <td>-206.332825</td>\n",
       "      <td>0.126221</td>\n",
       "      <td>0.063825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.708400</td>\n",
       "      <td>-0.019316</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.028850</td>\n",
       "      <td>-241.721924</td>\n",
       "      <td>-200.491150</td>\n",
       "      <td>-0.142620</td>\n",
       "      <td>-0.051855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>-0.010332</td>\n",
       "      <td>-0.003268</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.007064</td>\n",
       "      <td>-207.679718</td>\n",
       "      <td>-202.474121</td>\n",
       "      <td>-0.272509</td>\n",
       "      <td>-0.227004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.671000</td>\n",
       "      <td>0.023801</td>\n",
       "      <td>-0.022870</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.046671</td>\n",
       "      <td>-192.825653</td>\n",
       "      <td>-199.059723</td>\n",
       "      <td>-0.069575</td>\n",
       "      <td>-0.119481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>-240.550110</td>\n",
       "      <td>-220.514816</td>\n",
       "      <td>-0.290746</td>\n",
       "      <td>-0.251647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.701900</td>\n",
       "      <td>-0.007126</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.015590</td>\n",
       "      <td>-221.547180</td>\n",
       "      <td>-209.813843</td>\n",
       "      <td>-0.122461</td>\n",
       "      <td>-0.116030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.694100</td>\n",
       "      <td>-0.000585</td>\n",
       "      <td>-0.000427</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-272.646240</td>\n",
       "      <td>-228.523682</td>\n",
       "      <td>-0.125366</td>\n",
       "      <td>-0.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.016470</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>-234.657440</td>\n",
       "      <td>-195.840469</td>\n",
       "      <td>-0.186898</td>\n",
       "      <td>-0.106760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>-0.002406</td>\n",
       "      <td>-0.019814</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>-195.214523</td>\n",
       "      <td>-169.399750</td>\n",
       "      <td>-0.164559</td>\n",
       "      <td>-0.225210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.007348</td>\n",
       "      <td>0.029132</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.021785</td>\n",
       "      <td>-246.887238</td>\n",
       "      <td>-212.558807</td>\n",
       "      <td>-0.044989</td>\n",
       "      <td>-0.085156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.682600</td>\n",
       "      <td>0.022350</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.023776</td>\n",
       "      <td>-220.456360</td>\n",
       "      <td>-226.426620</td>\n",
       "      <td>-0.163178</td>\n",
       "      <td>-0.191412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.713300</td>\n",
       "      <td>-0.014567</td>\n",
       "      <td>0.023045</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.037612</td>\n",
       "      <td>-237.259949</td>\n",
       "      <td>-237.710846</td>\n",
       "      <td>0.098436</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>0.017883</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>-202.398102</td>\n",
       "      <td>-210.357574</td>\n",
       "      <td>-0.236912</td>\n",
       "      <td>-0.297411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.704700</td>\n",
       "      <td>-0.017690</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.019948</td>\n",
       "      <td>-237.286713</td>\n",
       "      <td>-218.498428</td>\n",
       "      <td>-0.051927</td>\n",
       "      <td>0.038281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.707800</td>\n",
       "      <td>-0.012440</td>\n",
       "      <td>0.014244</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.026684</td>\n",
       "      <td>-218.243622</td>\n",
       "      <td>-219.268478</td>\n",
       "      <td>-0.286617</td>\n",
       "      <td>-0.238468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>-0.013060</td>\n",
       "      <td>-0.004855</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.008205</td>\n",
       "      <td>-250.451508</td>\n",
       "      <td>-197.455856</td>\n",
       "      <td>-0.143865</td>\n",
       "      <td>-0.162471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.703900</td>\n",
       "      <td>-0.005103</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.020168</td>\n",
       "      <td>-219.033417</td>\n",
       "      <td>-175.069183</td>\n",
       "      <td>0.014595</td>\n",
       "      <td>0.038660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.018149</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>-226.370941</td>\n",
       "      <td>-194.254425</td>\n",
       "      <td>-0.310920</td>\n",
       "      <td>-0.155923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.693900</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>-0.002006</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>-204.713165</td>\n",
       "      <td>-223.620163</td>\n",
       "      <td>-0.102738</td>\n",
       "      <td>-0.221975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.016634</td>\n",
       "      <td>-213.779694</td>\n",
       "      <td>-255.268402</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>-0.173643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>-0.001453</td>\n",
       "      <td>-0.010012</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>-360.891052</td>\n",
       "      <td>-231.794128</td>\n",
       "      <td>-0.443201</td>\n",
       "      <td>-0.168338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>-0.016472</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>-224.581314</td>\n",
       "      <td>-169.771759</td>\n",
       "      <td>-0.461762</td>\n",
       "      <td>-0.283404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>-0.007092</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.020403</td>\n",
       "      <td>-221.660172</td>\n",
       "      <td>-198.467728</td>\n",
       "      <td>-0.175936</td>\n",
       "      <td>-0.181566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.032343</td>\n",
       "      <td>-196.417023</td>\n",
       "      <td>-206.737274</td>\n",
       "      <td>-0.155981</td>\n",
       "      <td>-0.227435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>0.021513</td>\n",
       "      <td>-0.016145</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.037658</td>\n",
       "      <td>-182.710846</td>\n",
       "      <td>-175.515930</td>\n",
       "      <td>-0.204605</td>\n",
       "      <td>-0.193802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>-236.770660</td>\n",
       "      <td>-234.993286</td>\n",
       "      <td>-0.238515</td>\n",
       "      <td>-0.319867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.012610</td>\n",
       "      <td>-239.142227</td>\n",
       "      <td>-206.864777</td>\n",
       "      <td>-0.019662</td>\n",
       "      <td>-0.068047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>-300.118530</td>\n",
       "      <td>-199.838806</td>\n",
       "      <td>-0.291400</td>\n",
       "      <td>-0.165621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.693600</td>\n",
       "      <td>-0.005866</td>\n",
       "      <td>-0.006985</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-207.223969</td>\n",
       "      <td>-231.154739</td>\n",
       "      <td>-0.298414</td>\n",
       "      <td>-0.365207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>-0.018612</td>\n",
       "      <td>-0.003660</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.014952</td>\n",
       "      <td>-227.729370</td>\n",
       "      <td>-242.073929</td>\n",
       "      <td>-0.037573</td>\n",
       "      <td>-0.027180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.003619</td>\n",
       "      <td>-273.876038</td>\n",
       "      <td>-182.659836</td>\n",
       "      <td>-0.337652</td>\n",
       "      <td>-0.117799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>-0.017358</td>\n",
       "      <td>-0.023540</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>-203.144867</td>\n",
       "      <td>-194.954651</td>\n",
       "      <td>-0.059918</td>\n",
       "      <td>-0.080526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.707400</td>\n",
       "      <td>-0.016764</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.026399</td>\n",
       "      <td>-208.091003</td>\n",
       "      <td>-203.539993</td>\n",
       "      <td>-0.240474</td>\n",
       "      <td>-0.261185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>-0.016605</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>-246.808777</td>\n",
       "      <td>-205.100449</td>\n",
       "      <td>-0.157545</td>\n",
       "      <td>-0.153206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>-0.034865</td>\n",
       "      <td>-0.015386</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.019479</td>\n",
       "      <td>-239.615631</td>\n",
       "      <td>-223.041718</td>\n",
       "      <td>-0.154923</td>\n",
       "      <td>-0.106555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.712700</td>\n",
       "      <td>-0.030979</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.037204</td>\n",
       "      <td>-245.889130</td>\n",
       "      <td>-200.814484</td>\n",
       "      <td>-0.141808</td>\n",
       "      <td>-0.042630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>-0.007126</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.006858</td>\n",
       "      <td>-198.468445</td>\n",
       "      <td>-220.508957</td>\n",
       "      <td>-0.114038</td>\n",
       "      <td>-0.186275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>-0.017657</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.030598</td>\n",
       "      <td>-257.846893</td>\n",
       "      <td>-173.875671</td>\n",
       "      <td>-0.014766</td>\n",
       "      <td>0.102902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>-0.021452</td>\n",
       "      <td>-0.028382</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.006930</td>\n",
       "      <td>-263.827393</td>\n",
       "      <td>-174.232452</td>\n",
       "      <td>-0.187172</td>\n",
       "      <td>0.036050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>-0.009640</td>\n",
       "      <td>-0.031434</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.021794</td>\n",
       "      <td>-246.173874</td>\n",
       "      <td>-227.506866</td>\n",
       "      <td>-0.156875</td>\n",
       "      <td>-0.141254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.672900</td>\n",
       "      <td>0.017518</td>\n",
       "      <td>-0.025888</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.043406</td>\n",
       "      <td>-233.911621</td>\n",
       "      <td>-180.863831</td>\n",
       "      <td>0.132094</td>\n",
       "      <td>0.239765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.031331</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>-229.857056</td>\n",
       "      <td>-188.570770</td>\n",
       "      <td>-0.159937</td>\n",
       "      <td>-0.035024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>-0.026995</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.034765</td>\n",
       "      <td>-254.786469</td>\n",
       "      <td>-167.098740</td>\n",
       "      <td>-0.081420</td>\n",
       "      <td>0.115828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.020456</td>\n",
       "      <td>-0.009096</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>-239.240494</td>\n",
       "      <td>-210.768555</td>\n",
       "      <td>-0.223556</td>\n",
       "      <td>-0.165723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>-0.007121</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.002936</td>\n",
       "      <td>-199.727432</td>\n",
       "      <td>-193.108948</td>\n",
       "      <td>-0.096250</td>\n",
       "      <td>-0.109466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>-0.009642</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.013406</td>\n",
       "      <td>-200.373688</td>\n",
       "      <td>-229.776825</td>\n",
       "      <td>-0.153181</td>\n",
       "      <td>-0.307398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>-0.013291</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.014193</td>\n",
       "      <td>-238.811783</td>\n",
       "      <td>-226.803131</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.047137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.660600</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>-0.053484</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.069460</td>\n",
       "      <td>-317.362854</td>\n",
       "      <td>-202.542618</td>\n",
       "      <td>-0.145522</td>\n",
       "      <td>0.040624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>-0.008557</td>\n",
       "      <td>-0.002700</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.005857</td>\n",
       "      <td>-336.219208</td>\n",
       "      <td>-207.199020</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.185317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>-0.002306</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>-232.573761</td>\n",
       "      <td>-216.334991</td>\n",
       "      <td>-0.345455</td>\n",
       "      <td>-0.328741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>-0.004575</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.006902</td>\n",
       "      <td>-220.034058</td>\n",
       "      <td>-262.510986</td>\n",
       "      <td>-0.161958</td>\n",
       "      <td>-0.196650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.684700</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>-0.016202</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.018325</td>\n",
       "      <td>-224.443314</td>\n",
       "      <td>-173.977921</td>\n",
       "      <td>-0.025725</td>\n",
       "      <td>0.021372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.703700</td>\n",
       "      <td>-0.036704</td>\n",
       "      <td>-0.016560</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.020144</td>\n",
       "      <td>-257.855255</td>\n",
       "      <td>-267.786469</td>\n",
       "      <td>-0.201448</td>\n",
       "      <td>-0.214636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.704300</td>\n",
       "      <td>-0.008146</td>\n",
       "      <td>0.012148</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-231.629059</td>\n",
       "      <td>-208.483856</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>-0.082616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.686500</td>\n",
       "      <td>-0.009196</td>\n",
       "      <td>-0.025122</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.015926</td>\n",
       "      <td>-247.295609</td>\n",
       "      <td>-155.214371</td>\n",
       "      <td>-0.155800</td>\n",
       "      <td>0.030485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.700200</td>\n",
       "      <td>-0.000967</td>\n",
       "      <td>0.012132</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.013099</td>\n",
       "      <td>-315.466827</td>\n",
       "      <td>-196.823059</td>\n",
       "      <td>-0.061886</td>\n",
       "      <td>0.187074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>0.013397</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>-214.159805</td>\n",
       "      <td>-215.400894</td>\n",
       "      <td>-0.191806</td>\n",
       "      <td>-0.235341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.007413</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>-220.491974</td>\n",
       "      <td>-201.065964</td>\n",
       "      <td>-0.094055</td>\n",
       "      <td>-0.114674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.699400</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>-210.261322</td>\n",
       "      <td>-229.795059</td>\n",
       "      <td>-0.091096</td>\n",
       "      <td>-0.172216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>0.016521</td>\n",
       "      <td>-0.020291</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.036813</td>\n",
       "      <td>-277.255524</td>\n",
       "      <td>-204.095413</td>\n",
       "      <td>-0.138237</td>\n",
       "      <td>-0.065946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.007991</td>\n",
       "      <td>-189.675201</td>\n",
       "      <td>-188.396576</td>\n",
       "      <td>-0.146417</td>\n",
       "      <td>-0.232623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.706800</td>\n",
       "      <td>-0.012283</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.025483</td>\n",
       "      <td>-248.141998</td>\n",
       "      <td>-177.644272</td>\n",
       "      <td>-0.295826</td>\n",
       "      <td>-0.102713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>-0.015847</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.019783</td>\n",
       "      <td>-162.919189</td>\n",
       "      <td>-146.883194</td>\n",
       "      <td>-0.214385</td>\n",
       "      <td>-0.238081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>-257.249390</td>\n",
       "      <td>-271.898499</td>\n",
       "      <td>-0.106594</td>\n",
       "      <td>-0.101420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>-0.000885</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-197.305038</td>\n",
       "      <td>-183.466202</td>\n",
       "      <td>0.024231</td>\n",
       "      <td>-0.060777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.018010</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>-215.895981</td>\n",
       "      <td>-191.721054</td>\n",
       "      <td>-0.112280</td>\n",
       "      <td>-0.122094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>-0.027815</td>\n",
       "      <td>-0.017103</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.010711</td>\n",
       "      <td>-251.976379</td>\n",
       "      <td>-231.184265</td>\n",
       "      <td>0.047491</td>\n",
       "      <td>0.082926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>-0.013142</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.013437</td>\n",
       "      <td>-208.220734</td>\n",
       "      <td>-212.172165</td>\n",
       "      <td>-0.165218</td>\n",
       "      <td>-0.242548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>-0.006293</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>-217.168564</td>\n",
       "      <td>-211.862305</td>\n",
       "      <td>-0.110961</td>\n",
       "      <td>-0.049369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.698800</td>\n",
       "      <td>-0.005854</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.010028</td>\n",
       "      <td>-173.409119</td>\n",
       "      <td>-191.759842</td>\n",
       "      <td>0.011953</td>\n",
       "      <td>-0.102292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.698100</td>\n",
       "      <td>-0.002040</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>-224.243103</td>\n",
       "      <td>-233.144043</td>\n",
       "      <td>-0.201730</td>\n",
       "      <td>-0.258142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>-0.014203</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>-218.438629</td>\n",
       "      <td>-202.760620</td>\n",
       "      <td>-0.153833</td>\n",
       "      <td>-0.229349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>-0.008189</td>\n",
       "      <td>-0.006229</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>-177.824661</td>\n",
       "      <td>-183.987518</td>\n",
       "      <td>-0.152388</td>\n",
       "      <td>-0.144568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.702800</td>\n",
       "      <td>-0.009802</td>\n",
       "      <td>0.007382</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.017185</td>\n",
       "      <td>-274.703400</td>\n",
       "      <td>-300.466644</td>\n",
       "      <td>-0.039777</td>\n",
       "      <td>-0.098564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.032239</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.018521</td>\n",
       "      <td>-201.724915</td>\n",
       "      <td>-237.295837</td>\n",
       "      <td>0.100080</td>\n",
       "      <td>-0.017090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>-0.013564</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.020168</td>\n",
       "      <td>-242.820160</td>\n",
       "      <td>-198.122406</td>\n",
       "      <td>-0.230370</td>\n",
       "      <td>-0.186442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.696200</td>\n",
       "      <td>-0.019470</td>\n",
       "      <td>-0.014227</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.005243</td>\n",
       "      <td>-218.051926</td>\n",
       "      <td>-200.861572</td>\n",
       "      <td>-0.143584</td>\n",
       "      <td>-0.114318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.714400</td>\n",
       "      <td>-0.030182</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>-0.040967</td>\n",
       "      <td>-237.800476</td>\n",
       "      <td>-260.265564</td>\n",
       "      <td>-0.219200</td>\n",
       "      <td>-0.325770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>-0.010941</td>\n",
       "      <td>-0.015590</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>-269.130249</td>\n",
       "      <td>-215.960983</td>\n",
       "      <td>-0.113056</td>\n",
       "      <td>-0.027742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>-0.026315</td>\n",
       "      <td>-0.017997</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.008317</td>\n",
       "      <td>-275.414886</td>\n",
       "      <td>-269.706238</td>\n",
       "      <td>-0.266856</td>\n",
       "      <td>-0.309447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>-0.012343</td>\n",
       "      <td>-0.019591</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.007248</td>\n",
       "      <td>-254.382294</td>\n",
       "      <td>-181.675354</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.157805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>-0.025339</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.036260</td>\n",
       "      <td>-241.135773</td>\n",
       "      <td>-218.044571</td>\n",
       "      <td>-0.025046</td>\n",
       "      <td>-0.144783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>-0.008257</td>\n",
       "      <td>0.012488</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.020745</td>\n",
       "      <td>-239.271545</td>\n",
       "      <td>-247.740906</td>\n",
       "      <td>0.077902</td>\n",
       "      <td>-0.079409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.008595</td>\n",
       "      <td>-226.948792</td>\n",
       "      <td>-243.901978</td>\n",
       "      <td>-0.299088</td>\n",
       "      <td>-0.275645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>-0.001226</td>\n",
       "      <td>-0.007124</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.005897</td>\n",
       "      <td>-264.000549</td>\n",
       "      <td>-198.992615</td>\n",
       "      <td>0.047133</td>\n",
       "      <td>0.173813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.670200</td>\n",
       "      <td>0.026285</td>\n",
       "      <td>-0.022117</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.048401</td>\n",
       "      <td>-262.291321</td>\n",
       "      <td>-241.904556</td>\n",
       "      <td>-0.201981</td>\n",
       "      <td>-0.245149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>-0.015335</td>\n",
       "      <td>-0.005173</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.010162</td>\n",
       "      <td>-184.608276</td>\n",
       "      <td>-260.143280</td>\n",
       "      <td>-0.012294</td>\n",
       "      <td>-0.080351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.014130</td>\n",
       "      <td>-0.030106</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.044236</td>\n",
       "      <td>-252.817917</td>\n",
       "      <td>-237.722504</td>\n",
       "      <td>-0.116608</td>\n",
       "      <td>-0.122717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>-0.024441</td>\n",
       "      <td>-0.016364</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.008076</td>\n",
       "      <td>-198.269226</td>\n",
       "      <td>-229.212250</td>\n",
       "      <td>-0.038616</td>\n",
       "      <td>-0.157166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.006266</td>\n",
       "      <td>-0.003938</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>-213.716217</td>\n",
       "      <td>-196.874847</td>\n",
       "      <td>-0.108960</td>\n",
       "      <td>-0.208244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>-0.011258</td>\n",
       "      <td>-0.022044</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>-274.645630</td>\n",
       "      <td>-198.237778</td>\n",
       "      <td>-0.387646</td>\n",
       "      <td>-0.220509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.680200</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>-0.007165</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.027890</td>\n",
       "      <td>-192.234024</td>\n",
       "      <td>-179.065826</td>\n",
       "      <td>-0.154835</td>\n",
       "      <td>-0.063528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>-0.008841</td>\n",
       "      <td>-0.011658</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>-209.495300</td>\n",
       "      <td>-326.889771</td>\n",
       "      <td>0.015593</td>\n",
       "      <td>-0.218027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>-0.019512</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.037624</td>\n",
       "      <td>-235.902466</td>\n",
       "      <td>-194.186615</td>\n",
       "      <td>-0.196510</td>\n",
       "      <td>-0.088658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.695500</td>\n",
       "      <td>-0.002151</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.002645</td>\n",
       "      <td>-211.087296</td>\n",
       "      <td>-221.519318</td>\n",
       "      <td>-0.174723</td>\n",
       "      <td>-0.280295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>-0.023578</td>\n",
       "      <td>-0.026757</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>-322.919800</td>\n",
       "      <td>-195.077225</td>\n",
       "      <td>-0.165934</td>\n",
       "      <td>0.037744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>-0.015165</td>\n",
       "      <td>-0.028779</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>-296.116364</td>\n",
       "      <td>-220.475204</td>\n",
       "      <td>-0.249993</td>\n",
       "      <td>-0.142679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.685100</td>\n",
       "      <td>-0.003511</td>\n",
       "      <td>-0.020229</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>-205.915405</td>\n",
       "      <td>-187.816849</td>\n",
       "      <td>-0.188054</td>\n",
       "      <td>-0.228734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.699400</td>\n",
       "      <td>-0.018369</td>\n",
       "      <td>-0.009065</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>-226.732605</td>\n",
       "      <td>-256.097687</td>\n",
       "      <td>-0.009539</td>\n",
       "      <td>-0.063227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>-0.005590</td>\n",
       "      <td>-0.016441</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.010850</td>\n",
       "      <td>-200.952515</td>\n",
       "      <td>-219.593201</td>\n",
       "      <td>-0.297697</td>\n",
       "      <td>-0.340475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.010144</td>\n",
       "      <td>-0.002586</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.012730</td>\n",
       "      <td>-196.999451</td>\n",
       "      <td>-183.325134</td>\n",
       "      <td>-0.264901</td>\n",
       "      <td>-0.313675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>-0.005354</td>\n",
       "      <td>-0.010742</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>-186.450592</td>\n",
       "      <td>-191.078201</td>\n",
       "      <td>-0.036752</td>\n",
       "      <td>-0.055280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>-0.000907</td>\n",
       "      <td>0.015725</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.016631</td>\n",
       "      <td>-221.685211</td>\n",
       "      <td>-225.932983</td>\n",
       "      <td>-0.255812</td>\n",
       "      <td>-0.197062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.015397</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.014184</td>\n",
       "      <td>-222.020340</td>\n",
       "      <td>-186.255112</td>\n",
       "      <td>-0.150573</td>\n",
       "      <td>-0.006466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.010111</td>\n",
       "      <td>0.017960</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>-266.366608</td>\n",
       "      <td>-210.401779</td>\n",
       "      <td>0.105581</td>\n",
       "      <td>0.126118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>-0.031872</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.037841</td>\n",
       "      <td>-257.579346</td>\n",
       "      <td>-229.298615</td>\n",
       "      <td>-0.203127</td>\n",
       "      <td>-0.078959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>0.011002</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.010015</td>\n",
       "      <td>-250.369751</td>\n",
       "      <td>-210.568054</td>\n",
       "      <td>-0.182448</td>\n",
       "      <td>-0.146868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.697400</td>\n",
       "      <td>-0.014793</td>\n",
       "      <td>-0.007259</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.007534</td>\n",
       "      <td>-194.481415</td>\n",
       "      <td>-212.543701</td>\n",
       "      <td>-0.193383</td>\n",
       "      <td>-0.215798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>-0.009062</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-250.017792</td>\n",
       "      <td>-220.689026</td>\n",
       "      <td>-0.283276</td>\n",
       "      <td>-0.214827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.710300</td>\n",
       "      <td>-0.012110</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.032319</td>\n",
       "      <td>-245.352814</td>\n",
       "      <td>-257.748596</td>\n",
       "      <td>-0.128260</td>\n",
       "      <td>-0.220426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.692800</td>\n",
       "      <td>-0.008078</td>\n",
       "      <td>-0.011142</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>-244.659302</td>\n",
       "      <td>-260.218079</td>\n",
       "      <td>-0.126668</td>\n",
       "      <td>-0.191533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>-0.016702</td>\n",
       "      <td>-0.013362</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.003339</td>\n",
       "      <td>-275.697998</td>\n",
       "      <td>-173.904968</td>\n",
       "      <td>-0.152667</td>\n",
       "      <td>0.052401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.681300</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>-0.020214</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>-224.764511</td>\n",
       "      <td>-221.302307</td>\n",
       "      <td>-0.203162</td>\n",
       "      <td>-0.144847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>-0.022237</td>\n",
       "      <td>-0.003894</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.018343</td>\n",
       "      <td>-228.959244</td>\n",
       "      <td>-198.081390</td>\n",
       "      <td>-0.332425</td>\n",
       "      <td>-0.185493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.685700</td>\n",
       "      <td>0.005855</td>\n",
       "      <td>-0.011013</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.016869</td>\n",
       "      <td>-217.877121</td>\n",
       "      <td>-210.143478</td>\n",
       "      <td>-0.122517</td>\n",
       "      <td>-0.062204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.701900</td>\n",
       "      <td>-0.016373</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.016828</td>\n",
       "      <td>-206.702118</td>\n",
       "      <td>-223.577240</td>\n",
       "      <td>-0.006151</td>\n",
       "      <td>-0.082177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.013686</td>\n",
       "      <td>-0.013929</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.027615</td>\n",
       "      <td>-202.766510</td>\n",
       "      <td>-246.800934</td>\n",
       "      <td>-0.082924</td>\n",
       "      <td>-0.174931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.010911</td>\n",
       "      <td>-0.025614</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.036525</td>\n",
       "      <td>-230.507935</td>\n",
       "      <td>-232.072418</td>\n",
       "      <td>-0.152109</td>\n",
       "      <td>-0.213869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.009683</td>\n",
       "      <td>-0.020941</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.030624</td>\n",
       "      <td>-278.988556</td>\n",
       "      <td>-234.767578</td>\n",
       "      <td>-0.164794</td>\n",
       "      <td>-0.136403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.708600</td>\n",
       "      <td>-0.031660</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.029530</td>\n",
       "      <td>-189.875061</td>\n",
       "      <td>-191.651855</td>\n",
       "      <td>-0.070496</td>\n",
       "      <td>-0.170557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>-0.017606</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.019698</td>\n",
       "      <td>-259.027802</td>\n",
       "      <td>-218.918335</td>\n",
       "      <td>0.017831</td>\n",
       "      <td>-0.059521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>-0.028632</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.028074</td>\n",
       "      <td>-233.445465</td>\n",
       "      <td>-214.448914</td>\n",
       "      <td>-0.366177</td>\n",
       "      <td>-0.288685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>-0.009118</td>\n",
       "      <td>-0.009347</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>-203.805725</td>\n",
       "      <td>-172.643112</td>\n",
       "      <td>-0.174478</td>\n",
       "      <td>-0.092103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.702700</td>\n",
       "      <td>-0.002524</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.017500</td>\n",
       "      <td>-190.182785</td>\n",
       "      <td>-279.145874</td>\n",
       "      <td>0.196839</td>\n",
       "      <td>-0.044481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>-0.005197</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.009545</td>\n",
       "      <td>-235.400299</td>\n",
       "      <td>-243.300369</td>\n",
       "      <td>-0.207264</td>\n",
       "      <td>-0.219194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>-0.026212</td>\n",
       "      <td>-0.019811</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.006401</td>\n",
       "      <td>-283.896667</td>\n",
       "      <td>-224.609573</td>\n",
       "      <td>-0.228779</td>\n",
       "      <td>-0.135256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>-0.039756</td>\n",
       "      <td>-0.024021</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.015735</td>\n",
       "      <td>-245.430328</td>\n",
       "      <td>-227.780777</td>\n",
       "      <td>0.204720</td>\n",
       "      <td>0.170708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.018616</td>\n",
       "      <td>-0.002050</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>-225.289642</td>\n",
       "      <td>-241.286453</td>\n",
       "      <td>-0.148067</td>\n",
       "      <td>-0.322796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.681200</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>-0.020636</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.025339</td>\n",
       "      <td>-201.692062</td>\n",
       "      <td>-259.790222</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>-0.099066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>-0.024066</td>\n",
       "      <td>-0.009816</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>-239.927872</td>\n",
       "      <td>-237.833359</td>\n",
       "      <td>-0.167364</td>\n",
       "      <td>-0.197425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.020838</td>\n",
       "      <td>-0.006626</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027464</td>\n",
       "      <td>-293.335846</td>\n",
       "      <td>-223.820465</td>\n",
       "      <td>-0.172208</td>\n",
       "      <td>-0.091966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>-214.445618</td>\n",
       "      <td>-223.668304</td>\n",
       "      <td>-0.183935</td>\n",
       "      <td>-0.314542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.676900</td>\n",
       "      <td>0.016742</td>\n",
       "      <td>-0.017856</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>-319.352051</td>\n",
       "      <td>-248.219986</td>\n",
       "      <td>-0.198027</td>\n",
       "      <td>-0.036161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.704700</td>\n",
       "      <td>-0.028727</td>\n",
       "      <td>-0.006599</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.022128</td>\n",
       "      <td>-187.703613</td>\n",
       "      <td>-224.400146</td>\n",
       "      <td>-0.180698</td>\n",
       "      <td>-0.245686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>-0.035523</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.036824</td>\n",
       "      <td>-225.278656</td>\n",
       "      <td>-241.828796</td>\n",
       "      <td>-0.025187</td>\n",
       "      <td>-0.100861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>-0.007697</td>\n",
       "      <td>-0.028315</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.020618</td>\n",
       "      <td>-285.850006</td>\n",
       "      <td>-208.431427</td>\n",
       "      <td>-0.085544</td>\n",
       "      <td>-0.072012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>-0.002220</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.011414</td>\n",
       "      <td>-297.712006</td>\n",
       "      <td>-164.779739</td>\n",
       "      <td>-0.258391</td>\n",
       "      <td>0.047571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.022649</td>\n",
       "      <td>-0.001432</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.024081</td>\n",
       "      <td>-296.269958</td>\n",
       "      <td>-232.579956</td>\n",
       "      <td>-0.139914</td>\n",
       "      <td>-0.030419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.677500</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>-0.025759</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.033376</td>\n",
       "      <td>-278.339783</td>\n",
       "      <td>-235.288757</td>\n",
       "      <td>-0.294286</td>\n",
       "      <td>-0.204367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>-0.016191</td>\n",
       "      <td>-0.007557</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.008634</td>\n",
       "      <td>-267.341156</td>\n",
       "      <td>-248.356384</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.102483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>-0.003778</td>\n",
       "      <td>-0.013578</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>-279.713501</td>\n",
       "      <td>-239.656784</td>\n",
       "      <td>-0.140749</td>\n",
       "      <td>-0.062385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>-0.055426</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.048530</td>\n",
       "      <td>-219.392059</td>\n",
       "      <td>-248.003738</td>\n",
       "      <td>0.033970</td>\n",
       "      <td>0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.028418</td>\n",
       "      <td>0.006670</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.021748</td>\n",
       "      <td>-202.106750</td>\n",
       "      <td>-245.693176</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>-0.054350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.023380</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.021531</td>\n",
       "      <td>-219.166290</td>\n",
       "      <td>-241.034271</td>\n",
       "      <td>-0.166715</td>\n",
       "      <td>-0.264040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>-0.021190</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.030014</td>\n",
       "      <td>-220.540329</td>\n",
       "      <td>-223.162491</td>\n",
       "      <td>-0.225635</td>\n",
       "      <td>-0.137816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>-0.005380</td>\n",
       "      <td>-0.047270</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.041890</td>\n",
       "      <td>-229.910889</td>\n",
       "      <td>-198.513992</td>\n",
       "      <td>-0.278702</td>\n",
       "      <td>-0.290016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>-0.007535</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.021361</td>\n",
       "      <td>-234.948441</td>\n",
       "      <td>-209.896912</td>\n",
       "      <td>-0.029839</td>\n",
       "      <td>0.010911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>-0.004558</td>\n",
       "      <td>-0.033941</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>-235.463333</td>\n",
       "      <td>-279.278748</td>\n",
       "      <td>-0.081210</td>\n",
       "      <td>-0.184742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.702800</td>\n",
       "      <td>-0.010831</td>\n",
       "      <td>0.007015</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.017846</td>\n",
       "      <td>-195.759811</td>\n",
       "      <td>-184.584641</td>\n",
       "      <td>-0.275534</td>\n",
       "      <td>-0.207460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>-0.023940</td>\n",
       "      <td>-0.029027</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>-174.785034</td>\n",
       "      <td>-156.446793</td>\n",
       "      <td>-0.340184</td>\n",
       "      <td>-0.311197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.703500</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.018695</td>\n",
       "      <td>-279.517487</td>\n",
       "      <td>-259.988434</td>\n",
       "      <td>-0.220185</td>\n",
       "      <td>-0.285077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.008898</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>-239.996246</td>\n",
       "      <td>-225.995575</td>\n",
       "      <td>-0.035386</td>\n",
       "      <td>0.004814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>-0.014326</td>\n",
       "      <td>-0.021192</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>-204.742432</td>\n",
       "      <td>-225.404068</td>\n",
       "      <td>-0.133614</td>\n",
       "      <td>-0.136775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.700300</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.011704</td>\n",
       "      <td>-223.618851</td>\n",
       "      <td>-203.732361</td>\n",
       "      <td>-0.051906</td>\n",
       "      <td>-0.006333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>-0.026679</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.021094</td>\n",
       "      <td>-209.370316</td>\n",
       "      <td>-237.434784</td>\n",
       "      <td>-0.163903</td>\n",
       "      <td>-0.259376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.707400</td>\n",
       "      <td>-0.008877</td>\n",
       "      <td>0.018874</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.027750</td>\n",
       "      <td>-286.520874</td>\n",
       "      <td>-217.602386</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.027601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.701300</td>\n",
       "      <td>-0.029339</td>\n",
       "      <td>-0.014617</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.014722</td>\n",
       "      <td>-279.104675</td>\n",
       "      <td>-282.113953</td>\n",
       "      <td>-0.086556</td>\n",
       "      <td>-0.131619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>-0.002168</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.003735</td>\n",
       "      <td>-290.334442</td>\n",
       "      <td>-295.567963</td>\n",
       "      <td>-0.188788</td>\n",
       "      <td>-0.266674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>-0.042447</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.052637</td>\n",
       "      <td>-259.848480</td>\n",
       "      <td>-207.860962</td>\n",
       "      <td>0.078534</td>\n",
       "      <td>0.162936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>-0.007559</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.011011</td>\n",
       "      <td>-210.277344</td>\n",
       "      <td>-178.189789</td>\n",
       "      <td>-0.204751</td>\n",
       "      <td>-0.074470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>-0.028438</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.033424</td>\n",
       "      <td>-198.495682</td>\n",
       "      <td>-205.304840</td>\n",
       "      <td>-0.116134</td>\n",
       "      <td>-0.213761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>-0.006802</td>\n",
       "      <td>-0.009783</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>-272.384521</td>\n",
       "      <td>-208.291748</td>\n",
       "      <td>-0.190919</td>\n",
       "      <td>-0.026785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.695800</td>\n",
       "      <td>-0.010931</td>\n",
       "      <td>-0.007831</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>-287.798401</td>\n",
       "      <td>-247.923416</td>\n",
       "      <td>-0.043273</td>\n",
       "      <td>-0.033994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.696400</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.002086</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.005727</td>\n",
       "      <td>-256.463013</td>\n",
       "      <td>-270.387268</td>\n",
       "      <td>-0.143586</td>\n",
       "      <td>-0.193866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>-0.010531</td>\n",
       "      <td>0.025415</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.035946</td>\n",
       "      <td>-237.793945</td>\n",
       "      <td>-200.657089</td>\n",
       "      <td>-0.175139</td>\n",
       "      <td>-0.031726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.681300</td>\n",
       "      <td>0.006778</td>\n",
       "      <td>-0.018378</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.025156</td>\n",
       "      <td>-204.249023</td>\n",
       "      <td>-190.889862</td>\n",
       "      <td>-0.227580</td>\n",
       "      <td>-0.148351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>-0.030707</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.032167</td>\n",
       "      <td>-263.677307</td>\n",
       "      <td>-258.607788</td>\n",
       "      <td>-0.161548</td>\n",
       "      <td>-0.187588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.702800</td>\n",
       "      <td>-0.020963</td>\n",
       "      <td>-0.003243</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.017720</td>\n",
       "      <td>-191.182526</td>\n",
       "      <td>-201.735535</td>\n",
       "      <td>-0.229264</td>\n",
       "      <td>-0.329137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.686500</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>-0.004361</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.015379</td>\n",
       "      <td>-204.556610</td>\n",
       "      <td>-215.038177</td>\n",
       "      <td>-0.065953</td>\n",
       "      <td>-0.141924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.014199</td>\n",
       "      <td>-0.007609</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.021808</td>\n",
       "      <td>-210.873932</td>\n",
       "      <td>-215.403046</td>\n",
       "      <td>-0.036838</td>\n",
       "      <td>-0.092635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>-0.000873</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.008047</td>\n",
       "      <td>-306.831177</td>\n",
       "      <td>-195.996246</td>\n",
       "      <td>-0.217186</td>\n",
       "      <td>-0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>-0.008193</td>\n",
       "      <td>-0.027271</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.019078</td>\n",
       "      <td>-219.885956</td>\n",
       "      <td>-190.712067</td>\n",
       "      <td>-0.319521</td>\n",
       "      <td>-0.218256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>-0.017657</td>\n",
       "      <td>-0.012763</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.004894</td>\n",
       "      <td>-194.683990</td>\n",
       "      <td>-212.845688</td>\n",
       "      <td>0.101996</td>\n",
       "      <td>0.001684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>-0.010259</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.018815</td>\n",
       "      <td>-220.213409</td>\n",
       "      <td>-190.323883</td>\n",
       "      <td>-0.232836</td>\n",
       "      <td>-0.229898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>-0.017604</td>\n",
       "      <td>-0.027019</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>-234.046234</td>\n",
       "      <td>-206.592911</td>\n",
       "      <td>0.024192</td>\n",
       "      <td>0.078560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>-0.000512</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>-219.811646</td>\n",
       "      <td>-199.360107</td>\n",
       "      <td>-0.228665</td>\n",
       "      <td>-0.209760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>-0.002462</td>\n",
       "      <td>-0.034254</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.031792</td>\n",
       "      <td>-214.797211</td>\n",
       "      <td>-193.459610</td>\n",
       "      <td>-0.041055</td>\n",
       "      <td>0.052167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>-0.023079</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>-218.258972</td>\n",
       "      <td>-226.858063</td>\n",
       "      <td>-0.116496</td>\n",
       "      <td>-0.126947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>-0.018098</td>\n",
       "      <td>-0.034441</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>-220.808167</td>\n",
       "      <td>-270.184418</td>\n",
       "      <td>-0.163260</td>\n",
       "      <td>-0.322727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.701300</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>0.010141</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.013882</td>\n",
       "      <td>-240.067932</td>\n",
       "      <td>-194.135559</td>\n",
       "      <td>-0.298505</td>\n",
       "      <td>-0.188809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>-0.022352</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.027296</td>\n",
       "      <td>-307.134155</td>\n",
       "      <td>-172.708496</td>\n",
       "      <td>-0.019392</td>\n",
       "      <td>0.096042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>-0.001497</td>\n",
       "      <td>-0.018256</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>-216.348892</td>\n",
       "      <td>-223.176056</td>\n",
       "      <td>-0.075238</td>\n",
       "      <td>-0.149961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>-0.001689</td>\n",
       "      <td>-0.004578</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>-193.513184</td>\n",
       "      <td>-256.917419</td>\n",
       "      <td>-0.274849</td>\n",
       "      <td>-0.502602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.696100</td>\n",
       "      <td>-0.006527</td>\n",
       "      <td>-0.002014</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>-0.004513</td>\n",
       "      <td>-262.064575</td>\n",
       "      <td>-268.851440</td>\n",
       "      <td>-0.104214</td>\n",
       "      <td>-0.133706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>-0.012424</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>-232.747498</td>\n",
       "      <td>-223.323914</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.055894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>-0.006265</td>\n",
       "      <td>-0.002473</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>-0.003792</td>\n",
       "      <td>-292.802246</td>\n",
       "      <td>-194.194611</td>\n",
       "      <td>-0.251705</td>\n",
       "      <td>-0.088255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>-0.012101</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.012685</td>\n",
       "      <td>-206.080200</td>\n",
       "      <td>-231.442062</td>\n",
       "      <td>-0.306431</td>\n",
       "      <td>-0.397452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.705100</td>\n",
       "      <td>-0.021121</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.022766</td>\n",
       "      <td>-190.259888</td>\n",
       "      <td>-226.456116</td>\n",
       "      <td>-0.166037</td>\n",
       "      <td>-0.319246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>-0.027219</td>\n",
       "      <td>-0.024483</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.002736</td>\n",
       "      <td>-260.091003</td>\n",
       "      <td>-200.223999</td>\n",
       "      <td>-0.196497</td>\n",
       "      <td>-0.041736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>-0.029062</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.034921</td>\n",
       "      <td>-249.518829</td>\n",
       "      <td>-188.960297</td>\n",
       "      <td>-0.293284</td>\n",
       "      <td>-0.119835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>-0.005946</td>\n",
       "      <td>-0.032789</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.026843</td>\n",
       "      <td>-214.494019</td>\n",
       "      <td>-232.016144</td>\n",
       "      <td>-0.172055</td>\n",
       "      <td>-0.235238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.700300</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.016222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.012454</td>\n",
       "      <td>-218.145874</td>\n",
       "      <td>-219.397766</td>\n",
       "      <td>-0.083386</td>\n",
       "      <td>-0.145597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.669100</td>\n",
       "      <td>0.022436</td>\n",
       "      <td>-0.027969</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.050405</td>\n",
       "      <td>-206.097992</td>\n",
       "      <td>-198.196671</td>\n",
       "      <td>-0.299578</td>\n",
       "      <td>-0.309212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.696300</td>\n",
       "      <td>-0.023886</td>\n",
       "      <td>-0.018339</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.005547</td>\n",
       "      <td>-185.663239</td>\n",
       "      <td>-171.038116</td>\n",
       "      <td>-0.096780</td>\n",
       "      <td>-0.100364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>-0.018398</td>\n",
       "      <td>-0.011631</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.006767</td>\n",
       "      <td>-213.662781</td>\n",
       "      <td>-210.932709</td>\n",
       "      <td>-0.120030</td>\n",
       "      <td>-0.141285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.694600</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-190.107391</td>\n",
       "      <td>-206.974823</td>\n",
       "      <td>-0.203794</td>\n",
       "      <td>-0.314072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.701900</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.019079</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-225.992569</td>\n",
       "      <td>-169.770020</td>\n",
       "      <td>-0.409135</td>\n",
       "      <td>-0.239769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>-0.004863</td>\n",
       "      <td>-0.027130</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.022267</td>\n",
       "      <td>-266.017670</td>\n",
       "      <td>-218.466339</td>\n",
       "      <td>-0.027320</td>\n",
       "      <td>0.003582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>-0.027671</td>\n",
       "      <td>-0.013247</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.014424</td>\n",
       "      <td>-213.480011</td>\n",
       "      <td>-237.725952</td>\n",
       "      <td>0.040878</td>\n",
       "      <td>-0.102366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>-0.006907</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.029535</td>\n",
       "      <td>-255.285660</td>\n",
       "      <td>-218.941925</td>\n",
       "      <td>-0.427739</td>\n",
       "      <td>-0.337878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.698700</td>\n",
       "      <td>-0.026660</td>\n",
       "      <td>-0.016987</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.009673</td>\n",
       "      <td>-206.804810</td>\n",
       "      <td>-282.225159</td>\n",
       "      <td>-0.084931</td>\n",
       "      <td>-0.243723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.680200</td>\n",
       "      <td>0.032022</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.029063</td>\n",
       "      <td>-245.567642</td>\n",
       "      <td>-199.670334</td>\n",
       "      <td>-0.037552</td>\n",
       "      <td>-0.027784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.695100</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.008473</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.000469</td>\n",
       "      <td>-210.642426</td>\n",
       "      <td>-193.045471</td>\n",
       "      <td>-0.171904</td>\n",
       "      <td>-0.153351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>-0.007487</td>\n",
       "      <td>-0.023197</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.015710</td>\n",
       "      <td>-225.086090</td>\n",
       "      <td>-203.664139</td>\n",
       "      <td>-0.109494</td>\n",
       "      <td>-0.092305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.021066</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>-187.951935</td>\n",
       "      <td>-154.714050</td>\n",
       "      <td>-0.148600</td>\n",
       "      <td>-0.096952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.706700</td>\n",
       "      <td>-0.035820</td>\n",
       "      <td>-0.011009</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.024810</td>\n",
       "      <td>-182.694763</td>\n",
       "      <td>-195.979034</td>\n",
       "      <td>-0.164843</td>\n",
       "      <td>-0.216871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.699900</td>\n",
       "      <td>-0.024140</td>\n",
       "      <td>-0.011714</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.012426</td>\n",
       "      <td>-217.455368</td>\n",
       "      <td>-193.406021</td>\n",
       "      <td>-0.273624</td>\n",
       "      <td>-0.164809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.707600</td>\n",
       "      <td>-0.042468</td>\n",
       "      <td>-0.015233</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.027235</td>\n",
       "      <td>-293.084686</td>\n",
       "      <td>-205.978348</td>\n",
       "      <td>0.051369</td>\n",
       "      <td>0.182341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.019238</td>\n",
       "      <td>-235.159241</td>\n",
       "      <td>-221.525330</td>\n",
       "      <td>-0.266368</td>\n",
       "      <td>-0.234295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>-0.037553</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.038094</td>\n",
       "      <td>-208.552948</td>\n",
       "      <td>-273.202606</td>\n",
       "      <td>0.041749</td>\n",
       "      <td>-0.188226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.701600</td>\n",
       "      <td>0.004932</td>\n",
       "      <td>0.018851</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.013919</td>\n",
       "      <td>-217.425873</td>\n",
       "      <td>-142.050171</td>\n",
       "      <td>-0.195546</td>\n",
       "      <td>-0.107906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.693900</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-210.584686</td>\n",
       "      <td>-174.167175</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>-0.010429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>-0.010025</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.030114</td>\n",
       "      <td>-218.936142</td>\n",
       "      <td>-191.138458</td>\n",
       "      <td>-0.233643</td>\n",
       "      <td>-0.149071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>-0.003196</td>\n",
       "      <td>0.017162</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>-199.506836</td>\n",
       "      <td>-167.281250</td>\n",
       "      <td>-0.182568</td>\n",
       "      <td>-0.200610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.681300</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>-0.023623</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.025537</td>\n",
       "      <td>-214.038864</td>\n",
       "      <td>-174.399292</td>\n",
       "      <td>-0.258553</td>\n",
       "      <td>-0.242106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.017568</td>\n",
       "      <td>-0.013305</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.030873</td>\n",
       "      <td>-231.985138</td>\n",
       "      <td>-161.751251</td>\n",
       "      <td>-0.152022</td>\n",
       "      <td>0.038272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.703200</td>\n",
       "      <td>-0.002412</td>\n",
       "      <td>0.015888</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.018300</td>\n",
       "      <td>-222.124649</td>\n",
       "      <td>-189.683563</td>\n",
       "      <td>-0.263334</td>\n",
       "      <td>-0.201838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.682900</td>\n",
       "      <td>-0.006760</td>\n",
       "      <td>-0.028368</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.021609</td>\n",
       "      <td>-220.086914</td>\n",
       "      <td>-200.194031</td>\n",
       "      <td>-0.264487</td>\n",
       "      <td>-0.232172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>-0.014630</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.036845</td>\n",
       "      <td>-258.245239</td>\n",
       "      <td>-228.193527</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.016892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.679900</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>-0.012629</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.028254</td>\n",
       "      <td>-241.770828</td>\n",
       "      <td>-213.561890</td>\n",
       "      <td>-0.269909</td>\n",
       "      <td>-0.248885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>-0.018613</td>\n",
       "      <td>-0.015490</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.003122</td>\n",
       "      <td>-226.811066</td>\n",
       "      <td>-203.127045</td>\n",
       "      <td>-0.018728</td>\n",
       "      <td>-0.021585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>-0.013974</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.015077</td>\n",
       "      <td>-175.834854</td>\n",
       "      <td>-201.397446</td>\n",
       "      <td>-0.140203</td>\n",
       "      <td>-0.297971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.684400</td>\n",
       "      <td>0.019146</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>-192.181091</td>\n",
       "      <td>-197.277557</td>\n",
       "      <td>-0.061686</td>\n",
       "      <td>-0.046370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.713200</td>\n",
       "      <td>-0.026956</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.037561</td>\n",
       "      <td>-258.152588</td>\n",
       "      <td>-202.522324</td>\n",
       "      <td>-0.299242</td>\n",
       "      <td>-0.196637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.698100</td>\n",
       "      <td>-0.005835</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.008073</td>\n",
       "      <td>-216.420471</td>\n",
       "      <td>-234.848785</td>\n",
       "      <td>-0.028390</td>\n",
       "      <td>-0.135280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.683200</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.021613</td>\n",
       "      <td>-241.894577</td>\n",
       "      <td>-222.742935</td>\n",
       "      <td>-0.211672</td>\n",
       "      <td>-0.172671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.700100</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.011786</td>\n",
       "      <td>-279.370117</td>\n",
       "      <td>-242.178543</td>\n",
       "      <td>-0.154459</td>\n",
       "      <td>-0.087629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>-0.023539</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.026817</td>\n",
       "      <td>-213.392426</td>\n",
       "      <td>-213.647980</td>\n",
       "      <td>-0.262532</td>\n",
       "      <td>-0.236997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>-0.023026</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.025758</td>\n",
       "      <td>-257.532715</td>\n",
       "      <td>-200.572632</td>\n",
       "      <td>-0.052216</td>\n",
       "      <td>0.015681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>-0.024041</td>\n",
       "      <td>-0.001210</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.022831</td>\n",
       "      <td>-178.004562</td>\n",
       "      <td>-206.303955</td>\n",
       "      <td>-0.206360</td>\n",
       "      <td>-0.256828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>-0.035197</td>\n",
       "      <td>-0.041506</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>-259.037750</td>\n",
       "      <td>-191.663788</td>\n",
       "      <td>-0.130082</td>\n",
       "      <td>-0.007318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>0.025439</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.037238</td>\n",
       "      <td>-240.303284</td>\n",
       "      <td>-226.379364</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>-0.002504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>-0.011309</td>\n",
       "      <td>-0.001954</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.009355</td>\n",
       "      <td>-195.214279</td>\n",
       "      <td>-166.894302</td>\n",
       "      <td>-0.178415</td>\n",
       "      <td>-0.239932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>-193.735291</td>\n",
       "      <td>-212.186142</td>\n",
       "      <td>-0.107444</td>\n",
       "      <td>-0.221334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.698800</td>\n",
       "      <td>-0.032295</td>\n",
       "      <td>-0.022912</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.009383</td>\n",
       "      <td>-253.726318</td>\n",
       "      <td>-208.973862</td>\n",
       "      <td>-0.453179</td>\n",
       "      <td>-0.427284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>-0.009613</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>-246.088226</td>\n",
       "      <td>-183.366089</td>\n",
       "      <td>-0.341789</td>\n",
       "      <td>-0.141050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.698800</td>\n",
       "      <td>-0.002650</td>\n",
       "      <td>0.006524</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.009174</td>\n",
       "      <td>-189.614746</td>\n",
       "      <td>-196.907318</td>\n",
       "      <td>-0.177199</td>\n",
       "      <td>-0.235401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.701100</td>\n",
       "      <td>-0.034461</td>\n",
       "      <td>-0.020312</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.014149</td>\n",
       "      <td>-218.782928</td>\n",
       "      <td>-214.347595</td>\n",
       "      <td>-0.223789</td>\n",
       "      <td>-0.238029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>-0.025914</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.020166</td>\n",
       "      <td>-298.844849</td>\n",
       "      <td>-231.160294</td>\n",
       "      <td>-0.354986</td>\n",
       "      <td>-0.236505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>-0.035719</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.038753</td>\n",
       "      <td>-201.593689</td>\n",
       "      <td>-178.698608</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>0.087414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.699900</td>\n",
       "      <td>-0.012875</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.009652</td>\n",
       "      <td>-221.085266</td>\n",
       "      <td>-244.642944</td>\n",
       "      <td>-0.051389</td>\n",
       "      <td>-0.125294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.696300</td>\n",
       "      <td>-0.010341</td>\n",
       "      <td>-0.004891</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.005451</td>\n",
       "      <td>-247.998001</td>\n",
       "      <td>-199.030289</td>\n",
       "      <td>-0.251434</td>\n",
       "      <td>-0.227434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.685200</td>\n",
       "      <td>-0.013296</td>\n",
       "      <td>-0.031420</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.018124</td>\n",
       "      <td>-280.908203</td>\n",
       "      <td>-210.192673</td>\n",
       "      <td>-0.273007</td>\n",
       "      <td>-0.133434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>-0.002748</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>-255.639771</td>\n",
       "      <td>-244.839935</td>\n",
       "      <td>-0.165477</td>\n",
       "      <td>-0.247184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>-0.016358</td>\n",
       "      <td>-0.024362</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>-249.421448</td>\n",
       "      <td>-216.312195</td>\n",
       "      <td>-0.097822</td>\n",
       "      <td>-0.082920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>-0.009255</td>\n",
       "      <td>-0.022080</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>-344.142029</td>\n",
       "      <td>-210.049438</td>\n",
       "      <td>-0.107115</td>\n",
       "      <td>0.189957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>-0.023216</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.025748</td>\n",
       "      <td>-190.325745</td>\n",
       "      <td>-225.814301</td>\n",
       "      <td>-0.040293</td>\n",
       "      <td>-0.094699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.700700</td>\n",
       "      <td>-0.018350</td>\n",
       "      <td>-0.006104</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.012246</td>\n",
       "      <td>-229.416016</td>\n",
       "      <td>-266.326843</td>\n",
       "      <td>-0.088802</td>\n",
       "      <td>-0.162437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.026224</td>\n",
       "      <td>-219.954407</td>\n",
       "      <td>-209.872421</td>\n",
       "      <td>0.029510</td>\n",
       "      <td>0.010878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>-0.023579</td>\n",
       "      <td>-0.049971</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.026391</td>\n",
       "      <td>-192.634308</td>\n",
       "      <td>-230.239532</td>\n",
       "      <td>-0.075730</td>\n",
       "      <td>-0.233424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>-0.003575</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>-268.632477</td>\n",
       "      <td>-203.621063</td>\n",
       "      <td>-0.139277</td>\n",
       "      <td>-0.059781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>-0.012630</td>\n",
       "      <td>-0.005647</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.006983</td>\n",
       "      <td>-287.413086</td>\n",
       "      <td>-167.799774</td>\n",
       "      <td>-0.013086</td>\n",
       "      <td>0.160182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>-0.009012</td>\n",
       "      <td>-0.025574</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>-193.789612</td>\n",
       "      <td>-219.110657</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>-0.181812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.701900</td>\n",
       "      <td>-0.032966</td>\n",
       "      <td>-0.016937</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.016029</td>\n",
       "      <td>-267.700317</td>\n",
       "      <td>-260.372986</td>\n",
       "      <td>-0.307522</td>\n",
       "      <td>-0.337746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.007301</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>-203.493958</td>\n",
       "      <td>-215.654449</td>\n",
       "      <td>-0.182682</td>\n",
       "      <td>-0.216166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.684400</td>\n",
       "      <td>-0.031804</td>\n",
       "      <td>-0.051628</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.019823</td>\n",
       "      <td>-242.285965</td>\n",
       "      <td>-191.950775</td>\n",
       "      <td>-0.136993</td>\n",
       "      <td>0.008294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.700900</td>\n",
       "      <td>-0.024664</td>\n",
       "      <td>-0.011334</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.013330</td>\n",
       "      <td>-221.980484</td>\n",
       "      <td>-254.258743</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>-0.101959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.671800</td>\n",
       "      <td>-0.003171</td>\n",
       "      <td>-0.048001</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>-256.213776</td>\n",
       "      <td>-196.244537</td>\n",
       "      <td>-0.270319</td>\n",
       "      <td>-0.178689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>-0.026120</td>\n",
       "      <td>0.017892</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.044011</td>\n",
       "      <td>-234.716858</td>\n",
       "      <td>-255.849228</td>\n",
       "      <td>-0.129167</td>\n",
       "      <td>-0.208128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>-0.003838</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-278.143127</td>\n",
       "      <td>-224.395859</td>\n",
       "      <td>-0.310329</td>\n",
       "      <td>-0.194338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>-0.027664</td>\n",
       "      <td>-0.048514</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.020850</td>\n",
       "      <td>-214.983978</td>\n",
       "      <td>-208.568176</td>\n",
       "      <td>0.020949</td>\n",
       "      <td>0.060557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>-0.017655</td>\n",
       "      <td>0.020533</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.038188</td>\n",
       "      <td>-220.924622</td>\n",
       "      <td>-221.928406</td>\n",
       "      <td>-0.143720</td>\n",
       "      <td>-0.130473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>-0.014090</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.020670</td>\n",
       "      <td>-241.662628</td>\n",
       "      <td>-233.921997</td>\n",
       "      <td>-0.187074</td>\n",
       "      <td>-0.065989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.665800</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.054403</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.057055</td>\n",
       "      <td>-269.963196</td>\n",
       "      <td>-247.321335</td>\n",
       "      <td>-0.077213</td>\n",
       "      <td>-0.133332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>-0.003802</td>\n",
       "      <td>-0.021965</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.018163</td>\n",
       "      <td>-236.907883</td>\n",
       "      <td>-257.696472</td>\n",
       "      <td>-0.204102</td>\n",
       "      <td>-0.230714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.020336</td>\n",
       "      <td>-271.357819</td>\n",
       "      <td>-245.977646</td>\n",
       "      <td>-0.033513</td>\n",
       "      <td>0.066691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.016222</td>\n",
       "      <td>0.009499</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>-248.317001</td>\n",
       "      <td>-122.681252</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.278854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.014928</td>\n",
       "      <td>-0.032939</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.047866</td>\n",
       "      <td>-264.720398</td>\n",
       "      <td>-184.149475</td>\n",
       "      <td>-0.125633</td>\n",
       "      <td>-0.081054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>-0.002261</td>\n",
       "      <td>-0.064815</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.062554</td>\n",
       "      <td>-213.807404</td>\n",
       "      <td>-171.714020</td>\n",
       "      <td>-0.183752</td>\n",
       "      <td>-0.088777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>-0.010667</td>\n",
       "      <td>-0.038083</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.027416</td>\n",
       "      <td>-266.958740</td>\n",
       "      <td>-231.490158</td>\n",
       "      <td>0.099918</td>\n",
       "      <td>0.110244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>-0.011316</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.018669</td>\n",
       "      <td>-256.818665</td>\n",
       "      <td>-214.074432</td>\n",
       "      <td>-0.030978</td>\n",
       "      <td>0.071667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.671100</td>\n",
       "      <td>-0.007927</td>\n",
       "      <td>-0.063984</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.056057</td>\n",
       "      <td>-211.214966</td>\n",
       "      <td>-254.427109</td>\n",
       "      <td>-0.217779</td>\n",
       "      <td>-0.376046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>-0.010875</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.011575</td>\n",
       "      <td>-216.159744</td>\n",
       "      <td>-229.174835</td>\n",
       "      <td>-0.278825</td>\n",
       "      <td>-0.320936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>-0.006277</td>\n",
       "      <td>0.011170</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.017447</td>\n",
       "      <td>-254.489883</td>\n",
       "      <td>-209.524704</td>\n",
       "      <td>-0.144030</td>\n",
       "      <td>-0.041273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.695100</td>\n",
       "      <td>-0.009392</td>\n",
       "      <td>-0.008019</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.001373</td>\n",
       "      <td>-266.679169</td>\n",
       "      <td>-231.593475</td>\n",
       "      <td>0.085518</td>\n",
       "      <td>0.161819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>-0.013932</td>\n",
       "      <td>-0.012870</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.001061</td>\n",
       "      <td>-234.245575</td>\n",
       "      <td>-232.178726</td>\n",
       "      <td>0.251782</td>\n",
       "      <td>0.189035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>-0.015269</td>\n",
       "      <td>-0.026242</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.010973</td>\n",
       "      <td>-164.598877</td>\n",
       "      <td>-187.607437</td>\n",
       "      <td>-0.016792</td>\n",
       "      <td>-0.057788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>-0.006415</td>\n",
       "      <td>-0.025767</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.019352</td>\n",
       "      <td>-204.812683</td>\n",
       "      <td>-183.433517</td>\n",
       "      <td>-0.156004</td>\n",
       "      <td>-0.207088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.698200</td>\n",
       "      <td>-0.018089</td>\n",
       "      <td>-0.009554</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.008535</td>\n",
       "      <td>-206.437759</td>\n",
       "      <td>-212.164841</td>\n",
       "      <td>-0.090934</td>\n",
       "      <td>-0.198972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>-0.002561</td>\n",
       "      <td>-0.020374</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.017813</td>\n",
       "      <td>-227.101501</td>\n",
       "      <td>-166.375015</td>\n",
       "      <td>-0.169134</td>\n",
       "      <td>-0.058210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>-0.018012</td>\n",
       "      <td>-0.029350</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.011339</td>\n",
       "      <td>-298.182190</td>\n",
       "      <td>-232.089966</td>\n",
       "      <td>-0.268336</td>\n",
       "      <td>-0.118824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>-0.001146</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.016284</td>\n",
       "      <td>-273.125580</td>\n",
       "      <td>-193.490219</td>\n",
       "      <td>-0.014823</td>\n",
       "      <td>0.125006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>-0.008661</td>\n",
       "      <td>-0.032753</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.024092</td>\n",
       "      <td>-239.231995</td>\n",
       "      <td>-198.066116</td>\n",
       "      <td>-0.110444</td>\n",
       "      <td>0.010730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>-0.007281</td>\n",
       "      <td>-0.030122</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.022841</td>\n",
       "      <td>-236.403427</td>\n",
       "      <td>-188.872147</td>\n",
       "      <td>-0.043045</td>\n",
       "      <td>-0.040914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>-0.007939</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>-230.284973</td>\n",
       "      <td>-226.732330</td>\n",
       "      <td>-0.088601</td>\n",
       "      <td>-0.143534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.704400</td>\n",
       "      <td>-0.017604</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.021092</td>\n",
       "      <td>-213.960480</td>\n",
       "      <td>-182.723785</td>\n",
       "      <td>-0.307581</td>\n",
       "      <td>-0.221402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>-0.026035</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.028260</td>\n",
       "      <td>-268.319580</td>\n",
       "      <td>-210.608185</td>\n",
       "      <td>-0.088284</td>\n",
       "      <td>-0.108296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.676300</td>\n",
       "      <td>0.019578</td>\n",
       "      <td>-0.016063</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.035641</td>\n",
       "      <td>-240.472916</td>\n",
       "      <td>-220.806976</td>\n",
       "      <td>-0.180264</td>\n",
       "      <td>-0.197489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>-0.013644</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>-233.220276</td>\n",
       "      <td>-197.262482</td>\n",
       "      <td>-0.173167</td>\n",
       "      <td>-0.085067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>-0.034379</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.067367</td>\n",
       "      <td>-221.986237</td>\n",
       "      <td>-243.355225</td>\n",
       "      <td>-0.342990</td>\n",
       "      <td>-0.420399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.694400</td>\n",
       "      <td>-0.016879</td>\n",
       "      <td>-0.016241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>-251.547028</td>\n",
       "      <td>-213.207458</td>\n",
       "      <td>-0.196918</td>\n",
       "      <td>-0.115264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>-0.009924</td>\n",
       "      <td>-0.043710</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.033786</td>\n",
       "      <td>-238.688904</td>\n",
       "      <td>-208.777100</td>\n",
       "      <td>-0.117395</td>\n",
       "      <td>-0.042355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>-0.016963</td>\n",
       "      <td>-0.028967</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>-233.476318</td>\n",
       "      <td>-214.850952</td>\n",
       "      <td>-0.149839</td>\n",
       "      <td>-0.077874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>-0.042591</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.043778</td>\n",
       "      <td>-312.141785</td>\n",
       "      <td>-244.032913</td>\n",
       "      <td>-0.159362</td>\n",
       "      <td>-0.134444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.695200</td>\n",
       "      <td>-0.022771</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.001774</td>\n",
       "      <td>-233.584122</td>\n",
       "      <td>-234.057007</td>\n",
       "      <td>-0.198500</td>\n",
       "      <td>-0.207989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>-0.009544</td>\n",
       "      <td>-0.032109</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.022565</td>\n",
       "      <td>-193.610947</td>\n",
       "      <td>-223.277863</td>\n",
       "      <td>0.042863</td>\n",
       "      <td>-0.150859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.706700</td>\n",
       "      <td>-0.009317</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-232.557938</td>\n",
       "      <td>-215.337326</td>\n",
       "      <td>0.045831</td>\n",
       "      <td>0.032326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>-0.022609</td>\n",
       "      <td>-0.025262</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-202.883682</td>\n",
       "      <td>-223.386139</td>\n",
       "      <td>-0.160939</td>\n",
       "      <td>-0.240214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.676700</td>\n",
       "      <td>0.021962</td>\n",
       "      <td>-0.012406</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.034368</td>\n",
       "      <td>-221.473480</td>\n",
       "      <td>-215.601776</td>\n",
       "      <td>-0.106523</td>\n",
       "      <td>-0.144648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>-0.015301</td>\n",
       "      <td>-0.024600</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.009298</td>\n",
       "      <td>-195.087738</td>\n",
       "      <td>-209.541473</td>\n",
       "      <td>-0.245464</td>\n",
       "      <td>-0.349892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>-0.004103</td>\n",
       "      <td>-0.037876</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.033773</td>\n",
       "      <td>-231.366516</td>\n",
       "      <td>-185.413208</td>\n",
       "      <td>0.104781</td>\n",
       "      <td>0.094343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.670100</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>-0.041406</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.047280</td>\n",
       "      <td>-211.764450</td>\n",
       "      <td>-214.260468</td>\n",
       "      <td>-0.117894</td>\n",
       "      <td>-0.143578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>-0.062758</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.078074</td>\n",
       "      <td>-267.473236</td>\n",
       "      <td>-208.091217</td>\n",
       "      <td>-0.246774</td>\n",
       "      <td>-0.167171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>-0.010922</td>\n",
       "      <td>-0.015079</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>-219.938736</td>\n",
       "      <td>-276.638031</td>\n",
       "      <td>0.048958</td>\n",
       "      <td>-0.191793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.696400</td>\n",
       "      <td>-0.026014</td>\n",
       "      <td>-0.020987</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.005026</td>\n",
       "      <td>-189.529175</td>\n",
       "      <td>-161.934601</td>\n",
       "      <td>-0.138935</td>\n",
       "      <td>-0.105742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.005703</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.001343</td>\n",
       "      <td>-203.307907</td>\n",
       "      <td>-185.297974</td>\n",
       "      <td>-0.150046</td>\n",
       "      <td>-0.084486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>-0.018377</td>\n",
       "      <td>-0.037138</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.018761</td>\n",
       "      <td>-258.332489</td>\n",
       "      <td>-224.091003</td>\n",
       "      <td>0.029115</td>\n",
       "      <td>0.052886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>-0.002904</td>\n",
       "      <td>0.015056</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.017960</td>\n",
       "      <td>-203.739578</td>\n",
       "      <td>-229.864273</td>\n",
       "      <td>0.026457</td>\n",
       "      <td>-0.142751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>-0.003884</td>\n",
       "      <td>-0.004527</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>-220.877319</td>\n",
       "      <td>-190.301453</td>\n",
       "      <td>-0.012781</td>\n",
       "      <td>0.008393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>0.011948</td>\n",
       "      <td>-0.007533</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>-261.227570</td>\n",
       "      <td>-209.296738</td>\n",
       "      <td>-0.262951</td>\n",
       "      <td>-0.109553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>-0.023091</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.028399</td>\n",
       "      <td>-222.947479</td>\n",
       "      <td>-196.129639</td>\n",
       "      <td>-0.077145</td>\n",
       "      <td>-0.026804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>-0.005723</td>\n",
       "      <td>-0.012442</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>-242.024429</td>\n",
       "      <td>-220.721954</td>\n",
       "      <td>-0.056041</td>\n",
       "      <td>-0.118885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.701400</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>-0.009022</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-214.563141</td>\n",
       "      <td>-192.015839</td>\n",
       "      <td>-0.204590</td>\n",
       "      <td>-0.221635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.672900</td>\n",
       "      <td>0.008741</td>\n",
       "      <td>-0.034014</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.042755</td>\n",
       "      <td>-279.471008</td>\n",
       "      <td>-216.841461</td>\n",
       "      <td>-0.293709</td>\n",
       "      <td>-0.207334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>0.005520</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008970</td>\n",
       "      <td>-258.550171</td>\n",
       "      <td>-203.485107</td>\n",
       "      <td>-0.414327</td>\n",
       "      <td>-0.312450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>-0.003028</td>\n",
       "      <td>-0.031910</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.028882</td>\n",
       "      <td>-302.096191</td>\n",
       "      <td>-269.034424</td>\n",
       "      <td>-0.161872</td>\n",
       "      <td>-0.190671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>-0.007781</td>\n",
       "      <td>-0.022141</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.014361</td>\n",
       "      <td>-199.551544</td>\n",
       "      <td>-244.375061</td>\n",
       "      <td>0.044802</td>\n",
       "      <td>-0.238440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>-0.018867</td>\n",
       "      <td>-0.018528</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>-219.433197</td>\n",
       "      <td>-225.262497</td>\n",
       "      <td>-0.323007</td>\n",
       "      <td>-0.378778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>-0.017144</td>\n",
       "      <td>-0.037915</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.020771</td>\n",
       "      <td>-231.341156</td>\n",
       "      <td>-186.860382</td>\n",
       "      <td>-0.410857</td>\n",
       "      <td>-0.329093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.681200</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.011266</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>-209.203049</td>\n",
       "      <td>-236.718811</td>\n",
       "      <td>-0.336448</td>\n",
       "      <td>-0.335433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.006232</td>\n",
       "      <td>-0.027535</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.033767</td>\n",
       "      <td>-237.630310</td>\n",
       "      <td>-207.895111</td>\n",
       "      <td>-0.198258</td>\n",
       "      <td>-0.113841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.694400</td>\n",
       "      <td>-0.007557</td>\n",
       "      <td>-0.008036</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>-204.450272</td>\n",
       "      <td>-198.323273</td>\n",
       "      <td>-0.084369</td>\n",
       "      <td>-0.151675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>-0.008603</td>\n",
       "      <td>-0.021246</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>-298.509430</td>\n",
       "      <td>-230.657471</td>\n",
       "      <td>-0.254300</td>\n",
       "      <td>-0.135914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>-0.008846</td>\n",
       "      <td>-0.019074</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.010228</td>\n",
       "      <td>-225.024109</td>\n",
       "      <td>-211.303101</td>\n",
       "      <td>-0.354880</td>\n",
       "      <td>-0.302515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.685100</td>\n",
       "      <td>-0.018961</td>\n",
       "      <td>-0.037924</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.018963</td>\n",
       "      <td>-243.761292</td>\n",
       "      <td>-219.732727</td>\n",
       "      <td>-0.138776</td>\n",
       "      <td>-0.070932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>-0.019046</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>-236.657410</td>\n",
       "      <td>-224.261414</td>\n",
       "      <td>-0.018962</td>\n",
       "      <td>-0.106237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>-0.007258</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.015980</td>\n",
       "      <td>-216.197754</td>\n",
       "      <td>-216.224442</td>\n",
       "      <td>-0.272806</td>\n",
       "      <td>-0.244994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>-0.017684</td>\n",
       "      <td>-0.026331</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>-312.315277</td>\n",
       "      <td>-248.885315</td>\n",
       "      <td>-0.101117</td>\n",
       "      <td>-0.026934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>-0.009550</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>-277.270691</td>\n",
       "      <td>-215.564362</td>\n",
       "      <td>-0.391944</td>\n",
       "      <td>-0.297062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>-0.012608</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.030092</td>\n",
       "      <td>-179.598053</td>\n",
       "      <td>-213.897537</td>\n",
       "      <td>-0.093463</td>\n",
       "      <td>-0.164055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.694500</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>-0.010278</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>-195.265701</td>\n",
       "      <td>-182.920258</td>\n",
       "      <td>-0.240066</td>\n",
       "      <td>-0.191123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.662900</td>\n",
       "      <td>0.025746</td>\n",
       "      <td>-0.037477</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.063223</td>\n",
       "      <td>-179.969925</td>\n",
       "      <td>-219.290024</td>\n",
       "      <td>0.053260</td>\n",
       "      <td>-0.118191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>-0.041195</td>\n",
       "      <td>-0.030970</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.010226</td>\n",
       "      <td>-298.313873</td>\n",
       "      <td>-285.072205</td>\n",
       "      <td>-0.169798</td>\n",
       "      <td>-0.164507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>-0.003300</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>-199.501099</td>\n",
       "      <td>-226.445663</td>\n",
       "      <td>-0.107714</td>\n",
       "      <td>-0.198510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>-0.015548</td>\n",
       "      <td>-0.031478</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>-233.074448</td>\n",
       "      <td>-238.729889</td>\n",
       "      <td>-0.243637</td>\n",
       "      <td>-0.172198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdpo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:142\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/flash2/jbjorner3/miniconda3/envs/star-v1/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
